{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "xffJLUZCLC51",
   "metadata": {
    "id": "xffJLUZCLC51"
   },
   "source": [
    "## Declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7TgHFX6XLQGl",
   "metadata": {
    "id": "7TgHFX6XLQGl"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d093180",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4674,
     "status": "ok",
     "timestamp": 1685806019663,
     "user": {
      "displayName": "Ri ga",
      "userId": "15539171450533002544"
     },
     "user_tz": -120
    },
    "id": "2d093180",
    "outputId": "bc84c9fd-b81e-49b9-d8a7-0c7e03c595e0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "kb = tf.keras.backend\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c5e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x1, x2, test_size=0.2, val_size=0.0, seed=0):\n",
    "    if val_size + test_size >= 1:\n",
    "        return None\n",
    "    x1_train, x1_test, x2_train, x2_test = train_test_split(\n",
    "        x1, x2, test_size=test_size + val_size, random_state=seed\n",
    "    )\n",
    "    x1_val = None\n",
    "    x2_val = None\n",
    "    if val_size > 0:\n",
    "        x1_test, x1_val, x2_test, x2_val = train_test_split(\n",
    "            x1,\n",
    "            x2,\n",
    "            test_size=val_size / (test_size + val_size),\n",
    "            random_state=seed,\n",
    "        )\n",
    "    return x1_train, x1_val, x1_test, x2_train, x2_val, x2_test\n",
    "\n",
    "\n",
    "def create_dataset(x1, x2, vectorization=None):\n",
    "\n",
    "    if x1 is None or x2 is None:\n",
    "        return None \n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x1, x2))\n",
    "\n",
    "    if vectorization is None:\n",
    "        return dataset.map(lambda i, c: {'image': i, 'caption': c})\n",
    "    \n",
    "    else:\n",
    "        return dataset.map(lambda i, c: {'image': i, 'caption': vectorization(c)})\n",
    "    \n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    # convert input string to lowercase\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    # replace special characters with empty string\n",
    "    # TODO\n",
    "    #return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "    return lowercase\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-icpKfuyLSrH",
   "metadata": {
    "id": "-icpKfuyLSrH"
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cf13ac",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1685806019664,
     "user": {
      "displayName": "Ri ga",
      "userId": "15539171450533002544"
     },
     "user_tz": -120
    },
    "id": "f3cf13ac"
   },
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "\n",
    "image_dir = \"./resized_train\"\n",
    "caption_pred_file = \"caption_prediction_train.csv\"\n",
    "concept_det_file = \"concept_detection_train.csv\"\n",
    "concept_file = \"concepts.csv\"\n",
    "\n",
    "image_size = (128, 128, 3)\n",
    "\n",
    "batch_size = 10\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n1w8gAuILhCl",
   "metadata": {
    "id": "n1w8gAuILhCl"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8164b24",
   "metadata": {},
   "source": [
    "### File Reading and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36f4e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_df = pd.read_csv(caption_pred_file, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a05b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usually if we extract a string from the dataframe it will get truncated to \n",
    "# 50 characters. This way instead we select the longest string in the dataframe and\n",
    "# use that as max truncation. This means that no string will be truncated\n",
    "max_len = captions_df.caption.str.len().max()\n",
    "pd.set_option('display.max_colwidth', int(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b994c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ad [SOS] and [EOS] tokens at the beginning and end of every caption.\n",
    "captions_df['caption'] = captions_df['caption'].map('[SOS] {} [EOS]'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60333c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute size of vocabulary\n",
    "result = \"\"\n",
    "for i in captions_df['caption'].to_numpy():\n",
    "    result += \" \" + i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db85d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_df = pd.read_csv(caption_pred_file, sep='\\t')\n",
    "\n",
    "# Usually if we extract a string from the dataframe it will get truncated to \n",
    "# 50 characters. This way instead we select the longest string in the dataframe and\n",
    "# use that as max truncation. This means that no string will be truncated\n",
    "max_len = captions_df.caption.str.len().max()\n",
    "pd.set_option('display.max_colwidth', int(max_len))\n",
    "\n",
    "# Ad [SOS] and [EOS] tokens at the beginning and end of every caption.\n",
    "captions_df['caption'] = captions_df['caption'].map('[SOS] {} [EOS]'.format)\n",
    "\n",
    "# Compute size of vocabulary\n",
    "result = \"\"\n",
    "for i in captions_df['caption'].to_numpy():\n",
    "    result += \" \" + i\n",
    "result = custom_standardization(result)\n",
    "result = bytes.decode(result.numpy())\n",
    "vocab_size = len(set(result.split()))\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "# Compute longest sequence\n",
    "idx = captions_df.caption.str.len().idxmax()\n",
    "longest = captions_df['caption'][idx]\n",
    "longest = custom_standardization(longest)\n",
    "longest = bytes.decode(longest.numpy())\n",
    "longest = longest.split()\n",
    "sequence_length = len(longest)\n",
    "print(f\"Longest sequence: {sequence_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983d7bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the filenames of the images\n",
    "image_filenames = sorted(os.listdir(image_dir))\n",
    "num_images = len(image_filenames)\n",
    "\n",
    "# Pre-allocate the whole numpy array to store images\n",
    "images = np.zeros((num_images, image_size[0], image_size[1], image_size[2]), dtype=np.float16)\n",
    "\n",
    "captions = []\n",
    "\n",
    "# Iterate over the dataframe and match the images with captions\n",
    "for i, image_filename in enumerate(tqdm(image_filenames)):\n",
    "    \n",
    "    # Extract the image ID from the filename\n",
    "    image_id = image_filename.split('.')[0]\n",
    "\n",
    "    # Load image\n",
    "    image_path = image_dir + '/' + image_filename\n",
    "    image = tf.keras.preprocessing.image.load_img(image_path, target_size=(image_size[0], image_size[1]))\n",
    "    image = tf.keras.preprocessing.image.img_to_array(image, dtype='float16')\n",
    "    image /= 255.0\n",
    "\n",
    "    # Insert image in array\n",
    "    images[i] = image\n",
    "\n",
    "    # Find corresponding caption\n",
    "    caption = captions_df[captions_df['ID'] == image_id]['caption'].to_string(index=False)\n",
    "    captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963fd265",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, val_images, test_images, train_captions, val_captions, test_captions = split(\n",
    "        images, captions, test_size=0.2, seed=seed\n",
    "    )\n",
    "\n",
    "# Free unused memory\n",
    "del captions, images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203f993e",
   "metadata": {},
   "source": [
    "### Download Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83436f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocess = hub.KerasLayer(\n",
    "        \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2\",\n",
    "        name=\"text_preprocessing\",\n",
    "    )\n",
    "\n",
    "text_transformer = hub.KerasLayer(\n",
    "        \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\",\n",
    "        trainable=True,\n",
    "        name=\"bert\",\n",
    "    )\n",
    "\n",
    "img_preprocess = tfk.applications.convnext.preprocess_input\n",
    "\n",
    "img_supernet = tfk.applications.ConvNeXtTiny(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce46f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = create_dataset(train_images, train_captions)\n",
    "val_ds = create_dataset(val_images, val_captions)\n",
    "test_ds = create_dataset(test_images, test_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e870567",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_images, val_images, test_images, train_captions, val_captions, test_captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oiUz4hxwNRVS",
   "metadata": {
    "id": "oiUz4hxwNRVS"
   },
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WhVp1-YONWpY",
   "metadata": {
    "id": "WhVp1-YONWpY"
   },
   "source": [
    "### Network blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc14deb",
   "metadata": {
    "executionInfo": {
     "elapsed": 441,
     "status": "ok",
     "timestamp": 1685807496567,
     "user": {
      "displayName": "Ri ga",
      "userId": "15539171450533002544"
     },
     "user_tz": -120
    },
    "id": "2bc14deb"
   },
   "outputs": [],
   "source": [
    "def image_encoder(input_shape, latent_dim, embed_dim, seed=42, supernet=None, preprocessing=None):\n",
    "    \n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='img_input_layer')\n",
    "\n",
    "    x = preprocessing(input_layer)\n",
    "    x = supernet(x)\n",
    "\n",
    "    # Projection\n",
    "    x = tfkl.GlobalAveragePooling2D(name='GAP')(x)\n",
    "    x = tfkl.Dense(latent_dim, activation='relu')(x)\n",
    "    x = tfkl.Dense(embed_dim, name='img_embedding_output_layer')(x)\n",
    "\n",
    "    # Connect input and output through the Model class\n",
    "    cnn_encoder = tfk.Model(inputs=input_layer, outputs=x, name='image_encoder')\n",
    "\n",
    "    # Return the encoder\n",
    "    return cnn_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bc32b1",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1685807500205,
     "user": {
      "displayName": "Ri ga",
      "userId": "15539171450533002544"
     },
     "user_tz": -120
    },
    "id": "50bc32b1"
   },
   "outputs": [],
   "source": [
    "def text_encoder(latent_dim, embed_dim, preprocess, transformer, trainable=True):\n",
    "\n",
    "    transformer.trainable = trainable\n",
    "    \n",
    "    input_layer = tfkl.Input(shape=(), dtype=tf.string, name=\"text_input\")\n",
    "    x = preprocess(input_layer)\n",
    "    x = transformer(x)[\"pooled_output\"]\n",
    "    \n",
    "\n",
    "    # Projection\n",
    "    x = tfkl.Dense(latent_dim, activation='relu')(x)\n",
    "    x = tfkl.Dense(embed_dim, name='txt_embedding_output_layer')(x)\n",
    "\n",
    "    text_encoder = tfk.Model(inputs=input_layer, outputs=x, name=\"text_encoder\")\n",
    "    \n",
    "    return text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tQfOhkjPjz70",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1685807500648,
     "user": {
      "displayName": "Ri ga",
      "userId": "15539171450533002544"
     },
     "user_tz": -120
    },
    "id": "tQfOhkjPjz70"
   },
   "outputs": [],
   "source": [
    "class CLIP(tfk.Model):\n",
    "    def __init__(self, image_encoder, text_encoder, temp=0.07, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.temp = temp\n",
    "        self.loss_tracker = tfk.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "\n",
    "    def call(self, features, training=False):\n",
    "        image_emb = self.image_encoder(features[\"image\"], training=training)\n",
    "        text_emb = self.text_encoder(features[\"caption\"], training=training)\n",
    "        return image_emb, text_emb\n",
    "\n",
    "    def CLIP_loss(self, image_emb, text_emb):\n",
    "        norm_image_emb = tf.math.l2_normalize(image_emb, axis=1)\n",
    "        norm_text_emb = tf.math.l2_normalize(text_emb, axis=1)\n",
    "\n",
    "        logits = tf.linalg.matmul(norm_image_emb, norm_text_emb, transpose_b=True) * tf.math.exp(self.temp)\n",
    "\n",
    "        n = tf.shape(logits)[0]\n",
    "        labels = tf.range(n)\n",
    "\n",
    "        loss_img = tfk.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "        loss_txt = tfk.losses.sparse_categorical_crossentropy(labels, kb.transpose(logits), from_logits=True)\n",
    "\n",
    "        return (loss_img + loss_txt) / tf.constant(2.0)\n",
    "\n",
    "    def train_step(self, features):\n",
    "        with tf.GradientTape() as tape:\n",
    "            image_embeddings, caption_embeddings = self(features, training=True)\n",
    "            loss = self.CLIP_loss(caption_embeddings, image_embeddings)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, features):\n",
    "        image_embeddings, caption_embeddings = self(features, training=False)\n",
    "        loss = self.CLIP_loss(caption_embeddings, image_embeddings)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FWPNhGjzNbnP",
   "metadata": {
    "id": "FWPNhGjzNbnP"
   },
   "source": [
    "### Building network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mra2VO7JoqGj",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1685807502773,
     "user": {
      "displayName": "Ri ga",
      "userId": "15539171450533002544"
     },
     "user_tz": -120
    },
    "id": "mra2VO7JoqGj"
   },
   "outputs": [],
   "source": [
    "def build_clip(img_input_shape=(128,128,3),\n",
    "               txt_input_shape=(393, ), \n",
    "               latent_dim=1024, \n",
    "               embed_dim=128, \n",
    "               temp=0.07,\n",
    "               learning_rate=5e-5,\n",
    "               img_supernet=None,\n",
    "               img_preprocess=None,\n",
    "               text_transformer=None,\n",
    "               text_preprocess=None):\n",
    "\n",
    "    \n",
    "    text_encoder_model = text_encoder(latent_dim, embed_dim, text_preprocess, text_transformer)\n",
    "    image_encoder_model = image_encoder(img_input_shape, latent_dim, embed_dim, supernet=img_supernet, preprocessing=img_preprocess)\n",
    "\n",
    "    clip = CLIP(image_encoder_model, text_encoder_model, temp)\n",
    "    clip.compile(optimizer = tf.optimizers.Adam(learning_rate=learning_rate))\n",
    "\n",
    "    return image_encoder_model, text_encoder_model, clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0778191",
   "metadata": {
    "executionInfo": {
     "elapsed": 564,
     "status": "ok",
     "timestamp": 1685807504408,
     "user": {
      "displayName": "Ri ga",
      "userId": "15539171450533002544"
     },
     "user_tz": -120
    },
    "id": "f0778191"
   },
   "outputs": [],
   "source": [
    "image_encoder, text_encoder, clip = build_clip(\n",
    "    img_supernet=img_supernet,\n",
    "    img_preprocess=img_preprocess,\n",
    "    text_transformer=text_transformer,\n",
    "    text_preprocess=text_preprocess,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mPnccec1Nfwh",
   "metadata": {
    "id": "mPnccec1Nfwh"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eqrFQi0ZNhvo",
   "metadata": {
    "id": "eqrFQi0ZNhvo"
   },
   "source": [
    "### CLIP pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jS2cVFlVrHLs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "executionInfo": {
     "elapsed": 1920448,
     "status": "error",
     "timestamp": 1685809433717,
     "user": {
      "displayName": "Ri ga",
      "userId": "15539171450533002544"
     },
     "user_tz": -120
    },
    "id": "jS2cVFlVrHLs",
    "outputId": "fc619277-db77-4c8a-f742-c0a373c094b7"
   },
   "outputs": [],
   "source": [
    "# Create a learning rate scheduler callback.\n",
    "reduce_lr = tfk.callbacks.ReduceLROnPlateau(\n",
    "    monitor = \"val_loss\", factor = 0.2, patience = 3\n",
    ")\n",
    "\n",
    "# Create an early stopping callback.\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = \"val_loss\", patience = 5, restore_best_weights = True\n",
    ")\n",
    "\n",
    "\n",
    "history = clip.fit(\n",
    "    train_ds.batch(batch_size),\n",
    "    epochs = epochs,\n",
    "    validation_data = test_ds.batch(batch_size),\n",
    "    callbacks = [reduce_lr, early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ovNQyvWYy8g4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 10294,
     "status": "ok",
     "timestamp": 1676839846200,
     "user": {
      "displayName": "Ri ga",
      "userId": "15539171450533002544"
     },
     "user_tz": -60
    },
    "id": "ovNQyvWYy8g4",
    "outputId": "62de2a46-d1e4-4280-825c-a2baf08f90de"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"train\", \"valid\"], loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8WqAGCTRJYxn",
   "metadata": {
    "id": "8WqAGCTRJYxn"
   },
   "outputs": [],
   "source": [
    "clip.save(\"keras_clip_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jaCktt68IRcq",
   "metadata": {
    "id": "jaCktt68IRcq"
   },
   "source": [
    "### Encoder/Decoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Dpv8yhO1Ih2j",
   "metadata": {
    "id": "Dpv8yhO1Ih2j"
   },
   "outputs": [],
   "source": [
    "def read_image(image_path):\n",
    "    image_array = tf.image.decode_jpeg(tf.io.read_file(image_path), channels=3)\n",
    "    return image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E5uDwwUkJE5u",
   "metadata": {
    "id": "E5uDwwUkJE5u"
   },
   "outputs": [],
   "source": [
    "def find_matches(image_embeddings, queries, k=5, normalize=True):\n",
    "    queries_vec = [text_vectorization(query) for query in queries]\n",
    "    queries_vec = tf.data.Dataset.from_tensor_slices(queries_vec).batch(batch_size)\n",
    "    # Get the embedding for the query.\n",
    "    query_embedding = text_encoder.predict(queries_vec)\n",
    "    # Normalize the query and the image embeddings.\n",
    "    if normalize:\n",
    "        image_embeddings = tf.math.l2_normalize(image_embeddings, axis=1)\n",
    "        query_embedding = tf.math.l2_normalize(query_embedding, axis=1)\n",
    "    # Compute the dot product between the query and the image embeddings.\n",
    "    dot_similarity = tf.matmul(query_embedding, image_embeddings, transpose_b=True)\n",
    "    # Retrieve top k indices.\n",
    "    results = tf.math.top_k(dot_similarity, k).indices.numpy()\n",
    "    # Return matching image paths.\n",
    "    return [[train_image_paths[idx] for idx in indices] for indices in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eWCwgWo_zMf1",
   "metadata": {
    "id": "eWCwgWo_zMf1"
   },
   "outputs": [],
   "source": [
    "train_data = [p for p in ds_train]\n",
    "val_data = [p for p in ds_val]\n",
    "test_data = [p for p in ds_test]\n",
    "\n",
    "train_image_paths = [e[\"image path\"] for e in train_data]\n",
    "test_image_paths = [e[\"image path\"] for e in test_data]\n",
    "\n",
    "# TODO: this part only generates embeddings on the training dataset for now\n",
    "# TODO: this code re-reads the images\n",
    "image_embeddings = image_encoder.predict(\n",
    "    tf.data.Dataset.from_tensor_slices(train_image_paths).map(read_image).batch(batch_size),\n",
    "    verbose=1,\n",
    ")\n",
    "print(f\"Image embeddings shape: {image_embeddings.shape}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zWFOxWhV0kbF",
   "metadata": {
    "id": "zWFOxWhV0kbF"
   },
   "outputs": [],
   "source": [
    "query = \"active pheochromocytoma\"\n",
    "matches = find_matches(image_embeddings, [query], normalize=True)[0]\n",
    "\n",
    "print(\"Top matches for query: \\\"\" + query + \"\\\"\")\n",
    "\n",
    "plt.figure(figsize=(18, 18))\n",
    "for i in range(9):\n",
    "    path = matches[i].numpy().decode('UTF-8')\n",
    "    caption = next(x[\"raw caption\"] for x in train_data if x[\"image path\"].numpy().decode('UTF-8') == path)\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(mpimg.imread(path))\n",
    "    plt.axis(\"off\")\n",
    "    print(caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Jor40RYWJefh",
   "metadata": {
    "id": "Jor40RYWJefh"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X3HGLJ8w0ZdF",
   "metadata": {
    "id": "X3HGLJ8w0ZdF"
   },
   "outputs": [],
   "source": [
    "# TODO: might not work, needs revising\n",
    "\n",
    "def compute_top_k_accuracy(image_paths, k=5):\n",
    "    hits = 0\n",
    "    num_batches = int(np.ceil(len(image_paths) / batch_size))\n",
    "    for idx in range(num_batches):\n",
    "        start_idx = idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        current_image_paths = image_paths[start_idx:end_idx]\n",
    "        queries = [ captions[os.path.splitext(image_path.numpy().decode('UTF-8').split(os.sep)[-1])[0]] for image_path in current_image_paths ]\n",
    "        result = find_matches(image_embeddings, queries, k)\n",
    "        hits += sum(\n",
    "            [\n",
    "                image_path in matches for (image_path, matches) in list(zip(current_image_paths, result))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return hits / len(image_paths)\n",
    "\n",
    "n = 1920\n",
    "\n",
    "print(test_image_paths)\n",
    "\n",
    "print(\"Scoring training data...\")\n",
    "train_accuracy = compute_top_k_accuracy(random.sample(train_image_paths, n))\n",
    "print(f\"Train accuracy: {round(train_accuracy * 100, 3)}%\")\n",
    "\n",
    "print(\"Scoring evaluation data...\")\n",
    "eval_accuracy = compute_top_k_accuracy(random.sample(test_image_paths, n))\n",
    "print(f\"Eval accuracy: {round(eval_accuracy * 100, 3)}%\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "jaCktt68IRcq",
    "Jor40RYWJefh"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "5380e256bec2a872a4245067cef5da364603399a350ba03e757739343e36dd55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

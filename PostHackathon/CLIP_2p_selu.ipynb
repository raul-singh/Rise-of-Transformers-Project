{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "xffJLUZCLC51",
   "metadata": {
    "id": "xffJLUZCLC51"
   },
   "source": [
    "## Declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7TgHFX6XLQGl",
   "metadata": {
    "id": "7TgHFX6XLQGl"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d093180",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4674,
     "status": "ok",
     "timestamp": 1685806019663,
     "user": {
      "displayName": "Ri ga",
      "userId": "15539171450533002544"
     },
     "user_tz": -120
    },
    "id": "2d093180",
    "outputId": "bc84c9fd-b81e-49b9-d8a7-0c7e03c595e0",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "kb = tf.keras.backend\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1c5e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x1, x2, test_size=0.2, val_size=0.0, seed=0):\n",
    "    if val_size + test_size >= 1:\n",
    "        return None\n",
    "    x1_train, x1_test, x2_train, x2_test = train_test_split(\n",
    "        x1, x2, test_size=test_size + val_size, random_state=seed\n",
    "    )\n",
    "    x1_val = None\n",
    "    x2_val = None\n",
    "    if val_size > 0:\n",
    "        x1_test, x1_val, x2_test, x2_val = train_test_split(\n",
    "            x1,\n",
    "            x2,\n",
    "            test_size=val_size / (test_size + val_size),\n",
    "            random_state=seed,\n",
    "        )\n",
    "    return x1_train, x1_val, x1_test, x2_train, x2_val, x2_test\n",
    "\n",
    "\n",
    "def create_dataset(x1, x2, vectorization=None):\n",
    "\n",
    "    if x1 is None or x2 is None:\n",
    "        return None \n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x1, x2))\n",
    "\n",
    "    if vectorization is None:\n",
    "        return dataset.map(lambda i, c: {'image': i, 'caption': c})\n",
    "    \n",
    "    else:\n",
    "        return dataset.map(lambda i, c: {'image': i, 'caption': vectorization(c)})\n",
    "    \n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    # convert input string to lowercase\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    # replace special characters with empty string\n",
    "    # TODO\n",
    "    #return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "    return lowercase\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-icpKfuyLSrH",
   "metadata": {
    "id": "-icpKfuyLSrH"
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3cf13ac",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1685806019664,
     "user": {
      "displayName": "Ri ga",
      "userId": "15539171450533002544"
     },
     "user_tz": -120
    },
    "id": "f3cf13ac"
   },
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "\n",
    "image_dir = \"./resized_train\"\n",
    "caption_pred_file = \"caption_prediction_train.csv\"\n",
    "concept_det_file = \"concept_detection_train.csv\"\n",
    "concept_file = \"concepts.csv\"\n",
    "\n",
    "image_size = (128, 128, 3)\n",
    "\n",
    "batch_size = 10\n",
    "epochs = 100\n",
    "learning_rate = 2e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n1w8gAuILhCl",
   "metadata": {
    "id": "n1w8gAuILhCl"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8164b24",
   "metadata": {},
   "source": [
    "### File Reading and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c36f4e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_df = pd.read_csv(caption_pred_file, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa7a05b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usually if we extract a string from the dataframe it will get truncated to \n",
    "# 50 characters. This way instead we select the longest string in the dataframe and\n",
    "# use that as max truncation. This means that no string will be truncated\n",
    "max_len = captions_df.caption.str.len().max()\n",
    "pd.set_option('display.max_colwidth', int(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b994c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ad [SOS] and [EOS] tokens at the beginning and end of every caption.\n",
    "captions_df['caption'] = captions_df['caption'].map('[SOS] {} [EOS]'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f60333c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute size of vocabulary\n",
    "result = \"\"\n",
    "for i in captions_df['caption'].to_numpy():\n",
    "    result += \" \" + i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db85d65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 35491\n",
      "Longest sequence: 393\n"
     ]
    }
   ],
   "source": [
    "captions_df = pd.read_csv(caption_pred_file, sep='\\t')\n",
    "\n",
    "# Usually if we extract a string from the dataframe it will get truncated to \n",
    "# 50 characters. This way instead we select the longest string in the dataframe and\n",
    "# use that as max truncation. This means that no string will be truncated\n",
    "max_len = captions_df.caption.str.len().max()\n",
    "pd.set_option('display.max_colwidth', int(max_len))\n",
    "\n",
    "# Ad [SOS] and [EOS] tokens at the beginning and end of every caption.\n",
    "captions_df['caption'] = captions_df['caption'].map('[SOS] {} [EOS]'.format)\n",
    "\n",
    "# Compute size of vocabulary\n",
    "result = \"\"\n",
    "for i in captions_df['caption'].to_numpy():\n",
    "    result += \" \" + i\n",
    "result = custom_standardization(result)\n",
    "result = bytes.decode(result.numpy())\n",
    "vocab_size = len(set(result.split()))\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "# Compute longest sequence\n",
    "idx = captions_df.caption.str.len().idxmax()\n",
    "longest = captions_df['caption'][idx]\n",
    "longest = custom_standardization(longest)\n",
    "longest = bytes.decode(longest.numpy())\n",
    "longest = longest.split()\n",
    "sequence_length = len(longest)\n",
    "print(f\"Longest sequence: {sequence_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "983d7bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83275/83275 [06:13<00:00, 222.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get the filenames of the images\n",
    "image_filenames = sorted(os.listdir(image_dir))\n",
    "num_images = len(image_filenames)\n",
    "\n",
    "# Pre-allocate the whole numpy array to store images\n",
    "images = np.zeros((num_images, image_size[0], image_size[1], image_size[2]), dtype=np.float16)\n",
    "\n",
    "captions = []\n",
    "\n",
    "# Iterate over the dataframe and match the images with captions\n",
    "for i, image_filename in enumerate(tqdm(image_filenames)):\n",
    "    \n",
    "    # Extract the image ID from the filename\n",
    "    image_id = image_filename.split('.')[0]\n",
    "\n",
    "    # Load image\n",
    "    image_path = image_dir + '/' + image_filename\n",
    "    image = tf.keras.preprocessing.image.load_img(image_path, target_size=(image_size[0], image_size[1]))\n",
    "    image = tf.keras.preprocessing.image.img_to_array(image, dtype='float16')\n",
    "    image /= 255.0\n",
    "\n",
    "    # Insert image in array\n",
    "    images[i] = image\n",
    "\n",
    "    # Find corresponding caption\n",
    "    caption = captions_df[captions_df['ID'] == image_id]['caption'].to_string(index=False)\n",
    "    captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "963fd265",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, val_images, test_images, train_captions, val_captions, test_captions = split(\n",
    "        images, captions, test_size=0.2, seed=seed\n",
    "    )\n",
    "\n",
    "# Free unused memory\n",
    "del captions, images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203f993e",
   "metadata": {},
   "source": [
    "### Download Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b83436f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocess = hub.KerasLayer(\n",
    "        \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2\",\n",
    "        name=\"text_preprocessing\",\n",
    "    )\n",
    "\n",
    "text_transformer = hub.KerasLayer(\n",
    "        \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\",\n",
    "        trainable=True,\n",
    "        name=\"bert\",\n",
    "    )\n",
    "\n",
    "img_preprocess = tfk.applications.convnext.preprocess_input\n",
    "\n",
    "img_supernet = tfk.applications.ConvNeXtTiny(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fce46f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = create_dataset(train_images, train_captions)\n",
    "val_ds = create_dataset(val_images, val_captions)\n",
    "test_ds = create_dataset(test_images, test_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e870567",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_images, val_images, test_images, train_captions, val_captions, test_captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oiUz4hxwNRVS",
   "metadata": {
    "id": "oiUz4hxwNRVS"
   },
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WhVp1-YONWpY",
   "metadata": {
    "id": "WhVp1-YONWpY"
   },
   "source": [
    "### Network blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bc14deb",
   "metadata": {
    "executionInfo": {
     "elapsed": 441,
     "status": "ok",
     "timestamp": 1685807496567,
     "user": {
      "displayName": "Ri ga",
      "userId": "15539171450533002544"
     },
     "user_tz": -120
    },
    "id": "2bc14deb"
   },
   "outputs": [],
   "source": [
    "def image_encoder(input_shape, latent_dim, embed_dim, seed=42, supernet=None, preprocessing=None):\n",
    "    \n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='img_input_layer')\n",
    "\n",
    "    x = preprocessing(input_layer)\n",
    "    x = supernet(x)\n",
    "\n",
    "    x = tfkl.GlobalAveragePooling2D(name='GAP')(x)\n",
    "\n",
    "    # Projection\n",
    "    embeddings = tfkl.Dense(embed_dim)(x)\n",
    "    x = tf.nn.selu(embeddings)\n",
    "    x = tfkl.Dense(embed_dim, name='img_embedding_output_layer')(x)\n",
    "    x = tfkl.Dropout(0.1)(x)\n",
    "    x = tfkl.Add()([x, embeddings])\n",
    "    x = tfkl.LayerNormalization()(x)\n",
    "\n",
    "    # Connect input and output through the Model class\n",
    "    cnn_encoder = tfk.Model(inputs=input_layer, outputs=x, name='image_encoder')\n",
    "\n",
    "    # Return the encoder\n",
    "    return cnn_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50bc32b1",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1685807500205,
     "user": {
      "displayName": "Ri ga",
      "userId": "15539171450533002544"
     },
     "user_tz": -120
    },
    "id": "50bc32b1"
   },
   "outputs": [],
   "source": [
    "def text_encoder(latent_dim, embed_dim, preprocess, transformer, trainable=True):\n",
    "\n",
    "    transformer.trainable = trainable\n",
    "    \n",
    "    input_layer = tfkl.Input(shape=(), dtype=tf.string, name=\"text_input\")\n",
    "    x = preprocess(input_layer)\n",
    "    x = transformer(x)[\"pooled_output\"]\n",
    "    \n",
    "\n",
    "    # Projection\n",
    "    embeddings = tfkl.Dense(embed_dim)(x)\n",
    "    x = tf.nn.selu(embeddings)\n",
    "    x = tfkl.Dense(embed_dim, name='img_embedding_output_layer')(x)\n",
    "    x = tfkl.Dropout(0.1)(x)\n",
    "    x = tfkl.Add()([x, embeddings])\n",
    "    x = tfkl.LayerNormalization()(x)\n",
    "\n",
    "    text_encoder = tfk.Model(inputs=input_layer, outputs=x, name=\"text_encoder\")\n",
    "    \n",
    "    return text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "tQfOhkjPjz70",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1685807500648,
     "user": {
      "displayName": "Ri ga",
      "userId": "15539171450533002544"
     },
     "user_tz": -120
    },
    "id": "tQfOhkjPjz70"
   },
   "outputs": [],
   "source": [
    "class CLIP(tfk.Model):\n",
    "    def __init__(self, image_encoder, text_encoder, temp=0.07, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.temp = temp\n",
    "        self.loss_tracker = tfk.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "\n",
    "    def call(self, features, training=False):\n",
    "        image_emb = self.image_encoder(features[\"image\"], training=training)\n",
    "        text_emb = self.text_encoder(features[\"caption\"], training=training)\n",
    "        return image_emb, text_emb\n",
    "\n",
    "    def CLIP_loss(self, image_emb, text_emb):\n",
    "        norm_image_emb = tf.math.l2_normalize(image_emb, axis=1)\n",
    "        norm_text_emb = tf.math.l2_normalize(text_emb, axis=1)\n",
    "\n",
    "        logits = tf.linalg.matmul(norm_image_emb, norm_text_emb, transpose_b=True) * tf.math.exp(self.temp)\n",
    "\n",
    "        n = tf.shape(logits)[0]\n",
    "        labels = tf.range(n)\n",
    "\n",
    "        loss_img = tfk.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "        loss_txt = tfk.losses.sparse_categorical_crossentropy(labels, kb.transpose(logits), from_logits=True)\n",
    "\n",
    "        return (loss_img + loss_txt) / tf.constant(2.0)\n",
    "\n",
    "    def train_step(self, features):\n",
    "        with tf.GradientTape() as tape:\n",
    "            image_embeddings, caption_embeddings = self(features, training=True)\n",
    "            loss = self.CLIP_loss(caption_embeddings, image_embeddings)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, features):\n",
    "        image_embeddings, caption_embeddings = self(features, training=False)\n",
    "        loss = self.CLIP_loss(caption_embeddings, image_embeddings)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FWPNhGjzNbnP",
   "metadata": {
    "id": "FWPNhGjzNbnP"
   },
   "source": [
    "### Building network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "mra2VO7JoqGj",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1685807502773,
     "user": {
      "displayName": "Ri ga",
      "userId": "15539171450533002544"
     },
     "user_tz": -120
    },
    "id": "mra2VO7JoqGj"
   },
   "outputs": [],
   "source": [
    "def build_clip(img_input_shape=(128,128,3),\n",
    "               txt_input_shape=(393, ), \n",
    "               latent_dim=1024, \n",
    "               embed_dim=64, \n",
    "               temp=0.07,\n",
    "               learning_rate=2e-5,\n",
    "               img_supernet=None,\n",
    "               img_preprocess=None,\n",
    "               text_transformer=None,\n",
    "               text_preprocess=None):\n",
    "\n",
    "    \n",
    "    text_encoder_model = text_encoder(latent_dim, embed_dim, text_preprocess, text_transformer)\n",
    "    image_encoder_model = image_encoder(img_input_shape, latent_dim, embed_dim, supernet=img_supernet, preprocessing=img_preprocess)\n",
    "\n",
    "    clip = CLIP(image_encoder_model, text_encoder_model, temp)\n",
    "    clip.compile(optimizer = tf.optimizers.Adam(learning_rate=learning_rate))\n",
    "\n",
    "    return image_encoder_model, text_encoder_model, clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0778191",
   "metadata": {
    "executionInfo": {
     "elapsed": 564,
     "status": "ok",
     "timestamp": 1685807504408,
     "user": {
      "displayName": "Ri ga",
      "userId": "15539171450533002544"
     },
     "user_tz": -120
    },
    "id": "f0778191"
   },
   "outputs": [],
   "source": [
    "image_encoder, text_encoder, clip = build_clip(\n",
    "    img_supernet=img_supernet,\n",
    "    img_preprocess=img_preprocess,\n",
    "    text_transformer=text_transformer,\n",
    "    text_preprocess=text_preprocess,\n",
    "    learning_rate=learning_rate\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mPnccec1Nfwh",
   "metadata": {
    "id": "mPnccec1Nfwh"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eqrFQi0ZNhvo",
   "metadata": {
    "id": "eqrFQi0ZNhvo"
   },
   "source": [
    "### Phase 1\n",
    "Traning all the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "jS2cVFlVrHLs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "executionInfo": {
     "elapsed": 1920448,
     "status": "error",
     "timestamp": 1685809433717,
     "user": {
      "displayName": "Ri ga",
      "userId": "15539171450533002544"
     },
     "user_tz": -120
    },
    "id": "jS2cVFlVrHLs",
    "outputId": "fc619277-db77-4c8a-f742-c0a373c094b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6662/6662 [==============================] - 493s 70ms/step - loss: 1.8453 - val_loss: 1.7071 - lr: 2.0000e-05\n",
      "Epoch 2/100\n",
      "6662/6662 [==============================] - 435s 65ms/step - loss: 1.6834 - val_loss: 1.6727 - lr: 2.0000e-05\n",
      "Epoch 3/100\n",
      "6662/6662 [==============================] - 437s 66ms/step - loss: 1.6334 - val_loss: 1.6424 - lr: 2.0000e-05\n",
      "Epoch 4/100\n",
      "6662/6662 [==============================] - 434s 65ms/step - loss: 1.6019 - val_loss: 1.6305 - lr: 2.0000e-05\n",
      "Epoch 5/100\n",
      "6662/6662 [==============================] - 446s 67ms/step - loss: 1.5800 - val_loss: 1.6280 - lr: 2.0000e-05\n",
      "Epoch 6/100\n",
      "6662/6662 [==============================] - 445s 67ms/step - loss: 1.5639 - val_loss: 1.6235 - lr: 2.0000e-05\n",
      "Epoch 7/100\n",
      "6662/6662 [==============================] - 444s 67ms/step - loss: 1.5505 - val_loss: 1.6185 - lr: 2.0000e-05\n",
      "Epoch 8/100\n",
      "6662/6662 [==============================] - 442s 66ms/step - loss: 1.5391 - val_loss: 1.6290 - lr: 2.0000e-05\n",
      "Epoch 9/100\n",
      "6662/6662 [==============================] - 443s 66ms/step - loss: 1.5297 - val_loss: 1.6304 - lr: 2.0000e-05\n",
      "Epoch 10/100\n",
      "6662/6662 [==============================] - 439s 66ms/step - loss: 1.5223 - val_loss: 1.6140 - lr: 2.0000e-05\n",
      "Epoch 11/100\n",
      "6662/6662 [==============================] - 439s 66ms/step - loss: 1.5149 - val_loss: 1.6206 - lr: 2.0000e-05\n",
      "Epoch 12/100\n",
      "6662/6662 [==============================] - 437s 66ms/step - loss: 1.5103 - val_loss: 1.6199 - lr: 2.0000e-05\n",
      "Epoch 13/100\n",
      "6662/6662 [==============================] - 436s 65ms/step - loss: 1.5059 - val_loss: 1.6237 - lr: 2.0000e-05\n",
      "Epoch 14/100\n",
      "6662/6662 [==============================] - 437s 66ms/step - loss: 1.4969 - val_loss: 1.6195 - lr: 4.0000e-06\n",
      "Epoch 15/100\n",
      "6662/6662 [==============================] - 435s 65ms/step - loss: 1.4910 - val_loss: 1.6236 - lr: 4.0000e-06\n"
     ]
    }
   ],
   "source": [
    "# Create a learning rate scheduler callback.\n",
    "reduce_lr = tfk.callbacks.ReduceLROnPlateau(\n",
    "    monitor = \"val_loss\", factor = 0.2, patience = 3\n",
    ")\n",
    "\n",
    "# Create an early stopping callback.\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = \"val_loss\", patience = 5, restore_best_weights = True\n",
    ")\n",
    "\n",
    "\n",
    "history_phase1 = clip.fit(\n",
    "    train_ds.batch(batch_size),\n",
    "    epochs = epochs,\n",
    "    validation_data = test_ds.batch(batch_size),\n",
    "    callbacks = [reduce_lr, early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4185ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    }
   ],
   "source": [
    "clip.save(\"selu.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dc99dc",
   "metadata": {},
   "source": [
    "### Phase 2\n",
    "Training the projection only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ovNQyvWYy8g4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 10294,
     "status": "ok",
     "timestamp": 1676839846200,
     "user": {
      "displayName": "Ri ga",
      "userId": "15539171450533002544"
     },
     "user_tz": -60
    },
    "id": "ovNQyvWYy8g4",
    "outputId": "62de2a46-d1e4-4280-825c-a2baf08f90de"
   },
   "outputs": [],
   "source": [
    "img_supernet.trainable = False\n",
    "text_transformer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba75f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.compile(optimizer = tf.optimizers.Adam(learning_rate=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37b641ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6662/6662 [==============================] - 200s 29ms/step - loss: 1.5165 - val_loss: 1.6133\n",
      "Epoch 2/100\n",
      "6662/6662 [==============================] - 192s 29ms/step - loss: 1.5158 - val_loss: 1.6132\n",
      "Epoch 3/100\n",
      "6662/6662 [==============================] - 191s 29ms/step - loss: 1.5154 - val_loss: 1.6131\n",
      "Epoch 4/100\n",
      "6662/6662 [==============================] - 190s 28ms/step - loss: 1.5150 - val_loss: 1.6130\n",
      "Epoch 5/100\n",
      "6662/6662 [==============================] - 190s 29ms/step - loss: 1.5148 - val_loss: 1.6129\n",
      "Epoch 6/100\n",
      "6662/6662 [==============================] - 191s 29ms/step - loss: 1.5146 - val_loss: 1.6128\n",
      "Epoch 7/100\n",
      "6662/6662 [==============================] - 191s 29ms/step - loss: 1.5144 - val_loss: 1.6127\n",
      "Epoch 8/100\n",
      "6662/6662 [==============================] - 191s 29ms/step - loss: 1.5142 - val_loss: 1.6127\n",
      "Epoch 9/100\n",
      "6662/6662 [==============================] - 191s 29ms/step - loss: 1.5140 - val_loss: 1.6127\n",
      "Epoch 10/100\n",
      "6662/6662 [==============================] - 190s 29ms/step - loss: 1.5139 - val_loss: 1.6126\n",
      "Epoch 11/100\n",
      "6662/6662 [==============================] - 191s 29ms/step - loss: 1.5137 - val_loss: 1.6126\n",
      "Epoch 12/100\n",
      "6662/6662 [==============================] - 189s 28ms/step - loss: 1.5136 - val_loss: 1.6125\n",
      "Epoch 13/100\n",
      "6662/6662 [==============================] - 190s 28ms/step - loss: 1.5135 - val_loss: 1.6125\n",
      "Epoch 14/100\n",
      "6662/6662 [==============================] - 191s 29ms/step - loss: 1.5134 - val_loss: 1.6125\n",
      "Epoch 15/100\n",
      "6662/6662 [==============================] - 189s 28ms/step - loss: 1.5133 - val_loss: 1.6124\n",
      "Epoch 16/100\n",
      "6662/6662 [==============================] - 189s 28ms/step - loss: 1.5132 - val_loss: 1.6124\n",
      "Epoch 17/100\n",
      "6662/6662 [==============================] - 191s 29ms/step - loss: 1.5131 - val_loss: 1.6124\n",
      "Epoch 18/100\n",
      "6662/6662 [==============================] - 191s 29ms/step - loss: 1.5130 - val_loss: 1.6124\n",
      "Epoch 19/100\n",
      "6662/6662 [==============================] - 190s 28ms/step - loss: 1.5130 - val_loss: 1.6123\n",
      "Epoch 20/100\n",
      "6662/6662 [==============================] - 189s 28ms/step - loss: 1.5129 - val_loss: 1.6123\n",
      "Epoch 21/100\n",
      "6662/6662 [==============================] - 189s 28ms/step - loss: 1.5128 - val_loss: 1.6123\n",
      "Epoch 22/100\n",
      "6662/6662 [==============================] - 191s 29ms/step - loss: 1.5127 - val_loss: 1.6122\n",
      "Epoch 23/100\n",
      "6662/6662 [==============================] - 189s 28ms/step - loss: 1.5127 - val_loss: 1.6122\n",
      "Epoch 24/100\n",
      "6662/6662 [==============================] - 189s 28ms/step - loss: 1.5126 - val_loss: 1.6122\n",
      "Epoch 25/100\n",
      "6662/6662 [==============================] - 190s 29ms/step - loss: 1.5126 - val_loss: 1.6122\n",
      "Epoch 26/100\n",
      "6662/6662 [==============================] - 194s 29ms/step - loss: 1.5125 - val_loss: 1.6121\n",
      "Epoch 27/100\n",
      "6662/6662 [==============================] - 194s 29ms/step - loss: 1.5124 - val_loss: 1.6121\n",
      "Epoch 28/100\n",
      "6662/6662 [==============================] - 194s 29ms/step - loss: 1.5124 - val_loss: 1.6121\n",
      "Epoch 29/100\n",
      "6662/6662 [==============================] - 192s 29ms/step - loss: 1.5123 - val_loss: 1.6121\n",
      "Epoch 30/100\n",
      "6662/6662 [==============================] - 194s 29ms/step - loss: 1.5123 - val_loss: 1.6121\n",
      "Epoch 31/100\n",
      "6662/6662 [==============================] - 193s 29ms/step - loss: 1.5122 - val_loss: 1.6121\n",
      "Epoch 32/100\n",
      "6662/6662 [==============================] - 193s 29ms/step - loss: 1.5122 - val_loss: 1.6121\n",
      "Epoch 33/100\n",
      "6662/6662 [==============================] - 192s 29ms/step - loss: 1.5121 - val_loss: 1.6121\n",
      "Epoch 34/100\n",
      "6662/6662 [==============================] - 193s 29ms/step - loss: 1.5121 - val_loss: 1.6121\n",
      "Epoch 35/100\n",
      "6662/6662 [==============================] - 192s 29ms/step - loss: 1.5120 - val_loss: 1.6121\n",
      "Epoch 36/100\n",
      "6662/6662 [==============================] - 191s 29ms/step - loss: 1.5120 - val_loss: 1.6120\n",
      "Epoch 37/100\n",
      "6662/6662 [==============================] - 193s 29ms/step - loss: 1.5119 - val_loss: 1.6121\n",
      "Epoch 38/100\n",
      "6662/6662 [==============================] - 192s 29ms/step - loss: 1.5119 - val_loss: 1.6120\n",
      "Epoch 39/100\n",
      "6662/6662 [==============================] - 192s 29ms/step - loss: 1.5118 - val_loss: 1.6120\n",
      "Epoch 40/100\n",
      "6662/6662 [==============================] - 192s 29ms/step - loss: 1.5118 - val_loss: 1.6120\n",
      "Epoch 41/100\n",
      "6662/6662 [==============================] - 190s 29ms/step - loss: 1.5118 - val_loss: 1.6120\n",
      "Epoch 42/100\n",
      "6662/6662 [==============================] - 191s 29ms/step - loss: 1.5117 - val_loss: 1.6120\n",
      "Epoch 43/100\n",
      "6662/6662 [==============================] - 190s 29ms/step - loss: 1.5117 - val_loss: 1.6120\n",
      "Epoch 44/100\n",
      "6662/6662 [==============================] - 190s 29ms/step - loss: 1.5117 - val_loss: 1.6120\n",
      "Epoch 45/100\n",
      "6662/6662 [==============================] - 191s 29ms/step - loss: 1.5116 - val_loss: 1.6120\n",
      "Epoch 46/100\n",
      "6662/6662 [==============================] - 189s 28ms/step - loss: 1.5116 - val_loss: 1.6120\n",
      "Epoch 47/100\n",
      "6662/6662 [==============================] - 188s 28ms/step - loss: 1.5115 - val_loss: 1.6120\n",
      "Epoch 48/100\n",
      "6662/6662 [==============================] - 191s 29ms/step - loss: 1.5115 - val_loss: 1.6120\n",
      "Epoch 49/100\n",
      "6662/6662 [==============================] - 191s 29ms/step - loss: 1.5115 - val_loss: 1.6120\n",
      "Epoch 50/100\n",
      "6662/6662 [==============================] - 190s 28ms/step - loss: 1.5115 - val_loss: 1.6120\n",
      "Epoch 51/100\n",
      "6662/6662 [==============================] - 191s 29ms/step - loss: 1.5114 - val_loss: 1.6120\n",
      "Epoch 52/100\n",
      "6662/6662 [==============================] - 191s 29ms/step - loss: 1.5114 - val_loss: 1.6120\n",
      "Epoch 53/100\n",
      "6662/6662 [==============================] - 189s 28ms/step - loss: 1.5113 - val_loss: 1.6120\n",
      "Epoch 54/100\n",
      "6662/6662 [==============================] - 191s 29ms/step - loss: 1.5113 - val_loss: 1.6120\n",
      "Epoch 55/100\n",
      "6662/6662 [==============================] - 191s 29ms/step - loss: 1.5113 - val_loss: 1.6120\n",
      "Epoch 56/100\n",
      "6662/6662 [==============================] - 192s 29ms/step - loss: 1.5113 - val_loss: 1.6120\n"
     ]
    }
   ],
   "source": [
    "history_phase2 = clip.fit(\n",
    "    train_ds.batch(batch_size),\n",
    "    epochs = epochs,\n",
    "    validation_data = test_ds.batch(batch_size),\n",
    "    callbacks = [early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84ef9cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_phase1.history[\"loss\"]+history_phase2.history[\"loss\"])\n",
    "plt.plot(history_phase1.history[\"val_loss\"]+history_phase2.history[\"val_loss\"])\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"train\", \"valid\"], loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8WqAGCTRJYxn",
   "metadata": {
    "id": "8WqAGCTRJYxn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    }
   ],
   "source": [
    "clip.save(\"models/2phase_selu.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jaCktt68IRcq",
   "metadata": {
    "id": "jaCktt68IRcq"
   },
   "source": [
    "### Encoder/Decoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Dpv8yhO1Ih2j",
   "metadata": {
    "id": "Dpv8yhO1Ih2j"
   },
   "outputs": [],
   "source": [
    "def read_image(image_path):\n",
    "    image_array = tf.image.decode_jpeg(tf.io.read_file(image_path), channels=3)\n",
    "    return image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E5uDwwUkJE5u",
   "metadata": {
    "id": "E5uDwwUkJE5u"
   },
   "outputs": [],
   "source": [
    "def find_matches(image_embeddings, queries, k=5, normalize=True):\n",
    "    queries_vec = [text_vectorization(query) for query in queries]\n",
    "    queries_vec = tf.data.Dataset.from_tensor_slices(queries_vec).batch(batch_size)\n",
    "    # Get the embedding for the query.\n",
    "    query_embedding = text_encoder.predict(queries_vec)\n",
    "    # Normalize the query and the image embeddings.\n",
    "    if normalize:\n",
    "        image_embeddings = tf.math.l2_normalize(image_embeddings, axis=1)\n",
    "        query_embedding = tf.math.l2_normalize(query_embedding, axis=1)\n",
    "    # Compute the dot product between the query and the image embeddings.\n",
    "    dot_similarity = tf.matmul(query_embedding, image_embeddings, transpose_b=True)\n",
    "    # Retrieve top k indices.\n",
    "    results = tf.math.top_k(dot_similarity, k).indices.numpy()\n",
    "    # Return matching image paths.\n",
    "    return [[train_image_paths[idx] for idx in indices] for indices in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eWCwgWo_zMf1",
   "metadata": {
    "id": "eWCwgWo_zMf1"
   },
   "outputs": [],
   "source": [
    "train_data = [p for p in ds_train]\n",
    "val_data = [p for p in ds_val]\n",
    "test_data = [p for p in ds_test]\n",
    "\n",
    "train_image_paths = [e[\"image path\"] for e in train_data]\n",
    "test_image_paths = [e[\"image path\"] for e in test_data]\n",
    "\n",
    "# TODO: this part only generates embeddings on the training dataset for now\n",
    "# TODO: this code re-reads the images\n",
    "image_embeddings = image_encoder.predict(\n",
    "    tf.data.Dataset.from_tensor_slices(train_image_paths).map(read_image).batch(batch_size),\n",
    "    verbose=1,\n",
    ")\n",
    "print(f\"Image embeddings shape: {image_embeddings.shape}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zWFOxWhV0kbF",
   "metadata": {
    "id": "zWFOxWhV0kbF"
   },
   "outputs": [],
   "source": [
    "query = \"active pheochromocytoma\"\n",
    "matches = find_matches(image_embeddings, [query], normalize=True)[0]\n",
    "\n",
    "print(\"Top matches for query: \\\"\" + query + \"\\\"\")\n",
    "\n",
    "plt.figure(figsize=(18, 18))\n",
    "for i in range(9):\n",
    "    path = matches[i].numpy().decode('UTF-8')\n",
    "    caption = next(x[\"raw caption\"] for x in train_data if x[\"image path\"].numpy().decode('UTF-8') == path)\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(mpimg.imread(path))\n",
    "    plt.axis(\"off\")\n",
    "    print(caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Jor40RYWJefh",
   "metadata": {
    "id": "Jor40RYWJefh"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X3HGLJ8w0ZdF",
   "metadata": {
    "id": "X3HGLJ8w0ZdF"
   },
   "outputs": [],
   "source": [
    "# TODO: might not work, needs revising\n",
    "\n",
    "def compute_top_k_accuracy(image_paths, k=5):\n",
    "    hits = 0\n",
    "    num_batches = int(np.ceil(len(image_paths) / batch_size))\n",
    "    for idx in range(num_batches):\n",
    "        start_idx = idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        current_image_paths = image_paths[start_idx:end_idx]\n",
    "        queries = [ captions[os.path.splitext(image_path.numpy().decode('UTF-8').split(os.sep)[-1])[0]] for image_path in current_image_paths ]\n",
    "        result = find_matches(image_embeddings, queries, k)\n",
    "        hits += sum(\n",
    "            [\n",
    "                image_path in matches for (image_path, matches) in list(zip(current_image_paths, result))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return hits / len(image_paths)\n",
    "\n",
    "n = 1920\n",
    "\n",
    "print(test_image_paths)\n",
    "\n",
    "print(\"Scoring training data...\")\n",
    "train_accuracy = compute_top_k_accuracy(random.sample(train_image_paths, n))\n",
    "print(f\"Train accuracy: {round(train_accuracy * 100, 3)}%\")\n",
    "\n",
    "print(\"Scoring evaluation data...\")\n",
    "eval_accuracy = compute_top_k_accuracy(random.sample(test_image_paths, n))\n",
    "print(f\"Eval accuracy: {round(eval_accuracy * 100, 3)}%\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "jaCktt68IRcq",
    "Jor40RYWJefh"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "5380e256bec2a872a4245067cef5da364603399a350ba03e757739343e36dd55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

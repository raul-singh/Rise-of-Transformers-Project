{"cells":[{"cell_type":"markdown","metadata":{"id":"xffJLUZCLC51"},"source":["## Declarations"]},{"cell_type":"markdown","metadata":{"id":"7TgHFX6XLQGl"},"source":["### Imports"]},{"cell_type":"code","execution_count":null,"id":"e20ae855","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4674,"status":"ok","timestamp":1685806019663,"user":{"displayName":"Ri ga","userId":"15539171450533002544"},"user_tz":-120},"id":"2d093180","outputId":"bc84c9fd-b81e-49b9-d8a7-0c7e03c595e0","trusted":true},"outputs":[],"source":["import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import math\n","import string\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","from IPython.display import display\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from tqdm import tqdm\n","\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","\n","tfk = tf.keras\n","tfkl = tf.keras.layers\n","kb = tf.keras.backend\n","print(tf.__version__)\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"]},{"cell_type":"markdown","id":"50d1635b","metadata":{"id":"-icpKfuyLSrH"},"source":["### Constants"]},{"cell_type":"code","execution_count":null,"id":"6a88e800","metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1685806019664,"user":{"displayName":"Ri ga","userId":"15539171450533002544"},"user_tz":-120},"id":"f3cf13ac","trusted":true},"outputs":[],"source":["# Random seed for reproducibility\n","seed = 42\n","\n","random.seed(seed)\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","tf.compat.v1.set_random_seed(seed)\n","\n","kaggle1 = \"/kaggle/input/transformers-hackathon/\"\n","kaggle2 = \"/kaggle/input/transformers-hackathon-features/\"\n","\n","image_dir = \"./resized_train\"\n","caption_pred_file = \"caption_prediction_train.csv\"\n","concept_det_file = \"concept_detection_train.csv\"\n","concept_file = \"concepts.csv\"\n","\n","##### Kaggle filepath #####\n","#image_dir = kaggle1 + image_dir\n","#caption_pred_file = kaggle2 + caption_pred_file\n","#concept_det_file = kaggle2 + concept_det_file\n","#concept_file = kaggle2 + concept_file\n","###########################\n","\n","image_size = (128, 128, 3)\n","\n","batch_size = 32\n","epochs = 100"]},{"cell_type":"markdown","id":"f14e271a","metadata":{"id":"n1w8gAuILhCl"},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":null,"id":"e275e143","metadata":{"trusted":true},"outputs":[],"source":["feature_types = {'image': tf.float16, 'caption': tf.string, 'concepts': tf.bool, 'raw caption': tf.string, 'image path': tf.string}\n","feature_shapes = {'image': (128, 128, 3), 'caption': (), 'concepts': (8374)}\n","base_features = [\"image\", \"caption\"]"]},{"cell_type":"code","execution_count":null,"id":"c2f398b7","metadata":{"trusted":true},"outputs":[],"source":["concepts = pd.read_csv(concept_file, sep='\\t')\n","concept_list = concepts.set_index('concept')['concept_name'].to_dict()\n","# Concept one-hot encoder\n","concepts_onehot = MultiLabelBinarizer(classes = list(concept_list.keys()))\n","concepts_onehot.fit([list(concept_list.keys())])\n","\n","captions = pd.read_csv(caption_pred_file, sep='\\t')\n","captions = captions.set_index('ID')['caption'].to_dict()\n","captions = {id: \"[SOS] \" + caption + \" [EOS]\" for id, caption in captions.items()}\n","\n","concepts = pd.read_csv(concept_det_file, sep='\\t')\n","concepts = concepts.set_index('ID')['cuis'].to_dict()\n","concepts = {id: item_concepts.split(\";\") for id, item_concepts in concepts.items()}"]},{"cell_type":"code","execution_count":null,"id":"2a657bfb","metadata":{"trusted":true},"outputs":[],"source":["def split(x, test_size=0.2, val_size=0.0, seed=0):\n","    if val_size + test_size >= 1:\n","        return None\n","    x_train, x_test = train_test_split(\n","        x, test_size=test_size + val_size, random_state=seed\n","    )\n","    x_val = None\n","    if val_size > 0:\n","        x_test, x_val = train_test_split(\n","            x_test,\n","            test_size=val_size / (test_size + val_size),\n","            random_state=seed,\n","        )\n","    return x_train, x_val, x_test\n","\n","def load_image_from_path(path):\n","    image = tf.io.read_file(path)\n","    image = tf.io.decode_jpeg(image, channels=3, dct_method=\"INTEGER_ACCURATE\")\n","\n","    # may need resizing\n","    #image = tf.image.resize(image, image_shape[:2])\n","    image = tf.cast(image, dtype=tf.float16)\n","    image = image / 255.0\n","    return image"]},{"cell_type":"code","execution_count":null,"id":"84aab6dd","metadata":{"trusted":true},"outputs":[],"source":["def custom_standardization(input_string):\n","    # convert input string to lowercase\n","    lowercase = tf.strings.lower(input_string)\n","    # replace special characters with empty string\n","    # TODO\n","    #return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n","    return lowercase"]},{"cell_type":"code","execution_count":null,"id":"ec3c8bce","metadata":{"trusted":true},"outputs":[],"source":["result = \"\"\n","for i in captions.values():\n","    result += \" \" + i\n","result = custom_standardization(result)\n","result = bytes.decode(result.numpy())\n","vocab_size = len(set(result.split()))\n","print(\"Vocab size:\")\n","print(vocab_size)\n","\n","longest = max(captions.values(), key=len)\n","longest = custom_standardization(longest)\n","longest = bytes.decode(longest.numpy())\n","longest = longest.split()\n","sequence_length = len(longest)\n","print(\"Longest sequence:\")\n","print(sequence_length)\n","\n","concept_size = max([len(c) for _, c in concepts.items()])\n","print(\"Max number of concepts:\")\n","print(concept_size)"]},{"cell_type":"code","execution_count":null,"id":"9a9236f9","metadata":{"trusted":true},"outputs":[],"source":["def load_features(image_folder, captions_file, concepts_file, concept_encoder, filter_percent=1):\n","    features = []\n","    \n","    # Import CSVs\n","    csv_caption_dataset = tf.data.experimental.CsvDataset(\n","        captions_file,\n","        field_delim='\\t',\n","        record_defaults=[tf.string, tf.string],\n","        header=True,\n","        select_cols=[0, 1]\n","    )\n","    csv_concept_dataset = tf.data.experimental.CsvDataset(\n","        concepts_file,\n","        field_delim='\\t',\n","        record_defaults=[tf.string, tf.string],\n","        header=True,\n","        select_cols=[0, 1]\n","    )\n","    \n","    # We make the assumption that CSV files contain the same key values (image names)\n","    # following the same ordering\n","\n","    # Extract features from dataset\n","    print(\"Extracting features from CSV file(s)\")\n","    for caption_el, concept_el in tqdm(zip(csv_caption_dataset, csv_concept_dataset)):\n","        filename_cap, caption = caption_el\n","        filename_con , concepts = concept_el\n","        \n","        # Sanity check\n","        assert filename_cap == filename_con\n","        \n","        image_path = image_dir + \"/\" + filename_cap + \".jpg\"\n","        \n","        features.append({\n","            'caption': caption,\n","            'image path': image_path,\n","            'concepts': concept_encoder.transform([concepts.numpy().decode(\"utf-8\").split(\";\")]),\n","        })\n","        \n","    # Filter elements\n","    if filter_percent != 1:\n","        n_features = int(len(features) * filter_percent)\n","        features = random.sample(features, n_features)\n","        \n","    return features\n","\n","def preprocess_features(features, concept_encoder, filter_percent=1):\n","    print(\"Preprocessing features\")\n","    \n","    # Filter elements\n","    if filter_percent != 1:\n","        n_features = int(len(features) * filter_percent)\n","        features = random.sample(features, n_features)\n","        \n","    return {\n","        'image paths': tf.convert_to_tensor([x[\"image path\"] for x in tqdm(features)], dtype=tf.string),\n","        'captions': tf.convert_to_tensor([x[\"caption\"] for x in tqdm(features)], dtype=tf.string),\n","        'concepts': tf.convert_to_tensor(np.vstack([concept_encoder.transform(x[\"concepts\"]).flatten() for x in tqdm(features)]), dtype=tf.bool),\n","        # 'images': tf.convert_to_tensor([load_image(x[\"image path\"]) for x in tqdm(features)], dtype=tf.float16),\n","    }"]},{"cell_type":"code","execution_count":null,"id":"230ac383","metadata":{"trusted":true},"outputs":[],"source":["# Load dataset features from csv files, split them and preprocess them\n","features = load_features(image_dir, caption_pred_file, concept_det_file, concepts_onehot, filter_percent=1)\n","feat_train, feat_val, feat_test = split(features, test_size=0.2, val_size=0.0, seed=seed)\n","\n","#feat_train = preprocess_features(features, concepts_onehot, filter_percent=0.01) if feat_train else None\n","#feat_val = preprocess_features(features, concepts_onehot, filter_percent=0.01) if feat_val else None\n","#feat_test = preprocess_features(features, concepts_onehot, filter_percent=0.01) if feat_test else None"]},{"cell_type":"code","execution_count":null,"id":"27ae1cff","metadata":{"trusted":true},"outputs":[],"source":["def create_dataset(\n","        features, \n","        input_features_types,\n","        feature_shapes,\n","        x_features, y_features=None, \n","        x_dict=True, y_dict=True,\n","        load_images=True, \n","        shuffle_buffer_size=1024, \n","        batch_size=10, \n","        cached=False\n","):\n","    # Generate dataset following initial input feature types\n","    dataset = tf.data.Dataset.from_generator(\n","        lambda: features, { x: input_features_types[x] for x in input_features_types }\n","    )\n","    \n","    # Preprocessing internal functions\n","    def setshape(e):\n","        for (k, v) in feature_shapes.items():\n","            if k in e:\n","                e[k].set_shape(v)\n","        return e\n","    def add_images(e):\n","        # Maybe parametrize\n","        img_from = \"image path\"\n","        img_to = \"image\"\n","        new_features = list(input_features_types.keys()) + [img_to]\n","        return {f:e[f] if f != img_to else load_image_from_path(e[img_from]) for f in new_features}\n","    def split_xy(e):\n","        e_x = {xf:tf.squeeze(e[xf]) for xf in x_features} if x_dict else tf.squeeze([e[xf] for xf in x_features])\n","        if y_features:\n","            e_y = {yf:tf.squeeze(e[yf]) for yf in y_features} if y_dict else tf.squeeze([e[yf] for yf in y_features])\n","            return (e_x, e_y)\n","        return e_x\n","    \n","    # Preprocess\n","    if load_images:\n","        dataset = dataset.map(add_images)\n","    dataset = dataset.map(setshape)\n","    dataset = dataset.map(split_xy)\n","\n","    # Compile dataset\n","    if cached:\n","        dataset = dataset.cache()\n","    dataset = dataset.shuffle(shuffle_buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","    return dataset\n","\n","def visualize_first_of_dataset_batch(dataset_batch, nums=5):\n","    for c in range(0, nums):\n","        i = tf.cast(dataset_batch[\"image\"][c], dtype=tf.float32)\n","        t = dataset_batch[\"raw caption\"][c]\n","        plt.figure(figsize=(50, 100))\n","        plt.subplot(nums, 1, c + 1)\n","        plt.imshow(i)\n","        plt.title(f\"{t}\", fontsize=100)\n","        plt.xticks([])\n","        plt.yticks([])\n","def visualize_first_of_dataset_batch(dataset_batch, nums=5):\n","    for c in range(0, nums):\n","        i = tf.cast(dataset_batch[\"image\"][c], dtype=tf.float32)\n","        t = dataset_batch[\"raw caption\"][c]\n","        plt.figure(figsize=(50, 100))\n","        plt.subplot(nums, 1, c + 1)\n","        plt.imshow(i)\n","        plt.title(f\"{t}\", fontsize=100)\n","        plt.xticks([])\n","        plt.yticks([])"]},{"cell_type":"code","execution_count":null,"id":"475760da","metadata":{"trusted":true},"outputs":[],"source":["in_feat_typ = {'caption': tf.string, 'concepts': tf.bool, 'image path': tf.string}\n","x_features = ['caption', 'image']\n","x_features_iep = ['image']\n","y_features_iep = ['concepts']\n","\n","train_ds_size = len(feat_train) if feat_train else 0\n","val_ds_size = len(feat_val) if feat_val else 0\n","test_ds_size = len(feat_test) if feat_test else 0\n","\n","train_dataset = create_dataset(feat_train, input_features_types=in_feat_typ, feature_shapes=feature_shapes, x_features=x_features) if feat_train else None\n","val_dataset = create_dataset(feat_val, input_features_types=in_feat_typ, feature_shapes=feature_shapes, x_features=x_features) if feat_val else None\n","test_dataset = create_dataset(feat_test, input_features_types=in_feat_typ, feature_shapes=feature_shapes, x_features=x_features) if feat_test else None\n","\n","train_dataset_iep = create_dataset(feat_train, input_features_types=in_feat_typ, feature_shapes=feature_shapes, x_features=x_features_iep, y_features=y_features_iep, x_dict=False, y_dict=False, batch_size=batch_size, cached=True) if feat_train else None\n","val_dataset_iep = create_dataset(feat_val, input_features_types=in_feat_typ, feature_shapes=feature_shapes, x_features=x_features_iep, y_features=y_features_iep, x_dict=False, y_dict=False, batch_size=batch_size, cached=True) if feat_val else None\n","test_dataset_iep = create_dataset(feat_test, input_features_types=in_feat_typ, feature_shapes=feature_shapes, x_features=x_features_iep, y_features=y_features_iep, x_dict=False, y_dict=False, batch_size=batch_size, cached=True) if feat_test else None\n","\n","train_dataset_eval = create_dataset(feat_train, input_features_types=in_feat_typ, feature_shapes=feature_shapes, x_features=['image path', 'image'], y_features=['caption', 'concepts'], x_dict=True, y_dict=True, batch_size=1, shuffle_buffer_size=1)\n","test_dataset_eval = create_dataset(feat_test, input_features_types=in_feat_typ, feature_shapes=feature_shapes, x_features=['image path', 'image'], y_features=['caption', 'concepts'], x_dict=True, y_dict=True, batch_size=1, shuffle_buffer_size=1)"]},{"cell_type":"markdown","id":"0a7884f7","metadata":{},"source":["## Download Models"]},{"cell_type":"code","execution_count":null,"id":"186f59be","metadata":{"trusted":true},"outputs":[],"source":["text_preprocess = hub.KerasLayer(\n","        \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n","        name=\"text_preprocessing\",\n","    )\n","\n","text_transformer = hub.KerasLayer(\n","        \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\",\n","        trainable=True,\n","        name=\"bert\",\n","    )\n","\n","img_preprocess = tfk.applications.convnext.preprocess_input\n","\n","img_supernet = tfk.applications.ConvNeXtTiny(weights='imagenet', include_top=False)\n","supernet_name = img_supernet.name"]},{"cell_type":"markdown","id":"fe80c62b","metadata":{},"source":["## Pre-pre-training"]},{"cell_type":"code","execution_count":null,"id":"ada53b22","metadata":{"trusted":true},"outputs":[],"source":["def image_encoder_pretrainer(preprocessing, supernet, n_concepts, input_shape=(128,128,3), learning_rate=1e-5):\n","    \n","    input_layer = tfkl.Input(shape=input_shape, name='image')\n","\n","    x = preprocessing(input_layer)\n","    x = supernet(x)\n","    \n","    x = tfkl.GlobalMaxPooling2D(name='GAP')(x)\n","    x = tfkl.Dense(256, activation='relu')(x)\n","    x = tfkl.Dense(128, activation='relu')(x)\n","    x = tfkl.Dense(n_concepts, activation=\"sigmoid\", name='output')(x)\n","\n","    image_encoder_pretrainer = tfk.Model(inputs=input_layer, outputs=x, name=\"image_encoder_pretrainer\")\n","    image_encoder_pretrainer.compile(\n","        loss=\"binary_crossentropy\", optimizer=tf.optimizers.Adam(learning_rate=learning_rate),\n","    )\n","    \n","    return image_encoder_pretrainer"]},{"cell_type":"code","execution_count":null,"id":"7d2cd5c9","metadata":{"trusted":true},"outputs":[],"source":["iep = image_encoder_pretrainer(img_preprocess, img_supernet, len(concept_list.keys()))"]},{"cell_type":"code","execution_count":null,"id":"c51483e9","metadata":{"trusted":true},"outputs":[],"source":["# Create an early stopping callback.\n","early_stopping = tf.keras.callbacks.EarlyStopping(\n","    monitor = \"val_loss\", patience = 5, restore_best_weights = True\n",")\n","\n","history = iep.fit(\n","    train_dataset_iep,\n","    epochs = 1,\n","    validation_data = test_dataset_iep,\n","    callbacks = [early_stopping],\n",")"]},{"cell_type":"code","execution_count":null,"id":"58be81a7","metadata":{},"outputs":[],"source":["img_supernet = iep.layers[1]"]},{"cell_type":"code","execution_count":null,"id":"3bb47561","metadata":{},"outputs":[],"source":["img_supernet.save_weights('models/pre-trained_supernet.h5')"]},{"cell_type":"markdown","id":"8d26e9fa","metadata":{"id":"oiUz4hxwNRVS"},"source":["## Network"]},{"cell_type":"markdown","id":"00516090","metadata":{"id":"WhVp1-YONWpY"},"source":["### Network blocks"]},{"cell_type":"code","execution_count":null,"id":"8a8c5f6e","metadata":{},"outputs":[],"source":["def projection(embedding_input, embed_dim, name):\n","    \n","    embeddings = tfkl.Dense(embed_dim, name=f'{name}_1')(embedding_input)\n","    x = tf.nn.selu(embeddings)\n","    x = tfkl.Dense(embed_dim, name=f'{name}_2')(x)\n","    x = tfkl.Dropout(0.1)(x)\n","    x = tfkl.Add()([x, embeddings])\n","    embeddings = tfkl.LayerNormalization()(x)\n","\n","    return embeddings"]},{"cell_type":"code","execution_count":null,"id":"8bbf4ffb","metadata":{"executionInfo":{"elapsed":441,"status":"ok","timestamp":1685807496567,"user":{"displayName":"Ri ga","userId":"15539171450533002544"},"user_tz":-120},"id":"2bc14deb","trusted":true},"outputs":[],"source":["def image_encoder(input_shape, embed_dim, seed=42, supernet=None, preprocessing=None):\n","    \n","    tf.random.set_seed(seed)\n","\n","    input_layer = tfkl.Input(shape=input_shape, name='img_input_layer')\n","\n","    x = preprocessing(input_layer)\n","    x = supernet(x)\n","\n","    x = tfkl.GlobalAveragePooling2D(name='GAP')(x)\n","\n","    x = projection(x, embed_dim, 'img_embedding_dense_layer')\n","    \n","    # Connect input and output through the Model class\n","    cnn_encoder = tfk.Model(inputs=input_layer, outputs=x, name='image_encoder')\n","\n","    # Return the encoder\n","    return cnn_encoder"]},{"cell_type":"code","execution_count":null,"id":"bea6f9d3","metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1685807500205,"user":{"displayName":"Ri ga","userId":"15539171450533002544"},"user_tz":-120},"id":"50bc32b1","trusted":true},"outputs":[],"source":["def text_encoder(embed_dim, preprocess, transformer, trainable=True):\n","\n","    transformer.trainable = trainable\n","    \n","    input_layer = tfkl.Input(shape=(), dtype=tf.string, name=\"text_input\")\n","    x = preprocess(input_layer)\n","    x = transformer(x)[\"pooled_output\"]\n","    \n","\n","    x = projection(x, embed_dim, 'txt_embedding_dense_layer')\n","\n","    text_encoder = tfk.Model(inputs=input_layer, outputs=x, name=\"text_encoder\")\n","    \n","    return text_encoder"]},{"cell_type":"code","execution_count":null,"id":"6dfc619c","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1685807500648,"user":{"displayName":"Ri ga","userId":"15539171450533002544"},"user_tz":-120},"id":"tQfOhkjPjz70","trusted":true},"outputs":[],"source":["class CLIP(tfk.Model):\n","    def __init__(self, image_encoder, text_encoder, temp=1.0, **kwargs):\n","        super().__init__(**kwargs)\n","        self.image_encoder = image_encoder\n","        self.text_encoder = text_encoder\n","        self.loss_tracker = tfk.metrics.Mean(name=\"loss\")\n","        self.temp = self.add_weight(name='t',\n","                                 shape=(1, ),\n","                                 initializer=tfk.initializers.Constant(1.),\n","                                 trainable=True)\n","\n","        \n","    @property\n","    def metrics(self):\n","        return [self.loss_tracker]\n","\n","    def call(self, features, training=False):\n","        image_emb = self.image_encoder(features[\"image\"], training=training)\n","        text_emb = self.text_encoder(features[\"caption\"], training=training)\n","        return image_emb, text_emb\n","\n","    def CLIP_loss(self, image_emb, text_emb):\n","        norm_image_emb = tf.math.l2_normalize(image_emb, axis=1)\n","        norm_text_emb = tf.math.l2_normalize(text_emb, axis=1)\n","\n","        logits = tf.linalg.matmul(norm_image_emb, norm_text_emb, transpose_b=True) * tf.math.exp(self.temp)\n","\n","        n = tf.shape(logits)[0]\n","        labels = tf.range(n)\n","\n","        loss_img = tfk.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n","        loss_txt = tfk.losses.sparse_categorical_crossentropy(labels, kb.transpose(logits), from_logits=True)\n","\n","        return (loss_img + loss_txt) / tf.constant(2.0)\n","\n","    def train_step(self, features):\n","        with tf.GradientTape() as tape:\n","            image_embeddings, caption_embeddings = self(features, training=True)\n","            loss = self.CLIP_loss(caption_embeddings, image_embeddings)\n","\n","        gradients = tape.gradient(loss, self.trainable_variables)\n","        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n","\n","        self.loss_tracker.update_state(loss)\n","        return {\"loss\": self.loss_tracker.result()}\n","\n","    def test_step(self, features):\n","        image_embeddings, caption_embeddings = self(features, training=False)\n","        loss = self.CLIP_loss(caption_embeddings, image_embeddings)\n","        self.loss_tracker.update_state(loss)\n","        return {\"loss\": self.loss_tracker.result()}\n"]},{"cell_type":"markdown","id":"000547c7","metadata":{"id":"FWPNhGjzNbnP"},"source":["### Building network"]},{"cell_type":"code","execution_count":null,"id":"cac91951","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1685807502773,"user":{"displayName":"Ri ga","userId":"15539171450533002544"},"user_tz":-120},"id":"mra2VO7JoqGj","trusted":true},"outputs":[],"source":["def build_clip(img_input_shape=(128,128,3),\n","               txt_input_shape=(393, ), \n","               embed_dim=128, \n","               temp=1.0,\n","               learning_rate=2e-5,\n","               img_supernet=None,\n","               img_preprocess=None,\n","               text_transformer=None,\n","               text_preprocess=None):\n","\n","    \n","    text_encoder_model = text_encoder(embed_dim, text_preprocess, text_transformer)\n","    image_encoder_model = image_encoder(img_input_shape, embed_dim, supernet=img_supernet, preprocessing=img_preprocess)\n","\n","    clip = CLIP(image_encoder_model, text_encoder_model, temp)\n","    clip.compile(optimizer = tf.optimizers.AdamW(learning_rate=learning_rate))\n","\n","    return image_encoder_model, text_encoder_model, clip"]},{"cell_type":"code","execution_count":null,"id":"2100f802","metadata":{"executionInfo":{"elapsed":564,"status":"ok","timestamp":1685807504408,"user":{"displayName":"Ri ga","userId":"15539171450533002544"},"user_tz":-120},"id":"f0778191","trusted":true},"outputs":[],"source":["clip_image_encoder, clip_text_encoder, clip = build_clip(\n","    img_supernet=img_supernet,\n","    img_preprocess=img_preprocess,\n","    text_transformer=text_transformer,\n","    text_preprocess=text_preprocess,\n",")"]},{"cell_type":"markdown","id":"fcb46b29","metadata":{"id":"mPnccec1Nfwh"},"source":["## Training"]},{"cell_type":"markdown","id":"148b11b5","metadata":{"id":"eqrFQi0ZNhvo"},"source":["### Phase 1\n","Traning all the parameters"]},{"cell_type":"code","execution_count":null,"id":"ebabd381","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"executionInfo":{"elapsed":1920448,"status":"error","timestamp":1685809433717,"user":{"displayName":"Ri ga","userId":"15539171450533002544"},"user_tz":-120},"id":"jS2cVFlVrHLs","outputId":"fc619277-db77-4c8a-f742-c0a373c094b7","trusted":true},"outputs":[],"source":["# Create a learning rate scheduler callback.\n","reduce_lr = tfk.callbacks.ReduceLROnPlateau(\n","    monitor = \"val_loss\", factor = 0.2, patience = 3\n",")\n","\n","# Create an early stopping callback.\n","early_stopping = tf.keras.callbacks.EarlyStopping(\n","    monitor = \"val_loss\", patience = 5, restore_best_weights = True\n",")\n","\n","history_phase1 = clip.fit(\n","    train_dataset,\n","    epochs = epochs,\n","    validation_data = test_dataset,\n","    callbacks = [reduce_lr, early_stopping],\n",")"]},{"cell_type":"markdown","id":"fff4ad3e","metadata":{},"source":["### Phase 2\n","Training the projection only"]},{"cell_type":"code","execution_count":null,"id":"5f2cd6b0","metadata":{},"outputs":[],"source":["img_supernet.trainable = True\n","text_transformer.trainable = True"]},{"cell_type":"code","execution_count":null,"id":"93755181","metadata":{},"outputs":[],"source":["clip.compile(optimizer = tf.optimizers.AdamW(2e-5))"]},{"cell_type":"code","execution_count":null,"id":"afebd9de","metadata":{},"outputs":[],"source":["# Create a learning rate scheduler callback.\n","reduce_lr = tfk.callbacks.ReduceLROnPlateau(\n","    monitor = \"val_loss\", factor = 0.2, patience = 3\n",")\n","\n","# Create an early stopping callback.\n","early_stopping = tf.keras.callbacks.EarlyStopping(\n","    monitor = \"val_loss\", patience = 5, restore_best_weights = True\n",")\n","\n","history_phase2 = clip.fit(\n","    train_dataset,\n","    epochs = epochs,\n","    validation_data = test_dataset,\n","    callbacks = [early_stopping, reduce_lr],\n",")"]},{"cell_type":"code","execution_count":null,"id":"85789424","metadata":{},"outputs":[],"source":["plt.plot(history_phase1.history[\"loss\"]+history_phase2.history[\"loss\"])\n","plt.plot(history_phase1.history[\"val_loss\"]+history_phase2.history[\"val_loss\"])\n","plt.ylabel(\"Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.legend([\"train\", \"valid\"], loc=\"upper right\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"ef294d2c","metadata":{"id":"8WqAGCTRJYxn","trusted":true},"outputs":[],"source":["clip_image_encoder.save_weights('models/img_enc.h5')\n","clip_text_encoder.save_weights('models/text_enc.h5')"]},{"cell_type":"markdown","id":"233cb68e","metadata":{"id":"jaCktt68IRcq"},"source":["### Task performance"]},{"cell_type":"code","execution_count":null,"id":"7727e4c4","metadata":{},"outputs":[],"source":["def generate_image_embeddings(\n","    image_encoder,                 # Image encoder of clip model\n","    dataset_eval,                  # Dataset to generate embeddings (WARNING: the dataset must not be shuffling or have a shuffle buffer size of 1)\n","    dataset_pred_map=lambda *x: x, # Lambda mapping function for prediction\n","    dataset_ref_map=lambda *x: x,  # Lambda mapping function for reference\n","):\n","    print(\"Generating image embeddings\")\n","    image_embeddings = image_encoder.predict(\n","        dataset_eval.map(dataset_pred_map),\n","        verbose=1,\n","    )\n","    dataset_reference = [x for x in train_dataset_eval.map(dataset_ref_map).unbatch()]\n","    return dataset_reference, image_embeddings\n","\n","def find_t2i_matches(\n","    queries,                # Queries to search\n","    text_encoder,           # Text encoder of clip model\n","    image_embeddings,       # Generated image embeddings\n","    dataset_reference=None, # Reference for retreived dataset elements following indices\n","    k=5,                    # Number of elements for top-k\n","    normalize=True,         # Embedding normalization\n","):\n","    print(\"Computing Text-to-Image matches\")\n","    # Generate query dataset and get their embeddings\n","    queries_ds = tf.data.Dataset.from_tensor_slices(queries).batch(batch_size)\n","    query_embedding = text_encoder.predict(queries_ds)\n","    # Normalize the query and the image embeddings\n","    if normalize:\n","        image_embeddings = tf.math.l2_normalize(image_embeddings, axis=1)\n","        query_embedding = tf.math.l2_normalize(query_embedding, axis=1)\n","    # Compute the dot product between the query and the image embeddings\n","    dot_similarity = tf.matmul(query_embedding, image_embeddings, transpose_b=True)\n","    # Retrieve top k indices\n","    results = tf.math.top_k(dot_similarity, k).indices.numpy()\n","    return results\n","\n","def index_to_reference(results, dataset_reference):\n","    return [[dataset_reference[match] for match in result] for result in results]\n","\n","def visualize_t2i_results(matches):\n","    # Assuming matches are in the form of tuples: (image_path, caption)\n","    print(\"Top matches for query: \\\"\" + query + \"\\\"\")\n","    plt.figure(figsize=(18, 18))\n","    for i in range(len(matches)):\n","        path = matches[i][0].numpy().decode('UTF-8')\n","        caption = matches[i][0].numpy()#.decode('UTF-8')\n","        print('Caption: '+path)\n","        #ax = plt.subplot(3, 3, i + 1)\n","        #plt.imshow(mpimg.imread(path))\n","        #plt.axis(\"off\")\n","        print(f\"{i}) {caption}\")"]},{"cell_type":"code","execution_count":null,"id":"0194e410","metadata":{},"outputs":[],"source":["import matplotlib.image as mpimg\n","\n","query = \"head\"\n","# WARNING: currently using train_dataset_eval\n","'''\n","dataset_reference, image_embeddings = generate_image_embeddings(\n","    clip_image_encoder,\n","    train_dataset_eval,\n","    dataset_pred_map=lambda x, y: x['image'],\n","    dataset_ref_map=lambda x, y: (x['image path'], y['caption'])\n",") '''\n","results = find_t2i_matches([query], clip_text_encoder, test_image_embeddings, k=5, normalize=False)\n","results = index_to_reference(results, test_dataset_reference)\n","for matches in results:\n","    visualize_t2i_results(matches)"]},{"cell_type":"code","execution_count":null,"id":"d32626f6","metadata":{},"outputs":[],"source":["# TODO: assumption that the whole dataset is used as a query for most metrics: **\n","def compute_relevant_at_k(results, dataset_reference, k=None, reference_preprocess=lambda x: x, relevance=lambda m, o: m == o):\n","    if not k:\n","        k = len(results[0])\n","    return [ \n","        np.count_nonzero([relevance(match, original) for match in list(map(reference_preprocess, matches))[0:k]])\n","        for matches, original in zip(results, map(reference_preprocess, dataset_reference)) # **\n","    ]\n","\n","#def compute_total_relevance(queries, dataset_reference, reference_preprocess=lambda x: x, relevance=lambda m, o: m == o):\n","#    return [ \n","#        np.count_nonzero([relevance(query, element) for element in map(reference_preprocess, dataset_reference)])\n","#        for query in tqdm(queries)\n","#    ]\n","\n","def compute_top_k_accuracy(results, dataset_reference, relevant_at_k=None, k=None, reference_preprocess=lambda x: x, relevance=lambda m, o: m == o):\n","    if not relevant_at_k:\n","        relevant_at_k = compute_relevant_at_k(results, dataset_reference, k, reference_preprocess=reference_preprocess, relevance=relevance)\n","    hits = np.count_nonzero(relevant_at_k)\n","    return hits / len(dataset_reference)\n","\n","def compute_map_k(results, dataset_reference, relevant_at_k=None, k=None, reference_preprocess=lambda x: x, relevance=lambda m, o: m == o):\n","    if not k:\n","        k = len(results[0])\n","    if not relevant_at_k:\n","        relevant_at_k = compute_relevant_at_k(results, dataset_reference, k, reference_preprocess=reference_preprocess, relevance=relevance)\n","    precision_at_k = [r/k for r in relevant_at_k]\n","    return np.sum(precision_at_k) / len(dataset_reference)\n","\n","def compute_mar_k(results, dataset_reference, relevant_at_k=None, total_relevant=None, k=None, reference_preprocess=lambda x: x, relevance=lambda m, o: m == o):\n","    if not k:\n","        k = len(results[0])\n","    if not relevant_at_k:\n","        relevant_at_k = compute_relevant_at_k(results, dataset_reference, k, reference_preprocess=reference_preprocess, relevance=relevance)\n","    if not total_relevant_at_k:\n","        total_relevant = compute_total_relevance(results, dataset_reference, reference_preprocess=reference_preprocess, relevance=relevance)\n","    recall_at_k = [rk/tr for rk, tr in zip(relevant_at_k, total_relevant)] # **\n","    return np.sum(recall_at_k) / len(dataset_reference)"]},{"cell_type":"code","execution_count":null,"id":"c03158da","metadata":{},"outputs":[],"source":["k = 10\n","concept_overlap_threshold = 2\n","reference_preprocess_cap = lambda x: x[0].numpy().decode('UTF-8')                                    # Function to preprocess data when we want to evaluate captions\n","reference_preprocess_con = lambda x: x[1]                                                            # Function to preprocess data when we want to evaluate concepts\n","concept_relevance = lambda m, o: np.count_nonzero(np.logical_and(m, o)) >= concept_overlap_threshold # Function to compute if a match is relevant given concept arrays \n","\n","'''\n","print(\"### Scoring training data ###\")\n","train_dataset_reference, train_image_embeddings = generate_image_embeddings(\n","    clip_image_encoder,\n","    train_dataset_eval,\n","    dataset_pred_map=lambda x, y: x['image'],\n","    dataset_ref_map=lambda x, y: (y['caption'], y['concepts'])\n",")\n","train_queries = [e[0] for e in train_dataset_reference]\n","# Compute relevance for all the queries in the dataset using only caption equality as a metric\n","#train_tot_relevant = compute_total_relevance(train_queries, train_dataset_reference, reference_preprocess=reference_preprocess_cap)\n","# Compute matching results and extrapolate relevant matches based on different criterions\n","train_raw_results = find_t2i_matches(train_queries, clip_text_encoder, train_image_embeddings, k=5, normalize=True)\n","train_results = index_to_reference(train_raw_results, train_dataset_reference)\n","train_relevant_cap = compute_relevant_at_k(train_results, train_dataset_reference, reference_preprocess=reference_preprocess_cap)\n","train_relevant_con = compute_relevant_at_k(train_results, train_dataset_reference, reference_preprocess=reference_preprocess_con, relevance=concept_relevance)\n","\n","'''\n","print(\"\\n### Scoring test data ###\")\n","test_dataset_reference, test_image_embeddings = generate_image_embeddings(\n","    clip_image_encoder,\n","    test_dataset_eval,\n","    dataset_pred_map=lambda x, y: x['image'],\n","    dataset_ref_map=lambda x, y: (y['caption'], y['concepts'])\n",")\n","test_queries = [e[0] for e in test_dataset_reference]\n","# Compute relevance for all the queries in the dataset using only caption equality as a metric\n","#test_tot_relevant = compute_total_relevance(test_queries, test_dataset_reference, reference_preprocess=reference_preprocess_cap)\n","# Compute matching results and extrapolate relevant matches based on different criterions\n","test_raw_results = find_t2i_matches(test_queries, clip_text_encoder, test_image_embeddings, k=5, normalize=True)\n","test_results = index_to_reference(test_raw_results, test_dataset_reference)\n","test_relevant_cap = compute_relevant_at_k(test_results, test_dataset_reference, reference_preprocess=reference_preprocess_cap)\n","test_relevant_con = compute_relevant_at_k(test_results, test_dataset_reference, reference_preprocess=reference_preprocess_con, relevance=concept_relevance)"]},{"cell_type":"code","execution_count":null,"id":"b54a372a","metadata":{},"outputs":[],"source":["'''\n","print(\"### Training data ###\")\n","train_accuracy_cap = compute_top_k_accuracy(train_results, train_dataset_reference, relevant_at_k=train_relevant_cap)\n","train_accuracy_con = compute_top_k_accuracy(train_results, train_dataset_reference, relevant_at_k=train_relevant_con)\n","print(f\"Accuracy for caption equality: {round(train_accuracy_cap * 100, 3)}%\")\n","print(f\"Accuracy for concept overlap: {round(train_accuracy_con * 100, 3)}%\")\n","train_map_cap = compute_map_k(train_results, train_dataset_reference, relevant_at_k=train_relevant_cap)\n","train_map_con = compute_map_k(train_results, train_dataset_reference, relevant_at_k=train_relevant_con)\n","print(f\"Mean Average Precision for caption equality: {round(train_map_cap * 100, 3)}%\")\n","print(f\"Mean Average Precision for concept overlap: {round(train_map_con * 100, 3)}%\")\n","'''\n","\n","print(\"\\n### Test data ###\")\n","test_accuracy_cap = compute_top_k_accuracy(test_results, test_dataset_reference, relevant_at_k=test_relevant_cap)\n","test_accuracy_con = compute_top_k_accuracy(test_results, test_dataset_reference, relevant_at_k=test_relevant_con)\n","print(f\"Accuracy for caption equality: {round(test_accuracy_cap * 100, 3)}%\")\n","print(f\"Accuracy for concept overlap: {round(test_accuracy_con * 100, 3)}%\")\n","test_map_cap = compute_map_k(test_results, test_dataset_reference, relevant_at_k=test_relevant_cap)\n","test_map_con = compute_map_k(test_results, test_dataset_reference, relevant_at_k=test_relevant_con)\n","print(f\"Mean Average Precision for caption equality: {round(test_map_cap * 100, 3)}%\")\n","print(f\"Mean Average Precision for concept overlap: {round(test_map_con * 100, 3)}%\")"]},{"cell_type":"code","execution_count":null,"id":"a345466e","metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["jaCktt68IRcq","Jor40RYWJefh"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"},"vscode":{"interpreter":{"hash":"5380e256bec2a872a4245067cef5da364603399a350ba03e757739343e36dd55"}}},"nbformat":4,"nbformat_minor":5}

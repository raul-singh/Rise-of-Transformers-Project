{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Declarations","metadata":{}},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\nimport itertools\nimport math\nimport string\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport plotly.graph_objects as go\nimport numpy as np\nimport random\nfrom IPython.display import display\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom tqdm import tqdm\n\nimport tensorflow_hub as hub\nimport tensorflow_text as text\n\ntfk = tf.keras\ntfkl = tf.keras.layers\nkb = tf.keras.backend\nprint(tf.__version__)\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Constants","metadata":{}},{"cell_type":"code","source":"# Random seed for reproducibility\nseed = 42\n\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)\n\n# Turn ON for kaggle filepaths\nkaggle = True\n\nkaggle1 = \"/kaggle/input/transformers-hackathon/\"\nkaggle2 = \"/kaggle/input/transformers-hackathon-features/\"\nkaggle3 = \"/kaggle/input/clip-weights-v3/\"\n\nimage_dir = \"./resized_train\"\ncaption_pred_file = \"caption_prediction_train.csv\"\nconcept_det_file = \"concept_detection_train.csv\"\nconcept_file = \"concepts.csv\"\nclip_weights_file = \"clip_weights.h5\"\n\nif kaggle:\n    image_dir = kaggle1 + image_dir\n    caption_pred_file = kaggle2 + caption_pred_file\n    concept_det_file = kaggle2 + concept_det_file\n    concept_file = kaggle2 + concept_file\n    clip_weights_file = kaggle3 + clip_weights_file\n\nimage_size = (128, 128, 3)\n\nbatch_size = 32\nepochs = 100\n\nfilter_percent_dataset = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"def split(x, test_size=0.2, val_size=0.0, seed=0):\n    if val_size + test_size >= 1:\n        return None\n    x_train, x_test = train_test_split(\n        x, test_size=test_size + val_size, random_state=seed\n    )\n    x_val = None\n    if val_size > 0:\n        x_test, x_val = train_test_split(\n            x_test,\n            test_size=val_size / (test_size + val_size),\n            random_state=seed,\n        )\n    return x_train, x_val, x_test\n\ndef load_image_from_path(path):\n    image = tf.io.read_file(path)\n    image = tf.io.decode_jpeg(image, channels=3, dct_method=\"INTEGER_ACCURATE\")\n\n    # may need resizing\n    #image = tf.image.resize(image, image_shape[:2])\n    image = tf.cast(image, dtype=tf.float16)\n    image = image / 255.0\n    return image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_types = {'image': tf.float16, 'caption': tf.string, 'concepts': tf.bool, 'raw caption': tf.string, 'image path': tf.string}\nfeature_shapes = {'image': (128, 128, 3), 'caption': (), 'concepts': (8374)}\nbase_features = [\"image\", \"caption\"]\n\nconcepts = pd.read_csv(concept_file, sep='\\t')\nconcept_list = concepts.set_index('concept')['concept_name'].to_dict()\n# Concept one-hot encoder\nconcepts_onehot = MultiLabelBinarizer(classes = list(concept_list.keys()))\n_ = concepts_onehot.fit([list(concept_list.keys())])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_features(image_folder, captions_file, concepts_file, concept_encoder, filter_percent=1):\n    features = []\n    \n    # Import CSVs\n    csv_caption_dataset = tf.data.experimental.CsvDataset(\n        captions_file,\n        field_delim='\\t',\n        record_defaults=[tf.string, tf.string],\n        header=True,\n        select_cols=[0, 1]\n    )\n    csv_concept_dataset = tf.data.experimental.CsvDataset(\n        concepts_file,\n        field_delim='\\t',\n        record_defaults=[tf.string, tf.string],\n        header=True,\n        select_cols=[0, 1]\n    )\n    \n    # We make the assumption that CSV files contain the same key values (image names)\n    # following the same ordering\n\n    # Extract features from dataset\n    print(\"Extracting features from CSV file(s)\")\n    for caption_el, concept_el in tqdm(zip(csv_caption_dataset, csv_concept_dataset)):\n        filename_cap, caption = caption_el\n        filename_con , concepts = concept_el\n        \n        # Sanity check\n        assert filename_cap == filename_con\n        \n        image_path = image_dir + \"/\" + filename_cap + \".jpg\"\n        \n        features.append({\n            'caption': caption,\n            'image path': image_path,\n            'concepts': concept_encoder.transform([concepts.numpy().decode(\"utf-8\").split(\";\")]),\n        })\n        \n    # Filter elements\n    if filter_percent != 1:\n        n_features = int(len(features) * filter_percent)\n        features = random.sample(features, n_features)\n        \n    return features\n\ndef preprocess_features(features, concept_encoder, filter_percent=1):\n    print(\"Preprocessing features\")\n    \n    # Filter elements\n    if filter_percent != 1:\n        n_features = int(len(features) * filter_percent)\n        features = random.sample(features, n_features)\n        \n    return {\n        'image paths': tf.convert_to_tensor([x[\"image path\"] for x in tqdm(features)], dtype=tf.string),\n        'captions': tf.convert_to_tensor([x[\"caption\"] for x in tqdm(features)], dtype=tf.string),\n        'concepts': tf.convert_to_tensor(np.vstack([concept_encoder.transform(x[\"concepts\"]).flatten() for x in tqdm(features)]), dtype=tf.bool),\n        # 'images': tf.convert_to_tensor([load_image(x[\"image path\"]) for x in tqdm(features)], dtype=tf.float16),\n    }\n\ndef create_dataset(\n        features, \n        input_features_types,\n        feature_shapes,\n        x_features, y_features=None, \n        x_dict=True, y_dict=True,\n        load_images=True, \n        shuffle_buffer_size=1024, \n        batch_size=10, \n        cached=False\n):\n    # Generate dataset following initial input feature types\n    dataset = tf.data.Dataset.from_generator(\n        lambda: features, { x: input_features_types[x] for x in input_features_types }\n    )\n    \n    # Preprocessing internal functions\n    def setshape(e):\n        for (k, v) in feature_shapes.items():\n            if k in e:\n                e[k].set_shape(v)\n        return e\n    def add_images(e):\n        # Maybe parametrize\n        img_from = \"image path\"\n        img_to = \"image\"\n        new_features = list(input_features_types.keys()) + [img_to]\n        return {f:e[f] if f != img_to else load_image_from_path(e[img_from]) for f in new_features}\n    def split_xy(e):\n        e_x = {xf:tf.squeeze(e[xf]) for xf in x_features} if x_dict else tf.squeeze([e[xf] for xf in x_features])\n        if y_features:\n            e_y = {yf:tf.squeeze(e[yf]) for yf in y_features} if y_dict else tf.squeeze([e[yf] for yf in y_features])\n            return (e_x, e_y)\n        return e_x\n    \n    # Preprocess\n    if load_images:\n        dataset = dataset.map(add_images)\n    dataset = dataset.map(setshape)\n    dataset = dataset.map(split_xy)\n\n    # Compile dataset\n    if cached:\n        dataset = dataset.cache()\n    dataset = dataset.shuffle(shuffle_buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n    return dataset","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load dataset features from csv files, split them and preprocess them\nfeatures = load_features(image_dir, caption_pred_file, concept_det_file, concepts_onehot, filter_percent=filter_percent_dataset)\nfeat_train, feat_val, feat_test = split(features, test_size=0.2, val_size=0.0, seed=seed)\n\nin_feat_typ = {'caption': tf.string, 'concepts': tf.bool, 'image path': tf.string}\nx_features_eval = ['image path', 'image']\ny_features_eval = ['caption', 'concepts']\n\ntrain_ds_size = len(feat_train) if feat_train else 0\nval_ds_size = len(feat_val) if feat_val else 0\ntest_ds_size = len(feat_test) if feat_test else 0\n\ntrain_dataset_eval = create_dataset(feat_train, input_features_types=in_feat_typ, feature_shapes=feature_shapes, x_features=x_features_eval, y_features=y_features_eval, x_dict=True, y_dict=True, batch_size=1, shuffle_buffer_size=1)\ntest_dataset_eval = create_dataset(feat_test, input_features_types=in_feat_typ, feature_shapes=feature_shapes, x_features=x_features_eval, y_features=y_features_eval, x_dict=True, y_dict=True, batch_size=1, shuffle_buffer_size=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Import","metadata":{}},{"cell_type":"code","source":"def projection(embedding_input, embed_dim, name):\n    \n    embeddings = tfkl.Dense(embed_dim, name=f'{name}_1')(embedding_input)\n    x = tf.nn.selu(embeddings)\n    x = tfkl.Dense(embed_dim, name=f'{name}_2')(x)\n    x = tfkl.Dropout(0.1)(x)\n    x = tfkl.Add()([x, embeddings])\n    embeddings = tfkl.LayerNormalization()(x)\n\n    return embeddings\n\ndef image_encoder(input_shape, embed_dim, seed=42, supernet=None, preprocessing=None):\n    \n    tf.random.set_seed(seed)\n\n    input_layer = tfkl.Input(shape=input_shape, name='img_input_layer')\n    x = preprocessing(input_layer)\n    x = supernet(x)\n    x = tfkl.GlobalAveragePooling2D(name='GAP')(x)\n\n    x = projection(x, embed_dim, 'img_embedding_dense_layer')\n    \n    # Connect input and output through the Model class\n    cnn_encoder = tfk.Model(inputs=input_layer, outputs=x, name='image_encoder')\n\n    # Return the encoder\n    return cnn_encoder\n\ndef text_encoder(embed_dim, preprocess, transformer, trainable=True):\n\n    transformer.trainable = trainable\n    \n    input_layer = tfkl.Input(shape=(), dtype=tf.string, name=\"text_input\")\n    x = preprocess(input_layer)\n    x = transformer(x)[\"pooled_output\"]\n    x = projection(x, embed_dim, 'txt_embedding_dense_layer')\n\n    text_encoder = tfk.Model(inputs=input_layer, outputs=x, name=\"text_encoder\")\n    \n    return text_encoder\n\nclass CLIP(tfk.Model):\n    def __init__(self, image_encoder, text_encoder, **kwargs):\n        super().__init__(**kwargs)\n        self.image_encoder = image_encoder\n        self.text_encoder = text_encoder\n        self.loss_tracker = tfk.metrics.Mean(name=\"loss\")\n        self.temp = self.add_weight(name='t',\n                                 shape=(1, ),\n                                 initializer=tfk.initializers.Constant(1.),\n                                 trainable=True)\n\n        self.call_model()\n\n        \n    @property\n    def metrics(self):\n        return [self.loss_tracker]\n\n    def call(self, features, training=False):\n        image_emb = self.image_encoder(features[\"image\"], training=training)\n        text_emb = self.text_encoder(features[\"caption\"], training=training)\n        return image_emb, text_emb\n\n    def CLIP_loss(self, image_emb, text_emb):\n        norm_image_emb = tf.math.l2_normalize(image_emb, axis=1)\n        norm_text_emb = tf.math.l2_normalize(text_emb, axis=1)\n\n        logits = tf.linalg.matmul(norm_image_emb, norm_text_emb, transpose_b=True) * tf.math.exp(self.temp)\n\n        n = tf.shape(logits)[0]\n        labels = tf.range(n)\n\n        labels = tf.one_hot(labels, n)\n\n        loss_img = tfk.losses.categorical_crossentropy(labels, logits, from_logits=True)\n        loss_txt = tfk.losses.categorical_crossentropy(labels, kb.transpose(logits), from_logits=True)\n\n        return (loss_img + loss_txt) / tf.constant(2.0)\n\n    def train_step(self, features):\n        with tf.GradientTape() as tape:\n            image_embeddings, caption_embeddings = self(features, training=True)\n            loss = self.CLIP_loss(caption_embeddings, image_embeddings)\n\n        gradients = tape.gradient(loss, self.trainable_variables)\n        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n\n        self.loss_tracker.update_state(loss)\n        return {\"loss\": self.loss_tracker.result()}\n\n    def test_step(self, features):\n        image_embeddings, caption_embeddings = self(features, training=False)\n        loss = self.CLIP_loss(caption_embeddings, image_embeddings)\n        self.loss_tracker.update_state(loss)\n        return {\"loss\": self.loss_tracker.result()}\n\n    def call_model(self):\n\n        image = tf.reshape(tf.convert_to_tensor(np.zeros((128,128,3))), (1,128,128,3))\n        caption = tf.convert_to_tensor([\"Hello there\"], dtype=tf.string)\n\n        sample = {\"image\": image, \"caption\": caption}\n\n        self(sample)\n\n    def summary(self):\n        super().summary()\n\n        print(\"\\n\")\n        self.image_encoder.summary()\n\n        print(\"\\n\")\n        self.text_encoder.summary()\n\ndef build_clip(img_supernet,\n               img_preprocess,\n               text_transformer,\n               text_preprocess,\n               img_input_shape=(128,128,3),\n               txt_input_shape=(393, ), \n               embed_dim=64, \n               learning_rate=2e-5):\n\n    \n    text_encoder_model = text_encoder(embed_dim, text_preprocess, text_transformer)\n    image_encoder_model = image_encoder(img_input_shape, embed_dim, supernet=img_supernet, preprocessing=img_preprocess)\n\n    clip = CLIP(image_encoder_model, text_encoder_model)\n    clip.compile(optimizer = tf.optimizers.AdamW(learning_rate=learning_rate))\n\n    return image_encoder_model, text_encoder_model, clip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_preprocess = hub.KerasLayer(\n        \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n        name=\"text_preprocessing\",\n    )\n\ntext_transformer = hub.KerasLayer(\n        \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\",\n        trainable=True,\n        name=\"bert\",\n    )\n\nimg_preprocess = tfk.applications.convnext.preprocess_input\nimg_supernet = tfk.applications.ConvNeXtTiny(weights='imagenet', include_top=False)\nsupernet_name = img_supernet.name\n\nclip_image_encoder, clip_text_encoder, clip = build_clip(img_supernet, img_preprocess, text_transformer, text_preprocess)\n\nimg_supernet.trainable = False\ntext_transformer.trainable = False\n\nclip.load_weights(clip_weights_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"markdown","source":"### Evaluation Definitions","metadata":{}},{"cell_type":"markdown","source":"#### Evaluation Variables","metadata":{}},{"cell_type":"code","source":"k = 10\nconcept_overlap_threshold = 2\ndecimal_precision = 4\n# Function to preprocess data when we want to evaluate captions\nreference_preprocess_cap = lambda x: x[\"caption\"].numpy().decode('UTF-8')          \n# Function to preprocess data when we want to evaluate concepts\nreference_preprocess_con = lambda x: x[\"concepts\"].numpy()\nreference_preprocess_con_hash = lambda x: frozenset(sorted(np.where(x[\"concepts\"].numpy())[0]))\n# Function to compute if a match is relevant given concept arrays \nconcept_relevance = lambda m, o: np.count_nonzero(np.logical_and(m, o)) >= concept_overlap_threshold\nconcept_relevance_hash = lambda m, o: len(m.intersection(o)) >= concept_overlap_threshold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"METRIC_ACCURACY = \"Accuracy\"\nMETRIC_MAP = \"MAP\"\nMETRIC_MAR = \"MAR\"\nMETRIC_F1 = \"F1\"\n\nmetrics = [\n    {\"id\": METRIC_ACCURACY, \"name\": \"Accuracy\", \"color\": \"green\"},\n    {\"id\": METRIC_MAP, \"name\": \"Mean Average Precision\", \"color\": \"blue\"},\n    {\"id\": METRIC_MAR, \"name\": \"Mean Average Recall\", \"color\": \"red\"},\n    {\"id\": METRIC_F1, \"name\": \"F1 Score\", \"color\": \"blueviolet\"}\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Evaluation Functions","metadata":{}},{"cell_type":"code","source":"# Generate the embeddings and the corresponding dataset reference for an image dataset\ndef generate_image_embeddings(\n    image_encoder,                 # Image encoder of clip model\n    dataset_eval,                  # Dataset to generate embeddings (WARNING: the dataset must not be shuffling or have a shuffle buffer size of 1)\n    dataset_pred_map=lambda *x: x, # Lambda mapping function for prediction\n    dataset_ref_map=lambda *x: x,  # Lambda mapping function for reference\n):\n    print(\"Generating image embeddings\")\n    # Generate image embedding\n    image_embeddings = image_encoder.predict(\n        dataset_eval.map(dataset_pred_map),\n        verbose=1,\n    )\n    # Construct reference dataset for retrieving side data of elements\n    dataset_reference = [e for e in dataset_eval.map(dataset_ref_map).unbatch()]\n    return dataset_reference, image_embeddings\n\n# Return the results in the form of reference dataset indexes of a text to image retrieval for a series of queries\ndef find_t2i_matches(\n    queries,                # Queries to search\n    text_encoder,           # Text encoder of clip model\n    image_embeddings,       # Generated image embeddings\n    k=10,                   # Number of elements for top-k\n    normalize=True,         # Embedding normalization\n):\n    print(\"Computing Text-to-Image matches\")\n    # Generate query dataset and get their embeddings\n    queries_ds = tf.data.Dataset.from_tensor_slices(queries).batch(batch_size)\n    query_embedding = text_encoder.predict(queries_ds)\n    # Normalize the query and the image embeddings\n    if normalize:\n        image_embeddings = tf.math.l2_normalize(image_embeddings, axis=1)\n        query_embedding = tf.math.l2_normalize(query_embedding, axis=1)\n    # Compute the dot product between the query and the image embeddings\n    dot_similarity = tf.matmul(query_embedding, image_embeddings, transpose_b=True)\n    # Retrieve top k indices\n    results = tf.math.top_k(dot_similarity, k).indices.numpy()\n    return results\n\n# Extract the reference dataset objects given a list of indexes\ndef index_to_reference(results, dataset_reference):\n    return [[dataset_reference[match] | {\"index\": match} for match in result] for result in results]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Retrieve relevant items given a list of queries (DO NOT RUN THIS ON A COMPLETE DATASET!!!)\ndef retrieve_relevant(queries, reference_preprocess=lambda x: x, relevance=lambda m, o: m == o):\n    return [\n        [element for element in map(reference_preprocess, dataset_reference) if relevance(query, element)]\n        for query in queries\n    ]\n\n# Compute the number of relevant items in the first k matches in a list of results\n# If queries is None, it is assumed that the queries ran to obtain the list of results are parallel to the elements in dataset_reference\ndef compute_relevant_at_k(results, dataset_reference, queries=None, k=None, reference_preprocess=lambda x: x, relevance=lambda m, o: m == o):\n    if not k:\n        k = len(results[0])\n    if queries:\n        relevant_reference = retrieve_relevant(queries, reference_preprocess=reference_preprocess, relevance=relevance)\n    else:\n        relevant_reference = map(reference_preprocess, dataset_reference)\n    return [ \n        np.count_nonzero([relevance(match, reference) for match in list(map(reference_preprocess, matches))[0:k]])\n        for matches, reference in zip(results, relevant_reference) \n    ]\n\n# Computes the total number of relevant elements for a dataset or queries\n# It is assumed that the element returned by reference_preprocess is hashable and can be used as a dictionary key\n# If queries is None, it is assumed that the queries ran to obtain the list of results are parallel to the elements in dataset_reference\ndef compute_total_relevance(dataset_reference, queries=None, reference_preprocess=lambda x: x, relevance=lambda m, o: m == o):\n    # Check if queries are passed, if so run general function\n    if queries:\n        relevant_reference = retrieve_relevant(queries, reference_preprocess=reference_preprocess, relevance=relevance)\n        return [len(e) for e in relevant_reference]\n    # Build preprocessed dataset\n    relevant_reference = list(map(reference_preprocess, dataset_reference))\n    total_n = {}\n    # Iterate through dataset and count equal items\n    for element in relevant_reference:\n        if element in total_n:\n            total_n[element] += 1\n        else:\n            total_n[element] = 1\n    # Check bytecode of relevance function to determine if the relevance function is equality,\n    # if so, return counts, otherwise apply relevance to the whole dataset\n    if not relevance.__code__.co_code == (lambda m, o: m == o).__code__.co_code:\n        total_n = {element: sum([total_n[x] for x in total_n if relevance(x, element) and element != x]) + 1 for element in tqdm(total_n)} \n    return [total_n[element] for element in relevant_reference]\n\ndef load_relevance_from_csv(filename, dataset_reference, reference_preprocess=lambda x: x):\n    # TODO\n    pass\n\ndef save_relevance_to_csv(filename, total_n):\n    # TODO\n    # df = pd.DataFrame(test_tot_relevant_con, index=[0]) \n    # df.to_csv(r'TotRelevant_Train_1.csv', index=False, header=True)\n    pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_top_k_accuracy(results, dataset_reference, relevant_at_k):\n    hits = np.count_nonzero(relevant_at_k)\n    return hits / len(dataset_reference)\n\ndef compute_map_k(results, dataset_reference, relevant_at_k, k=None):\n    if not k:\n        k = len(results[0])\n    precision_at_k = [r/k for r in relevant_at_k]\n    return np.sum(precision_at_k) / len(dataset_reference)\n\ndef compute_mar_k(results, dataset_reference, relevant_at_k, total_relevant):\n    recall_at_k = [rk/tr for rk, tr in zip(relevant_at_k, total_relevant)]\n    return np.sum(recall_at_k) / len(dataset_reference)\n\ndef compute_F1_k(precision=0, recall=0):\n    if precision + recall == 0:\n        f1_score = 0\n    else:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n        return f1_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize results for text to image queries\ndef visualize_t2i_results(query, matches):\n    print(\"Top matches for query: \\\"\" + query + \"\\\"\")\n    if \"image path\" in matches[0]:\n        plt.figure(figsize=(18, 18))\n    for i in range(len(matches)):\n        if \"image path\" in matches[i]:\n            path = matches[i][\"image path\"].numpy().decode('UTF-8')\n            ax = plt.subplot(3, 3, i + 1)\n            plt.imshow(mpimg.imread(path))\n            plt.axis(\"off\")\n        if \"caption\" in matches[i]:\n            caption = matches[i][\"caption\"].numpy().decode('UTF-8')\n            print(f\"{i}) {caption}\")\n        \n# Standard isualization for a multi-purpose plotly graph\ndef visualize_multigraph(functions, titlexyf=(None, None, None), legend=True):\n    fig = go.Figure()\n    for function in functions:\n        x = function['x']\n        y = function['y']\n        label = function['label'] if 'label' in function else \"\"\n        color = function['color'] if 'color' in function else None\n        linestyle = function['style'] if 'style' in function else \"solid\"\n        marker = go.scatter.Marker(symbol=function['marker']) if 'marker' in function else None\n        opacity = function['opacity'] if 'opacity' in function else 1\n        k = len(x)\n        fig.add_trace(go.Scatter(\n            x=x, y=y,\n            line=go.scatter.Line(color=color, dash=linestyle),\n            opacity=opacity,\n            marker=marker,\n            mode=\"lines+markers+text\" if marker else \"lines+text\",\n            name=label,\n        ))\n    fig.update_xaxes(\n        title=titlexyf[0],\n        ticks=\"outside\", ticklen=8, minor=dict(dtick=0.5, ticklen=6, tickcolor=\"black\", showgrid=True), ticklabelstep=1, dtick=1, \n        range=(1,k), \n    )\n    fig.update_yaxes(\n        title=titlexyf[1],\n        ticks=\"outside\", ticklen=8, minor=dict(dtick=0.01, ticklen=6, tickcolor=\"black\", showgrid=True), ticklabelstep=1, dtick=0.1,\n    )\n    fig.update_layout(\n        title=titlexyf[2],\n        width=900, height=600,\n        margin=dict(l=50, r=50, b=20, t=40, pad=4),\n        paper_bgcolor=\"LightSteelBlue\",\n    )\n    fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute baselines for retrieval\n# Assumption of sampling with repetitions, results get more inaccurate as k/l -> inf\ndef retrieval_baselines(dataset_reference, total_relevant, k, metrics=[]):\n    l = len(dataset_reference)\n    metrics_out = {}\n    for metric in metrics:\n        if metric[\"id\"] == METRIC_ACCURACY:\n            metrics_out[METRIC_ACCURACY] = sum([ 1 - pow((l - n_el) / l, k) for n_el in total_relevant]) / l\n        elif metric[\"id\"] == METRIC_MAP:\n            metrics_out[METRIC_MAP] = sum([ n_el / l for n_el in total_relevant]) / l\n        elif metric[\"id\"] == METRIC_MAR:\n            metrics_out[METRIC_MAR] = sum([ k / l for n_el in total_relevant]) / l\n        elif metric[\"id\"] == METRIC_F1:\n            metrics_out[METRIC_F1] = compute_F1_k(metrics_out[METRIC_MAP], metrics_out[METRIC_MAR])\n    return metrics_out\n    \n# Computation of a retrieval report containing metrics\ndef retrieval_report(\n    results, reference, relevant,   # Task results, dataset reference and relevant hits at k for task\n    tot_relevant=None,              # Rotal number of relevant elements for each dataset element\n    k=None,                         # k for metrics computation (should be less or equal than k of retrieval)\n    baselines=True,                 # Calculate baselines alongside metrics\n    metrics=[],                     # Metrics to take into consideration\n    output=True,                    # Print outputs to stdout\n    title=\"Retrieval Report\",       # Title of the report\n    decimal_precision=4,            # Decimal precision of values\n):\n    if not k:\n        k = len(results[0])\n    metrics_out = {}\n    \n    for metric in metrics:\n        if metric[\"id\"] == METRIC_ACCURACY:\n            metrics_out[METRIC_ACCURACY] = compute_top_k_accuracy(results, reference, relevant)\n        elif metric[\"id\"] == METRIC_MAP:\n            metrics_out[METRIC_MAP] = compute_map_k(results, reference, relevant, k=k)\n        elif metric[\"id\"] == METRIC_MAR:\n            metrics_out[METRIC_MAR] = compute_mar_k(results, reference, relevant, tot_relevant)\n        elif metric[\"id\"] == METRIC_F1:\n            metrics_out[METRIC_F1] = compute_F1_k(metrics_out[METRIC_MAP], metrics_out[METRIC_MAR])\n            \n    if baselines:\n            baselines = retrieval_baselines(reference, tot_relevant, k, metrics=metrics)\n            \n    if output:\n        print(f\"\\n ### {title} ###\")\n        for metric in metrics:\n            string = f\"{metric['name']:<30}: {round(metrics_out[metric['id']] * 100, decimal_precision):10}%\"\n            if baselines:\n                string += f\"{'   Baseline':<8}: {round(baselines[metric['id']] * 100, decimal_precision):10}%\"\n            print(string)\n    \n    if baselines:\n        return metrics_out, baselines\n    return metrics_out\n        \n# Computation of a retrieval report in graph formcontaining metrics \ndef retrieval_graph_report(\n    results, reference,                 # Task results, dataset reference and relevant hits at k for task\n    tot_relevant=None,                  # Total number of relevant elements for each dataset element\n    k_range=(1, 10),                    # k range for metrics computation (maximum value shoul not be greater than k of retrieval)\n    baselines=True,                     # Calculate baselines alongside metrics\n    metrics=[],                         # Metrics to take into consideration\n    titlexyf=(None, None, None),        # Tuple containing: (title of x axis, title of y axis, figure title)\n    reference_preprocess=lambda x: x,   # Function to preprocess data contained in the reference dataset\n    relevance=lambda m, o: m == o,      # Function to compare elements\n):\n    functions = {metric[\"id\"]: {\"x\": [], \"y\": [], \"label\": metric[\"id\"], \"color\": metric[\"color\"], \"marker\": \"0\", \"opacity\": 0.8} for metric in metrics}\n    if baselines:\n        functions |= {metric[\"id\"] + \"_base\": {\"x\": [], \"y\": [], \"label\": metric[\"id\"] + \" Baseline\", \"color\": metric[\"color\"], \"style\": \"dash\", \"opacity\": 0.5} for metric in metrics}\n    for k in range(k_range[0], k_range[1] + 1):\n        relevant = compute_relevant_at_k(results, reference, k=k, reference_preprocess=reference_preprocess, relevance=relevance)\n        metrics_out, baselines = retrieval_report(results, reference, relevant, tot_relevant, k=k, baselines=baselines, metrics=metrics, output=False)\n        for metric in metrics_out:\n            functions[metric][\"x\"].append(k)\n            functions[metric][\"y\"].append(metrics_out[metric])\n            if baselines:\n                functions[metric + \"_base\"][\"x\"].append(k)\n                functions[metric + \"_base\"][\"y\"].append(baselines[metric])\n    visualize_multigraph(functions.values(), titlexyf)\n    return functions\n    \n# Manually compute some text to image queries\ndef manual_t2i_queries(queries, text_encoder, image_embeddings, dataset_reference, k=10, normalize=True):\n    results = find_t2i_matches(queries, clip_text_encoder, test_image_embeddings, k=k, normalize=normalize)\n    results = index_to_reference(results, test_dataset_reference)\n    for query, matches in zip(queries, results):\n        visualize_t2i_results(query, matches)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test Set Evaluation","metadata":{}},{"cell_type":"code","source":"print(\"\\n### Scoring test data ###\")\ntest_dataset_reference, test_image_embeddings = generate_image_embeddings(\n    clip_image_encoder,\n    test_dataset_eval,\n    dataset_pred_map=lambda x, y: x['image'],\n    dataset_ref_map=lambda x, y: y | {'image path': x['image path']}\n)\ntest_queries = [e[\"caption\"] for e in test_dataset_reference]\n# Compute relevance for all the queries in the dataset using only caption equality as a metric\ntest_tot_relevant_cap = compute_total_relevance(test_dataset_reference, reference_preprocess=reference_preprocess_cap)\ntest_tot_relevant_con = compute_total_relevance(test_dataset_reference, reference_preprocess=reference_preprocess_con_hash, relevance=concept_relevance_hash)\n# Compute matching results and extrapolate relevant matches based on different criterions\ntest_raw_results = find_t2i_matches(test_queries, clip_text_encoder, test_image_embeddings, k=k, normalize=True)\ntest_results = index_to_reference(test_raw_results, test_dataset_reference)\ntest_relevant_cap = compute_relevant_at_k(test_results, test_dataset_reference, k=k, reference_preprocess=reference_preprocess_cap)\ntest_relevant_con = compute_relevant_at_k(test_results, test_dataset_reference, k=k, reference_preprocess=reference_preprocess_con, relevance=concept_relevance)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Caption equality relevance metric","metadata":{}},{"cell_type":"code","source":"_ = retrieval_report(\n    test_results, test_dataset_reference, test_relevant_cap, test_tot_relevant_cap,\n    k=k,\n    metrics=metrics,\n    title=\"Test Data - Caption equality metrics\",\n    decimal_precision=decimal_precision\n)\n_ = retrieval_graph_report(\n    test_results, test_dataset_reference, test_tot_relevant_cap,\n    k_range=(1, k),\n    metrics=metrics, \n    titlexyf=(\"k\", None, \"Test Data - Caption equality metrics\"),\n    reference_preprocess=reference_preprocess_cap\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Concept overlap relevance metric","metadata":{}},{"cell_type":"code","source":"_ = retrieval_report(\n    test_results, test_dataset_reference, test_relevant_con, test_tot_relevant_con,\n    k=k,\n    metrics=metrics,\n    title=\"Test Data - Concept overlap metrics\",\n    decimal_precision=decimal_precision,\n)\n_ = retrieval_graph_report(\n    test_results, test_dataset_reference, test_tot_relevant_con,\n    metrics=metrics,\n    k_range=(1, k),\n    titlexyf=(\"k\", None, \"Test Data - Concept overlap metrics\"),\n    reference_preprocess=reference_preprocess_con, relevance=concept_relevance,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training Set Evaluation","metadata":{}},{"cell_type":"code","source":"print(\"### Scoring training data ###\")\ntrain_dataset_reference, train_image_embeddings = generate_image_embeddings(\n    clip_image_encoder,\n    train_dataset_eval,\n    dataset_pred_map=lambda x, y: x['image'],\n    dataset_ref_map=lambda x, y: y | {'image path': x['image path']}\n)\ntrain_queries = [e[\"caption\"] for e in train_dataset_reference]\n# Compute relevance for all the queries in the dataset using only caption equality as a metric\ntrain_tot_relevant_cap = compute_total_relevance(train_dataset_reference, reference_preprocess=reference_preprocess_cap)\ntrain_tot_relevant_con = compute_total_relevance(test_dataset_reference, reference_preprocess=reference_preprocess_con_hash, relevance=concept_relevance_hash)\n# Compute matching results and extrapolate relevant matches based on different criterions\ntrain_raw_results = find_t2i_matches(train_queries, clip_text_encoder, train_image_embeddings, k=k, normalize=True)\ntrain_results = index_to_reference(train_raw_results, train_dataset_reference)\ntrain_relevant_cap = compute_relevant_at_k(train_results, train_dataset_reference, k=k, reference_preprocess=reference_preprocess_cap)\ntrain_relevant_con = compute_relevant_at_k(train_results, train_dataset_reference, k=k, reference_preprocess=reference_preprocess_con, relevance=concept_relevance)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Caption equality relevance metric","metadata":{}},{"cell_type":"code","source":"_ = retrieval_report(\n    train_results, train_dataset_reference, train_relevant_cap, train_tot_relevant_cap,\n    k=k,\n    metrics=metrics,\n    title=\"Training Data - Caption equality metric\",\n    decimal_precision=decimal_precision\n)\n_ = retrieval_graph_report(\n    train_results, train_dataset_reference, train_tot_relevant_cap,\n    k_range=(1, k),\n    metrics=metrics, \n    titlexyf=(\"k\", None, \"Training Data - Caption equality metrics\"),\n    reference_preprocess=reference_preprocess_cap\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Concept overlap relevance metric","metadata":{}},{"cell_type":"code","source":"_ = retrieval_report(\n    train_results, train_dataset_reference, train_relevant_con, train_tot_relevant_con,\n    k=k,\n    metrics=metrics,\n    title=\"Training Data - Concept overlap metric\",\n    decimal_precision=decimal_precision\n)\n_ = retrieval_graph_report(\n    train_results, train_dataset_reference, train_tot_relevant_con,\n    k_range=(1, k),\n    metrics=metrics, \n    titlexyf=(\"k\", None, \"Training - Caption equality metrics\"),\n    reference_preprocess=reference_preprocess_cap\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
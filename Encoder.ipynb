{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c33eb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "kb = tf.keras.backend\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69a0b40",
   "metadata": {},
   "source": [
    "### Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b64e239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# Random seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf60401a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_embedding(input_shape, latent_dim, embed_dim, seed):\n",
    "    tf.random.set_seed(seed)\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='img_input_layer')\n",
    "    x = tfkl.ZeroPadding2D((2,2))(input_layer)\n",
    "\n",
    "    x = tfkl.Conv2D(64, 3, padding='same', strides=2)(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.ReLU()(x)\n",
    "\n",
    "    x = tfkl.Conv2D(128, 3, padding='same', strides=2)(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.ReLU()(x)\n",
    "\n",
    "    x = tfkl.Conv2D(256, 3, padding='same', strides=2)(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.ReLU()(x)\n",
    "\n",
    "    x = tfkl.Flatten()(x)\n",
    "    x = tfkl.Dense(latent_dim, activation='relu')(x)\n",
    "    x = tfkl.Dense(embed_dim, name='img_embedding_output_layer')(x)\n",
    "\n",
    "    # Connect input and output through the Model class\n",
    "    cnn_encoder = tfk.Model(inputs=input_layer, outputs=x, name='image_encoder')\n",
    "\n",
    "    # Return the discriminator\n",
    "    return cnn_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59ebc3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(tfkl.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Embedding layer for the token\n",
    "        self.token_emb = tfkl.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        # Embedding layer for the position\n",
    "        self.pos_emb = tfkl.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Find the maximum length of the input\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        # Create a tensor with positions from 0 to maxlen-1\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        # Embed the positions\n",
    "        positions = self.pos_emb(positions)\n",
    "        # Embed the tokens\n",
    "        x = self.token_emb(x)\n",
    "        # Add the token and position embeddings\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e8abbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(tfkl.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.att = tfkl.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tfk.Sequential(\n",
    "            [\n",
    "                tfkl.Dense(ff_dim, activation=\"relu\"), \n",
    "                tfkl.Dense(embed_dim)\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = tfkl.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tfkl.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tfkl.Dropout(rate)\n",
    "        self.dropout2 = tfkl.Dropout(rate)\n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "        # Self-attention\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        # Apply dropout to the attention output\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        # Add the attention output to the input and normalize\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        # Feed-forward\n",
    "        ffn_output = self.ffn(out1)\n",
    "        # Apply dropout to the feed-forward output\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        # Add the feed-forward output to the previous output and normalize\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22defb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_embedding(sequence_lenght, vocab_size, num_heads, embed_dim, latent_dim):\n",
    "    \n",
    "    input_layer = tfk.Input(shape=(None,), dtype=\"int64\", name=\"text_inputs\")\n",
    "    x = TokenAndPositionEmbedding(sequence_lenght, vocab_size, embed_dim)(input_layer)\n",
    "    x = TransformerEncoderBlock(embed_dim, num_heads, latent_dim)(x)\n",
    "    \n",
    "    text_encoder = tfk.Model(input_layer, x)\n",
    "    \n",
    "    return text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1310cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def combiner(img_embed, txt_embed, temp=0.07):\n",
    "#    img_norm = tfkl.Lambda(lambda x: kb.l2_normalize(x, axis=1))(img_embed)\n",
    "#    txt_norm = tfkl.Lambda(lambda x: kb.l2_normalize(x, axis=1))(txt_embed)\n",
    "#    \n",
    "#    logits = tfkl.Lambda(lambda x,y: kb.dot(x, kb.transpose(y)) * kb.exp(temp))(img_norm, txt_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27b1f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CLIP_loss(_, logits):\n",
    "    labels = np.arange(y_pred.shape[0])\n",
    "    \n",
    "    loss_img = tfk.losses.categorical_crossentropy(logits, labels)\n",
    "    loss_txt = tfk.losses.categorical_crossentropy(kb.transpose(logits), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "894ee5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clip(img_input_shape=(128,128,3),\n",
    "               txt_input_shape=(None, ), \n",
    "               latent_dim=1024, \n",
    "               embed_dim=128, \n",
    "               seq_lenght=393, \n",
    "               vocab_size=35491, \n",
    "               num_heads=4,\n",
    "               temp=0.07):\n",
    "    \n",
    "    img_input = tfk.Input(shape=img_input_shape)\n",
    "    txt_input = tfk.Input(shape=txt_input_shape)\n",
    "    \n",
    "    img_embed = CNN_embedding(img_input_shape, latent_dim, embed_dim, seed=42)(img_input)\n",
    "    txt_embed = text_embedding(seq_lenght, vocab_size, num_heads, embed_dim, latent_dim)(txt_input)\n",
    "    \n",
    "    img_norm = tfkl.Lambda(lambda x: kb.l2_normalize(x, axis=1))(img_embed)\n",
    "    txt_norm = tfkl.Lambda(lambda x: kb.l2_normalize(x, axis=1))(txt_embed)\n",
    "    \n",
    "    logits = tfkl.Lambda(lambda x: kb.dot(x[0], kb.transpose(x[1])) * kb.exp(temp))((img_norm, txt_norm))\n",
    "    \n",
    "    clip = tfk.Model(inputs=[img_input, txt_input], outputs=logits)\n",
    "    return clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb6af2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " image_encoder (Functional)     (None, 128)          76264448    ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, None, 128)    5120768     ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_3 (Lambda)              (None, 128)          0           ['image_encoder[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_4 (Lambda)              (None, None, 128)    0           ['model_1[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_5 (Lambda)              (None, 128, None)    0           ['lambda_3[0][0]',               \n",
      "                                                                  'lambda_4[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 81,385,216\n",
      "Trainable params: 81,384,320\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_clip()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f1d9640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "tfk.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38037f16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tf2] *",
   "language": "python",
   "name": "conda-env-.conda-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

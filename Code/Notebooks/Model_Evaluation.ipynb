{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Declarations","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import re\nimport os\nimport math\nimport string\nimport random\nimport requests\nimport importlib\nimport itertools\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimport plotly.graph_objects as go\n\nfrom tqdm import tqdm\n\nfrom IPython.display import display\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\ntfk = tf.keras\ntfkl = tf.keras.layers\nkb = tf.keras.backend","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tf.__version__)\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Constants","metadata":{}},{"cell_type":"code","source":"# Randomness\nseed = 42\n\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filepaths\nkaggle = True\n\nmodel_versions = [\"v4.0\"]\n\ngithub_repo = \"raul-singh/Rise-of-Transformers-Project\"\ngithub_branch = \"main\"\ngithub_python_prefix = [\"Code\", \"Notebooks\", \"py_files\"]\ngithub_clip_models_prefix = [\"Code\", \"Models\"] if kaggle else [\"..\", \"Models\"]\ngithub_pyfiles_data = [\n    {\"name\": \"preprocessing\", \"imports\": [\"import_datasets\"]}, \n    {\"name\": \"clip\", \"imports\": [\"build_clip\"]}\n]\ngithub_pyfiles = [\"/\".join(github_python_prefix) + \"/\" + pf[\"name\"] + \".py\" for pf in github_pyfiles_data]\ngithub_clip_models = [f\"{'/'.join(github_clip_models_prefix)}/{version}.yaml\" for version in model_versions]\n\nkaggle_dataset1 = \"/kaggle/input/transformers-hackathon/\"\nkaggle_dataset2 = \"/kaggle/input/transformers-hackathon-features/\"\nkaggle_weights = \"/kaggle/input/clip-weights/\"\nkaggle_relevance = \"/kaggle/input/clip-relevance/\"\n\nimage_dir = \"./resized_train\"\nrelevance_dir = \"./relevance\"\ncaption_pred_file = \"caption_prediction_train.csv\"\nconcept_det_file = \"concept_detection_train.csv\"\nconcept_file = \"concepts.csv\"\nclip_weights_files = [f\"{version}.h5\" for version in model_versions] if kaggle else [None for _ in model_versions]\n\nif kaggle:\n    image_dir = kaggle_dataset1 + image_dir\n    relevance_dir = kaggle_relevance + relevance_dir\n    caption_pred_file = kaggle_dataset2 + caption_pred_file\n    concept_det_file = kaggle_dataset2 + concept_det_file\n    concept_file = kaggle_dataset2 + concept_file\n    clip_weights_files = [kaggle_weights + weight for weight in clip_weights_files]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train/Val/Test split and filter percentages\ntest_size = 0.2\nval_size = 0\nfilter_percent_dataset = 1\n\n# Batch size\nbatch_size = 32\n\n# Import dataset types and shapes\nin_feat_typ = {'caption': tf.string, 'concepts': tf.bool, 'image path': tf.string}\nfeature_shapes = {'image': (128, 128, 3), 'caption': (), 'concepts': (8374)}\n\n# Output dataset structure\nx_features_eval = ['image path', 'image']\ny_features_eval = ['caption', 'concepts']\n\n# Define parameters for dataset import\ndataset_parameters = [{\n    'x_features': x_features_eval, 'y_features': y_features_eval,\n    'x_dict': True, 'y_dict': True,           \n    'shuffle_buffer_size': 1,\n    'batch_size': batch_size,\n    'cached': True,\n}]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Meta-Imports","metadata":{}},{"cell_type":"code","source":"def clean_recursive_imports(source, import_list, prefix):\n    import_prefix = re.sub(r\"/\", \".\", prefix)\n    for target_import in import_list:\n        source = re.sub(r\"from[ \\t]+\" + re.escape(target_import) + r\"[ \\t]+import\", f\"from {import_prefix + target_import} import\", source)\n    return source\n    \ndef import_py_from_repo(repository, branch, filepath, prefix, recursive_imports_list=None):\n    # Build path for retrieval and write name\n    path_pre = \"https://raw.githubusercontent.com/\"\n    path = path_pre + repository + \"/\" + branch + \"/\" + filepath \n    write_path = prefix + filepath.split(\"/\")[-1]\n    print(\"Downloading file from \" + path)\n    # Obtain raw text from file\n    text = requests.get(path).text\n    # Clean recursive imports\n    text = clean_recursive_imports(text, recursive_imports_list, prefix) if recursive_imports_list else text\n    # Create subdirectories if not exist\n    os.makedirs(os.path.dirname(write_path), exist_ok=True)\n    # Write file\n    f = open(write_path, \"w\")\n    f.write(text)\n    f.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if kaggle:\n    for pf_data, py_file in zip(github_pyfiles_data, github_pyfiles):\n        import_py_from_repo(\n            github_repo, github_branch, py_file, \n            \"/\".join(github_python_prefix) + \"/\", \n            recursive_imports_list=[pf[\"name\"] for pf in github_pyfiles_data],\n        )\n        import_string = f'from {\".\".join(github_python_prefix) + \".\" + pf_data[\"name\"]} import {\", \".join(pf_data[\"imports\"])}'\n        exec(import_string)\n    \n    for model in github_clip_models:\n        import_py_from_repo(github_repo, github_branch, model, \"/\".join(github_clip_models_prefix) + \"/\")\n        \nelse:\n    for pf_data in github_pyfiles_data:\n        import_string = f'from py_files.{pf_data[\"name\"]} import {\", \".join(pf_data[\"imports\"])}'\n        exec(import_string)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"concept_info, datasets, dataset_sizes = import_datasets(\n    image_dir, caption_pred_file, concept_file, concept_det_file,\n    in_feat_typ, feature_shapes,\n    dataset_parameters,\n    filter_percent_dataset,\n    test_size, val_size,\n    seed,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select loaded datasets and variables\nconcept_list, concepts_onehot = concept_info\n_, _, test_dataset = datasets[0]\ntrain_ds_size, val_ds_size, test_ds_size = dataset_sizes\n\ndel datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Import","metadata":{}},{"cell_type":"code","source":"models = []\nfor structure, weights in zip(github_clip_models, clip_weights_files):\n    print(f\"Creating model {structure}\")\n    clip_image_encoder, clip_text_encoder, clip = build_clip(structure, weights_path=weights)\n    models.append({\n        \"image_encoder\": clip_image_encoder,\n        \"clip_text_encoder\": clip_text_encoder,\n        \"clip\": clip,\n    })","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select the loaded model to evaluate\nclip_image_encoder, clip_text_encoder, clip = models[0].values()\n\ndel models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"markdown","source":"## Evaluation Definitions","metadata":{}},{"cell_type":"markdown","source":"### Evaluation Variables","metadata":{}},{"cell_type":"code","source":"# Top-k number\nk = 10\n# Threshold for concept overlap metric\nconcept_overlap_threshold = 2\n# Visualization decimal precision\ndecimal_precision = 4\n# Index to choose model from the array of models\nmodel_index = 0\n# Dictionaries used to load/save total relevance files\nrelevance_fileinfo_cap = {\"path\": relevance_dir, \"test_split\": test_size, \"val_split\": val_size, \"metric\": \"cap\"}\nrelevance_fileinfo_con = {\"path\": relevance_dir, \"test_split\": test_size, \"val_split\": val_size, \"metric\": \"con\", \"other\": [(\"conthresh\", concept_overlap_threshold)]}\n# Function to preprocess data when we want to evaluate captions\nreference_preprocess_cap = lambda x: x[\"caption\"].numpy().decode('UTF-8')          \n# Function to preprocess data when we want to evaluate concepts\nreference_preprocess_con = lambda x: x[\"concepts\"].numpy()\nreference_preprocess_con_hash = lambda x: frozenset(sorted(np.where(x[\"concepts\"].numpy())[0]))\n# Function to compute if a match is relevant given concept arrays \nconcept_relevance = lambda m, o: np.count_nonzero(np.logical_and(m, o)) >= min(concept_overlap_threshold, np.count_nonzero(m), np.count_nonzero(o))\nconcept_relevance_hash = lambda m, o: len(m.intersection(o)) >= min(concept_overlap_threshold, len(m), len(o))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Metric IDs\nclass EvalMetrics:\n    METRIC_ACCURACY = \"Accuracy\"\n    METRIC_MAP = \"MAP\"\n    METRIC_MAR = \"MAR\"\n    METRIC_F1 = \"F1\"\n# Import alias for consistency\nevm = EvalMetrics\n\n# Metric visualization parameters\nmetrics = [\n    {\"id\": evm.METRIC_ACCURACY, \"name\": \"Accuracy\", \"color\": \"green\"},\n    {\"id\": evm.METRIC_MAP, \"name\": \"Mean Average Precision\", \"color\": \"blue\"},\n    {\"id\": evm.METRIC_MAR, \"name\": \"Mean Average Recall\", \"color\": \"red\"},\n    {\"id\": evm.METRIC_F1, \"name\": \"F1 Score\", \"color\": \"blueviolet\"}\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation Functions","metadata":{}},{"cell_type":"code","source":"# Construct reference dataset for retrieving side data of elements\n# Unusable due to TensorFlow funny stuff\ndef generate_dataset_reference(\n    dataset_eval,                 # Dataset to generate embeddings\n    dataset_ref_map=lambda *x: x, # Lambda mapping function for reference\n):\n    return [e for e in dataset_eval.map(dataset_ref_map).unbatch()]\n\n# Generate the embeddings and the corresponding dataset reference for an image dataset\ndef generate_image_embeddings(\n    image_encoder,                 # Image encoder of clip model\n    dataset_eval,                  # Dataset to generate embeddings (WARNING: the dataset must not be shuffling or have a shuffle buffer size of 1)\n    dataset_pred_map=lambda *x: x, # Lambda mapping function for prediction\n    dataset_ref_map=lambda *x: x,  # Lambda mapping function for reference\n):\n    print(\"Generating image embeddings\")\n    # Generate image embedding\n    image_embeddings = image_encoder.predict(\n        dataset_eval.map(dataset_pred_map),\n        verbose=1,\n    )\n    # Construct reference dataset for retrieving side data of elements\n    dataset_reference = [e for e in dataset_eval.map(dataset_ref_map).unbatch()]\n    return dataset_reference, image_embeddings\n\n# Generate the embeddings and the corresponding dataset reference for a text dataset\ndef generate_text_embeddings(\n    text_encoder,                  # Image encoder of clip model\n    dataset_eval,                  # Dataset to generate embeddings (WARNING: the dataset must not be shuffling or have a shuffle buffer size of 1)\n    dataset_pred_map=lambda *x: x, # Lambda mapping function for prediction\n    dataset_ref_map=lambda *x: x,  # Lambda mapping function for reference\n):\n    print(\"Generating image embeddings\")\n    # Generate text embedding\n    text_embeddings = text_encoder.predict(\n        dataset_eval.map(dataset_pred_map),\n        verbose=1,\n    )\n    # Construct reference dataset for retrieving side data of elements\n    dataset_reference = [e for e in dataset_eval.map(dataset_ref_map).unbatch()]\n    return dataset_reference, text_embeddings\n\n# Return the results in the form of reference dataset indexes of a text to image retrieval for a series of queries\ndef find_t2i_matches(\n    queries,                # Queries to search\n    text_encoder,           # Text encoder of clip model\n    image_embeddings,       # Generated image embeddings\n    k=10,                   # Number of elements for top-k\n    normalize=True,         # Embedding normalization\n):\n    print(\"Computing Text-to-Image matches\")\n    query_embedding = text_encoder.predict(queries)\n    # Normalize the query and the image embeddings\n    if normalize:\n        image_embeddings = tf.math.l2_normalize(image_embeddings, axis=1)\n        query_embedding = tf.math.l2_normalize(query_embedding, axis=1)\n    # Compute the dot product between the query and the image embeddings\n    dot_similarity = tf.matmul(query_embedding, image_embeddings, transpose_b=True)\n    # Retrieve top k indices\n    results = tf.math.top_k(dot_similarity, k).indices.numpy()\n    return results\n\n# Return the results in the form of reference dataset indexes of a image to text retrieval for a series of queries\ndef find_i2t_matches(\n    queries,                # Queries to search\n    image_encoder,          # Text encoder of clip model\n    text_embeddings,        # Generated image embeddings\n    k=10,                   # Number of elements for top-k\n    normalize=True,         # Embedding normalization\n):\n    print(\"Computing Image-to-Text matches\")\n    query_embedding = image_encoder.predict(queries)\n    # Normalize the query and the text embeddings\n    if normalize:\n        text_embeddings = tf.math.l2_normalize(text_embeddings, axis=1)\n        query_embedding = tf.math.l2_normalize(query_embedding, axis=1)\n    # Compute the dot product between the query and the text embeddings\n    dot_similarity = tf.matmul(query_embedding, text_embeddings, transpose_b=True)\n    # Retrieve top k indices\n    results = tf.math.top_k(dot_similarity, k).indices.numpy()\n    return results\n\n# Extract the reference dataset objects given a list of indexes\ndef index_to_reference(results, dataset_reference):\n    return [[dataset_reference[match] | {\"index\": match} for match in result] for result in results]\n\n# Transform a one-hot encoded list of boolean concepts to the respective list of raw concepts labels\n# If the flag string_form is set to true, the returned list will contain strings of concatenated concept text\ndef decode_concepts(concepts, encoder, concept_list, string_form=True):\n    c = np.array(concepts)\n    c = encoder.inverse_transform(c)\n    if string_form:\n        c = [\" \".join([concept_list[concept] for concept in e]) for e in c]\n    return c","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Retrieve relevant items given a list of queries (DO NOT RUN THIS ON A COMPLETE DATASET!!!)\ndef retrieve_relevant(queries, dataset_reference, reference_preprocess=lambda x: x, relevance=lambda m, o: m == o):\n    return [\n        [element for element in map(reference_preprocess, dataset_reference) if relevance(query, element)]\n        for query in queries\n    ]\n\n# Compute the number of relevant items in the first k matches in a list of results\n# If queries is None, it is assumed that the queries ran to obtain the list of results are parallel to the elements in dataset_reference\ndef compute_relevant_at_k(results, dataset_reference, queries=None, k=None, reference_preprocess=lambda x: x, relevance=lambda m, o: m == o):\n    if not k:\n        k = len(results[0])\n    if queries:\n        relevant_reference = retrieve_relevant(queries, dataset_reference, reference_preprocess=reference_preprocess, relevance=relevance)\n    else:\n        relevant_reference = map(reference_preprocess, dataset_reference)\n    return [ \n        np.count_nonzero([relevance(match, reference) for match in list(map(reference_preprocess, matches))[0:k]])\n        for matches, reference in zip(results, relevant_reference) \n    ]\n\n# Computes the total number of relevant elements for a dataset or queries\n# It is assumed that the element returned by reference_preprocess is hashable and can be used as a dictionary key\n# If queries is None, it is assumed that the queries ran to obtain the list of results are parallel to the elements in dataset_reference\ndef compute_total_relevance(\n    dataset_reference, queries=None,                                   # Dataset reference or queries to compute total relevance for\n    reference_preprocess=lambda x: x, relevance=lambda m, o: m == o,   # Preprocessing function and relevance function\n    load_from_file=True, save_to_file=True,                            # Load/Save flags\n    fileinfo={}                                                        # Info for loading/saving data from/to a file in the form of a dictionary with the following keys:\n                                                                       # path, filename, test_split, val_split, split, metric, other\n                                                                       # if a filename is specified, only the base path is needed\n):\n    tot_relevant = True\n    # Check if queries are passed, if so run general function without loading/saving to file\n    if queries:\n        relevant_reference = retrieve_relevant(queries, dataset_reference, reference_preprocess=reference_preprocess, relevance=relevance)\n        return [len(e) for e in relevant_reference]\n    # Check for existing file and load relevance data\n    if load_from_file:\n        tot_relevant = load_relevance_from_csv(fileinfo)\n        if not tot_relevant:\n            print(\"Proceeding with total relevance calculation...\")\n    if not tot_relevant:\n        # Build preprocessed dataset\n        relevant_reference = list(map(reference_preprocess, dataset_reference))\n        total_n = {}\n        # Iterate through dataset and count equal items\n        for element in relevant_reference:\n            if element in total_n:\n                total_n[element] += 1\n            else:\n                total_n[element] = 1\n        # Check bytecode of relevance function to determine if the relevance function is equality,\n        # if so, return counts, otherwise apply relevance to the whole dataset\n        if not relevance.__code__.co_code == (lambda m, o: m == o).__code__.co_code:\n            total_n = {element: sum([total_n[x] for x in total_n if relevance(x, element) and element != x]) + 1 for element in tqdm(total_n)} \n        tot_relevant = [total_n[element] for element in relevant_reference]\n    # Check for existing file and save relevance data\n    if save_to_file:\n        save_relevance_to_csv(tot_relevant, fileinfo)\n    return tot_relevant\n\n# Build the filename for a relevance file given some dataset and preprocessing attributes\ndef build_relevance_filename(\n    path,                              # Base path for the file\n    filename=None,                     # Name of the csv file to load, if None it will be inferred from dataset attributes\n    test_split=0.2,                    # Test split percentage\n    val_split=0,                       # Validation split percentage\n    split=\"train\",                     # Either \"train\", \"test\" or \"val\"\n    metric=\"\",                         # Metric used to compute relevance\n    other=[],                          # Other attributes as an ordered list of (name, value) tuples\n):\n    if not filename:\n        filename = \"TotRelevant_\" + str(test_split) + \"_\" + str(val_split) + \"_\" + split + \"_\" + metric\n        for attr in other:\n            filename += \"_\" + attr[0] + \"-\" + str(attr[1])\n        filename += \".csv\"\n    filename = path + filename\n    return filename\n\n# Load a csv relevance file given a filename or some dataset and preprocessing attributes\ndef load_relevance_from_csv(fileinfo={}):\n    filename = build_relevance_filename(**fileinfo)\n    if not os.path.exists(filename):\n        print(f\"The relevance file \\\"{filename}\\\" does not exist!\")\n        return False\n    else:\n        try:\n            return np.squeeze(pd.read_csv(filename, header=None).values.tolist())\n        except OSError as error:\n            print(f\"Couldn't load file \\\"{filename}\\\": {error}\")\n    return False\n\n# Save total relevant data to a csv relevance file given a filename or some dataset and preprocessing attributes\ndef save_relevance_to_csv(tot_relevant, fileinfo={}):\n    filename = build_relevance_filename(**fileinfo)\n    if os.path.exists(filename):\n        print(f\"Overwriting \\\"{filename}\\\" relevance file!\")\n    df = pd.DataFrame(tot_relevant)\n    try:\n        df.to_csv(filename, index=False, header=False)\n        return True\n    except OSError as error:\n            print(f\"Couldn't save file \\\"{filename}\\\": {error}\")\n    return False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_top_k_accuracy(results, dataset_reference, relevant_at_k):\n    hits = np.count_nonzero(relevant_at_k)\n    return hits / len(dataset_reference)\n\ndef compute_map_k(results, dataset_reference, relevant_at_k, k=None):\n    if not k:\n        k = len(results[0])\n    precision_at_k = [r/k for r in relevant_at_k]\n    return np.sum(precision_at_k) / len(dataset_reference)\n\ndef compute_mar_k(results, dataset_reference, relevant_at_k, total_relevant):\n    recall_at_k = [rk/tr for rk, tr in zip(relevant_at_k, total_relevant)]\n    return np.sum(recall_at_k) / len(dataset_reference)\n\ndef compute_F1_k(precision=0, recall=0):\n    if precision + recall == 0:\n        f1_score = 0\n    else:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n        return f1_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize results for text to image queries\ndef visualize_t2i_results(query, matches):\n    print(\"Top matches for query: \\\"\" + query + \"\\\"\")\n    if \"image path\" in matches[0]:\n        plt.figure(figsize=(18, 18))\n    for i in range(len(matches)):\n        if \"image path\" in matches[i]:\n            path = matches[i][\"image path\"].numpy().decode('UTF-8')\n            ax = plt.subplot(3, 3, i + 1)\n            plt.imshow(mpimg.imread(path))\n            plt.axis(\"off\")\n        if \"caption\" in matches[i]:\n            caption = matches[i][\"caption\"].numpy().decode('UTF-8')\n            print(f\"{i}) {caption}\")\n        \n# Standard isualization for a multi-purpose plotly graph\ndef visualize_multigraph(functions, titlexyf=(None, None, None), legend=True):\n    fig = go.Figure()\n    for function in functions:\n        x = function['x']\n        y = function['y']\n        label = function['label'] if 'label' in function else \"\"\n        color = function['color'] if 'color' in function else None\n        linestyle = function['style'] if 'style' in function else \"solid\"\n        marker = go.scatter.Marker(symbol=function['marker'], size=10) if 'marker' in function else None\n        opacity = function['opacity'] if 'opacity' in function else 1\n        k = len(x)\n        fig.add_trace(go.Scatter(\n            x=x, y=y,\n            line=go.scatter.Line(color=color, dash=linestyle),\n            opacity=opacity,\n            marker=marker,\n            mode=\"lines+markers+text\" if marker else \"lines+text\",\n            name=label,\n        ))\n    fig.update_xaxes(\n        title=titlexyf[0],\n        ticks=\"outside\", ticklen=8, minor=dict(dtick=0.5, ticklen=6, tickcolor=\"black\", showgrid=True), ticklabelstep=1, dtick=1, \n        range=(1,k), \n    )\n    fig.update_yaxes(\n        title=titlexyf[1],\n        ticks=\"outside\", ticklen=8, minor=dict(dtick=0.01, ticklen=6, tickcolor=\"black\", showgrid=True), ticklabelstep=1, dtick=0.1,\n    )\n    fig.update_layout(\n        title=titlexyf[2],\n        width=900, height=600,\n        margin=dict(l=50, r=50, b=20, t=40, pad=4),\n        paper_bgcolor=\"LightSteelBlue\",\n    )\n    fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute baselines for retrieval\n# Assumption of sampling with repetitions, results get more inaccurate as k/l -> inf\ndef retrieval_baselines(dataset_reference, total_relevant, k, metrics=[]):\n    l = len(dataset_reference)\n    metrics_out = {}\n    for metric in metrics:\n        if metric[\"id\"] == evm.METRIC_ACCURACY:\n            metrics_out[evm.METRIC_ACCURACY] = sum([ 1 - pow((l - n_el) / l, k) for n_el in total_relevant]) / l\n        elif metric[\"id\"] == evm.METRIC_MAP:\n            metrics_out[evm.METRIC_MAP] = sum([ n_el / l for n_el in total_relevant]) / l\n        elif metric[\"id\"] == evm.METRIC_MAR:\n            metrics_out[evm.METRIC_MAR] = sum([ k / l for n_el in total_relevant]) / l\n        elif metric[\"id\"] == evm.METRIC_F1:\n            metrics_out[evm.METRIC_F1] = compute_F1_k(metrics_out[evm.METRIC_MAP], metrics_out[evm.METRIC_MAR])\n    return metrics_out\n    \n# Computation of a retrieval report containing metrics\ndef retrieval_report(\n    results, reference, relevant,   # Task results, dataset reference and relevant hits at k for task\n    tot_relevant=None,              # Rotal number of relevant elements for each dataset element\n    k=None,                         # k for metrics computation (should be less or equal than k of retrieval)\n    baselines=True,                 # Calculate baselines alongside metrics\n    metrics=[],                     # Metrics to take into consideration\n    output=True,                    # Print outputs to stdout\n    title=\"Retrieval Report\",       # Title of the report\n    decimal_precision=4,            # Decimal precision of values\n):\n    if not k:\n        k = len(results[0])\n    metrics_out = {}\n    \n    for metric in metrics:\n        if metric[\"id\"] == evm.METRIC_ACCURACY:\n            metrics_out[evm.METRIC_ACCURACY] = compute_top_k_accuracy(results, reference, relevant)\n        elif metric[\"id\"] == evm.METRIC_MAP:\n            metrics_out[evm.METRIC_MAP] = compute_map_k(results, reference, relevant, k=k)\n        elif metric[\"id\"] == evm.METRIC_MAR:\n            metrics_out[evm.METRIC_MAR] = compute_mar_k(results, reference, relevant, tot_relevant)\n        elif metric[\"id\"] == evm.METRIC_F1:\n            metrics_out[evm.METRIC_F1] = compute_F1_k(metrics_out[evm.METRIC_MAP], metrics_out[evm.METRIC_MAR])\n            \n    if baselines:\n            baselines = retrieval_baselines(reference, tot_relevant, k, metrics=metrics)\n            \n    if output:\n        print(f\"\\n ### {title} ###\")\n        for metric in metrics:\n            string = f\"{metric['name']:<30}: {round(metrics_out[metric['id']] * 100, decimal_precision):10}%\"\n            if baselines:\n                string += f\"{'   Baseline':<8}: {round(baselines[metric['id']] * 100, decimal_precision):10}%\"\n            print(string)\n    \n    if baselines:\n        return metrics_out, baselines\n    return metrics_out\n        \n# Computation of a retrieval report in graph form containing metrics \ndef retrieval_graph_report(\n    results, reference,                 # Task results and dataset reference\n    tot_relevant=None,                  # Total number of relevant elements for each dataset element\n    k_range=(1, 10),                    # k range for metrics computation (maximum value shoul not be greater than k of retrieval)\n    baselines=True,                     # Calculate baselines alongside metrics\n    metrics=[],                         # Metrics to take into consideration\n    titlexyf=(None, None, None),        # Tuple containing: (title of x axis, title of y axis, figure title)\n    reference_preprocess=lambda x: x,   # Function to preprocess data contained in the reference dataset\n    relevance=lambda m, o: m == o,      # Function to compare elements\n    functions=None,                     # Plot pre-existing function data\n):\n    if not functions:\n        functions = {metric[\"id\"]: {\"x\": [], \"y\": [], \"label\": metric[\"id\"], \"color\": metric[\"color\"], \"marker\": \"0\", \"opacity\": 0.8} for metric in metrics}\n        if baselines:\n            functions |= {metric[\"id\"] + \"_base\": {\"x\": [], \"y\": [], \"label\": metric[\"id\"] + \" Baseline\", \"color\": metric[\"color\"], \"style\": \"dash\", \"opacity\": 0.5} for metric in metrics}\n        for k in range(k_range[0], k_range[1] + 1):\n            relevant = compute_relevant_at_k(results, reference, k=k, reference_preprocess=reference_preprocess, relevance=relevance)\n            report = retrieval_report(results, reference, relevant, tot_relevant, k=k, baselines=baselines, metrics=metrics, output=False)\n            metrics_out = report[0] if baselines else report\n            baselines = report[1] if baselines else None\n            for metric in metrics_out:\n                functions[metric][\"x\"].append(k)\n                functions[metric][\"y\"].append(metrics_out[metric])\n                if baselines:\n                    functions[metric + \"_base\"][\"x\"].append(k)\n                    functions[metric + \"_base\"][\"y\"].append(baselines[metric])\n    visualize_multigraph(functions.values(), titlexyf)\n    return functions\n\n# Computation of a retrieval report in graph form containing metrics, comparing multiple models on the same dataset\ndef retrieval_graph_compare(\n    multi_results, reference,           # Per-model task results and dataset reference\n    model_ids,                          # List of ordered model ids and labels in the form {\"id\": id, \"label\": label} \n    tot_relevant=None,                  # Total number of relevant elements for each dataset element\n    k_range=(1, 10),                    # k range for metrics computation (maximum value shoul not be greater than k of retrieval)\n    metrics=[],                         # Metrics to take into consideration\n    titlexyf=(None, None, None),        # Tuple containing: (title of x axis, title of y axis, figure title)\n    reference_preprocess=lambda x: x,   # Function to preprocess data contained in the reference dataset\n    relevance=lambda m, o: m == o,      # Function to compare elements\n    functions=None,                     # Plot pre-existing function data\n):\n    if not functions:\n        # Add random markers\n        markers = [\"0\", \"1\", \"2\", \"3\", \"17\", \"26\"]\n        if len(markers) < len(model_ids):\n            print(\"Too many models!\")\n            return None\n        markers = random.sample(markers, len(model_ids))\n        model_ids = [model | {\"marker\": marker} for model, marker in zip(model_ids, markers)]\n        # Generate function models\n        functions = {\n            metric[\"id\"] + model[\"id\"]: \n            {\"x\": [], \"y\": [], \"label\": model[\"label\"] + \" \" + metric[\"id\"], \"color\": metric[\"color\"], \"marker\": model[\"marker\"], \"opacity\": 0.8} \n            for model in model_ids for metric in metrics\n        }\n        # Fill functions\n        for model, results in zip(model_ids, multi_results):\n            for k in range(k_range[0], k_range[1] + 1):\n                relevant = compute_relevant_at_k(results, reference, k=k, reference_preprocess=reference_preprocess, relevance=relevance)\n                metrics_out = retrieval_report(results, reference, relevant, tot_relevant, k=k, baselines=False, metrics=metrics, output=False)\n                for metric in metrics_out:\n                    functions[metric + model[\"id\"]][\"x\"].append(k)\n                    functions[metric + model[\"id\"]][\"y\"].append(metrics_out[metric])\n    visualize_multigraph(functions.values(), titlexyf)\n    return functions\n    \n# Manually compute some text to image queries\ndef manual_t2i_queries(queries, text_encoder, image_embeddings, dataset_reference, k=10, normalize=True):\n    results = find_t2i_matches(queries, clip_text_encoder, test_image_embeddings, k=k, normalize=normalize)\n    results = index_to_reference(results, dataset_reference)\n    for query, matches in zip(queries, results):\n        visualize_t2i_results(query, matches)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Metrics","metadata":{}},{"cell_type":"code","source":"# Generating embeddings for image-to-text and text-to-image tasks\ntest_dataset_reference, test_image_embeddings = generate_image_embeddings(\n    clip_image_encoder,\n    test_dataset,\n    dataset_pred_map=lambda x, y: x['image'],\n    dataset_ref_map=lambda x, y: y | {'image path': x['image path']}\n)\n\n_, test_text_embeddings = generate_text_embeddings(\n    clip_text_encoder,\n    test_dataset,\n    dataset_pred_map=lambda x, y: y['caption'],\n    dataset_ref_map=lambda x, y: y | {'image path': x['image path']}\n)\n\n# Compute relevance for all the test queries in the dataset \ntest_tot_relevant_cap = compute_total_relevance(test_dataset_reference, reference_preprocess=reference_preprocess_cap, save_to_file=False, fileinfo=relevance_fileinfo_cap | {\"split\": \"test\"})\ntest_tot_relevant_con = compute_total_relevance(test_dataset_reference, reference_preprocess=reference_preprocess_con_hash, relevance=concept_relevance_hash, save_to_file=False, fileinfo=relevance_fileinfo_con | {\"split\": \"test\"})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text to Image Task","metadata":{}},{"cell_type":"code","source":"print(\"\\n### Scoring test data ###\")\n\ntest_queries = test_dataset.map(lambda x, y: y[\"caption\"])\n\n# Compute matching results and extrapolate relevant matches based on different criterions\ntest_raw_results = find_t2i_matches(test_queries, clip_text_encoder, test_image_embeddings, k=k, normalize=True)\ntest_results = index_to_reference(test_raw_results, test_dataset_reference)\ntest_relevant_cap = compute_relevant_at_k(test_results, test_dataset_reference, k=k, reference_preprocess=reference_preprocess_cap)\ntest_relevant_con = compute_relevant_at_k(test_results, test_dataset_reference, k=k, reference_preprocess=reference_preprocess_con, relevance=concept_relevance)\n\n# Compute alternative caption results based on concept text concatenation\ntest_queries_fromconcepts = tf.data.Dataset.from_tensor_slices(decode_concepts([e[\"concepts\"] for e in test_dataset_reference], concepts_onehot, concept_list, string_form=True)).batch(batch_size)\ntest_results_fromconcepts = index_to_reference(find_t2i_matches(test_queries_fromconcepts, clip_text_encoder, test_image_embeddings, k=k, normalize=True), test_dataset_reference)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Caption equality relevance metric","metadata":{}},{"cell_type":"code","source":"_ = retrieval_report(\n    test_results, test_dataset_reference, test_relevant_cap, test_tot_relevant_cap,\n    k=k,\n    metrics=metrics,\n    title=f\"Test Data - Caption equality metrics @ k={k}\",\n    decimal_precision=decimal_precision\n)\n_ = retrieval_graph_report(\n    test_results, test_dataset_reference, test_tot_relevant_cap,\n    k_range=(1, k),\n    metrics=metrics, \n    titlexyf=(\"k\", None, \"Test Data - Caption equality metrics\"),\n    reference_preprocess=reference_preprocess_cap\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compare_models = [{\"id\": \"ds\", \"label\": \"Dataset Caption\"}, {\"id\": \"cpfs\", \"label\": \"Concept Fusion\"}]\n_ = retrieval_graph_compare(\n    [test_results, test_results_fromconcepts], test_dataset_reference, compare_models, test_tot_relevant_cap,\n    k_range=(1, k),\n    metrics=metrics, \n    titlexyf=(\"k\", None, \"Test Data - Dataset Captions vs Concept-Fusion Captions\"),\n    reference_preprocess=reference_preprocess_cap\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Concept overlap relevance metric","metadata":{}},{"cell_type":"code","source":"_ = retrieval_report(\n    test_results, test_dataset_reference, test_relevant_con, test_tot_relevant_con,\n    k=k,\n    metrics=metrics,\n    title=f\"Test Data - Concept overlap metrics @ k={k}\",\n    decimal_precision=decimal_precision,\n)\n_ = retrieval_graph_report(\n    test_results, test_dataset_reference, test_tot_relevant_con,\n    metrics=metrics,\n    k_range=(1, k),\n    titlexyf=(\"k\", None, \"Test Data - Concept overlap metrics\"),\n    reference_preprocess=reference_preprocess_con, relevance=concept_relevance,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image to Text","metadata":{}},{"cell_type":"code","source":"print(\"\\n### Scoring test data ###\")\n\ntest_queries = test_dataset.map(lambda x, y: x[\"image\"])\n\n# Compute matching results and extrapolate relevant matches based on different criterions\ntest_raw_results = find_i2t_matches(test_queries, clip_image_encoder, test_text_embeddings, k=k, normalize=True)\ntest_results = index_to_reference(test_raw_results, test_dataset_reference)\ntest_relevant_cap = compute_relevant_at_k(test_results, test_dataset_reference, k=k, reference_preprocess=reference_preprocess_cap)\ntest_relevant_con = compute_relevant_at_k(test_results, test_dataset_reference, k=k, reference_preprocess=reference_preprocess_con, relevance=concept_relevance)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Caption equality relevance metric","metadata":{}},{"cell_type":"code","source":"_ = retrieval_report(\n    test_results, test_dataset_reference, test_relevant_cap, test_tot_relevant_cap,\n    k=k,\n    metrics=metrics,\n    title=f\"Test Data - Caption equality metrics @ k={k}\",\n    decimal_precision=decimal_precision\n)\n_ = retrieval_graph_report(\n    test_results, test_dataset_reference, test_tot_relevant_cap,\n    k_range=(1, k),\n    metrics=metrics, \n    titlexyf=(\"k\", None, \"Test Data - Caption equality metrics\"),\n    reference_preprocess=reference_preprocess_cap\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Concept overlap relevance metric","metadata":{}},{"cell_type":"code","source":"_ = retrieval_report(\n    test_results, test_dataset_reference, test_relevant_con, test_tot_relevant_con,\n    k=k,\n    metrics=metrics,\n    title=f\"Test Data - Concept overlap metrics @ k={k}\",\n    decimal_precision=decimal_precision,\n)\n_ = retrieval_graph_report(\n    test_results, test_dataset_reference, test_tot_relevant_con,\n    metrics=metrics,\n    k_range=(1, k),\n    titlexyf=(\"k\", None, \"Test Data - Concept overlap metrics\"),\n    reference_preprocess=reference_preprocess_con, relevance=concept_relevance,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
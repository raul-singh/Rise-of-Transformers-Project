{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Declarations"]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import re\n","import os\n","import math\n","import string\n","import random\n","import requests\n","import importlib\n","import itertools\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","\n","import plotly.graph_objects as go\n","\n","from tqdm import tqdm\n","\n","from IPython.display import display\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","tfk = tf.keras\n","tfkl = tf.keras.layers\n","kb = tf.keras.backend"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(tf.__version__)\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"]},{"cell_type":"markdown","metadata":{},"source":["## Constants"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Randomness\n","seed = 42\n","\n","random.seed(seed)\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","tf.compat.v1.set_random_seed(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Filepaths\n","kaggle = False\n","\n","model_versions = [\"v4.0\"]\n","\n","github_repo = \"raul-singh/Rise-of-Transformers-Project\"\n","github_branch = \"main\"\n","github_python_prefix = [\"Code\", \"Notebooks\", \"py_files\"]\n","github_clip_models_prefix = [\"Code\", \"Models\"] if kaggle else [\"..\", \"Models\"]\n","github_pyfiles_data = [\n","    {\"name\": \"preprocessing\", \"imports\": [\"import_datasets\"]}, \n","    {\"name\": \"clip\", \"imports\": [\"build_clip\"]}\n","]\n","github_pyfiles = [\"/\".join(github_python_prefix) + \"/\" + pf[\"name\"] + \".py\" for pf in github_pyfiles_data]\n","github_clip_models = [f\"{'/'.join(github_clip_models_prefix)}/{version}.yaml\" for version in model_versions]\n","\n","kaggle_dataset1 = \"/kaggle/input/transformers-hackathon/\"\n","kaggle_dataset2 = \"/kaggle/input/transformers-hackathon-features/\"\n","kaggle_weights = \"/kaggle/input/clip-weights/\"\n","kaggle_relevance = \"/kaggle/input/clip-relevance/\"\n","\n","image_dir = \"./resized_train\"\n","relevance_dir = \"./relevance\"\n","caption_pred_file = \"caption_prediction_train.csv\"\n","concept_det_file = \"concept_detection_train.csv\"\n","concept_file = \"concepts.csv\"\n","clip_weights_files = [f\"{version}.h5\" for version in model_versions] if kaggle else [None for _ in model_versions]\n","\n","if kaggle:\n","    image_dir = kaggle_dataset1 + image_dir\n","    relevance_dir = kaggle_relevance + relevance_dir\n","    caption_pred_file = kaggle_dataset2 + caption_pred_file\n","    concept_det_file = kaggle_dataset2 + concept_det_file\n","    concept_file = kaggle_dataset2 + concept_file\n","    clip_weights_files = [kaggle_weights + weight for weight in clip_weights_files]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Train/Val/Test split and filter percentages\n","test_size = 0.2\n","val_size = 0\n","filter_percent_dataset = 1\n","\n","# Batch size\n","batch_size = 32\n","\n","# Import dataset types and shapes\n","in_feat_typ = {'caption': tf.string, 'concepts': tf.bool, 'image path': tf.string}\n","feature_shapes = {'image': (128, 128, 3), 'caption': (), 'concepts': (8374)}\n","\n","# Output dataset structure\n","x_features_eval = ['image path', 'image']\n","y_features_eval = ['caption', 'concepts']\n","\n","# Define parameters for dataset import\n","dataset_parameters = [{\n","    'x_features': x_features_eval, 'y_features': y_features_eval,\n","    'x_dict': True, 'y_dict': True,           \n","    'shuffle_buffer_size': 1,\n","    'batch_size': batch_size,\n","    'cached': True,\n","}]"]},{"cell_type":"markdown","metadata":{},"source":["## Meta-Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def clean_recursive_imports(source, import_list, prefix):\n","    import_prefix = re.sub(r\"/\", \".\", prefix)\n","    for target_import in import_list:\n","        source = re.sub(r\"from[ \\t]+\" + re.escape(target_import) + r\"[ \\t]+import\", f\"from {import_prefix + target_import} import\", source)\n","    return source\n","    \n","def import_py_from_repo(repository, branch, filepath, prefix, recursive_imports_list=None):\n","    # Build path for retrieval and write name\n","    path_pre = \"https://raw.githubusercontent.com/\"\n","    path = path_pre + repository + \"/\" + branch + \"/\" + filepath \n","    write_path = prefix + filepath.split(\"/\")[-1]\n","    print(\"Downloading file from \" + path)\n","    # Obtain raw text from file\n","    text = requests.get(path).text\n","    # Clean recursive imports\n","    text = clean_recursive_imports(text, recursive_imports_list, prefix) if recursive_imports_list else text\n","    # Create subdirectories if not exist\n","    os.makedirs(os.path.dirname(write_path), exist_ok=True)\n","    # Write file\n","    f = open(write_path, \"w\")\n","    f.write(text)\n","    f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if kaggle:\n","    for pf_data, py_file in zip(github_pyfiles_data, github_pyfiles):\n","        import_py_from_repo(\n","            github_repo, github_branch, py_file, \n","            \"/\".join(github_python_prefix) + \"/\", \n","            recursive_imports_list=[pf[\"name\"] for pf in github_pyfiles_data],\n","        )\n","        import_string = f'from {\".\".join(github_python_prefix) + \".\" + pf_data[\"name\"]} import {\", \".join(pf_data[\"imports\"])}'\n","        exec(import_string)\n","    \n","    for model in github_clip_models:\n","        import_py_from_repo(github_repo, github_branch, model, \"/\".join(github_clip_models_prefix) + \"/\")\n","        \n","else:\n","    for pf_data in github_pyfiles_data:\n","        import_string = f'from py_files.{pf_data[\"name\"]} import {\", \".join(pf_data[\"imports\"])}'\n","        exec(import_string)"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["concept_info, datasets, dataset_sizes = import_datasets(\n","    image_dir, caption_pred_file, concept_file, concept_det_file,\n","    in_feat_typ, feature_shapes,\n","    dataset_parameters,\n","    filter_percent_dataset,\n","    test_size, val_size,\n","    seed,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Select loaded datasets and variables\n","concept_list, concepts_onehot = concept_info\n","train_dataset, _, test_dataset = datasets[0]\n","train_ds_size, val_ds_size, test_ds_size = dataset_sizes"]},{"cell_type":"markdown","metadata":{},"source":["# Model Import"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["models = []\n","for structure, weights in zip(github_clip_models, clip_weights_files):\n","    print(f\"Creating model {structure}\")\n","    clip_image_encoder, clip_text_encoder, clip = build_clip(structure, weights_path=weights)\n","    models.append({\n","        \"image_encoder\": clip_image_encoder,\n","        \"clip_text_encoder\": clip_text_encoder,\n","        \"clip\": clip,\n","    })"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Select the loaded model to evaluate\n","clip_image_encoder, clip_text_encoder, clip = models[0].values()"]},{"cell_type":"markdown","metadata":{},"source":["# Model Evaluation"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation Definitions"]},{"cell_type":"markdown","metadata":{},"source":["### Evaluation Variables"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Top-k number\n","k = 10\n","# Threshold for concept overlap metric\n","concept_overlap_threshold = 2\n","# Visualization decimal precision\n","decimal_precision = 4\n","# Index to choose model from the array of models\n","model_index = 0\n","# Dictionaries used to load/save total relevance files\n","relevance_fileinfo_cap = {\"path\": relevance_dir, \"test_split\": test_size, \"val_split\": val_size, \"metric\": \"cap\"}\n","relevance_fileinfo_con = {\"path\": relevance_dir, \"test_split\": test_size, \"val_split\": val_size, \"metric\": \"con\", \"other\": [(\"conthresh\", concept_overlap_threshold)]}\n","# Function to preprocess data when we want to evaluate captions\n","reference_preprocess_cap = lambda x: x[\"caption\"].numpy().decode('UTF-8')          \n","# Function to preprocess data when we want to evaluate concepts\n","reference_preprocess_con = lambda x: x[\"concepts\"].numpy()\n","reference_preprocess_con_hash = lambda x: frozenset(sorted(np.where(x[\"concepts\"].numpy())[0]))\n","# Function to compute if a match is relevant given concept arrays \n","concept_relevance = lambda m, o: np.count_nonzero(np.logical_and(m, o)) >= min(concept_overlap_threshold, np.count_nonzero(m), np.count_nonzero(o))\n","concept_relevance_hash = lambda m, o: len(m.intersection(o)) >= min(concept_overlap_threshold, len(m), len(o))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Metric IDs\n","class EvalMetrics:\n","    METRIC_ACCURACY = \"Accuracy\"\n","    METRIC_MAP = \"MAP\"\n","    METRIC_MAR = \"MAR\"\n","    METRIC_F1 = \"F1\"\n","# Import alias for consistency\n","evm = EvalMetrics\n","\n","# Metric visualization parameters\n","metrics = [\n","    {\"id\": evm.METRIC_ACCURACY, \"name\": \"Accuracy\", \"color\": \"green\"},\n","    {\"id\": evm.METRIC_MAP, \"name\": \"Mean Average Precision\", \"color\": \"blue\"},\n","    {\"id\": evm.METRIC_MAR, \"name\": \"Mean Average Recall\", \"color\": \"red\"},\n","    {\"id\": evm.METRIC_F1, \"name\": \"F1 Score\", \"color\": \"blueviolet\"}\n","]"]},{"cell_type":"markdown","metadata":{},"source":["### Evaluation Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Construct reference dataset for retrieving side data of elements\n","def generate_dataset_reference(\n","    dataset_eval,                 # Dataset to generate embeddings\n","    dataset_ref_map=lambda *x: x, # Lambda mapping function for reference\n","):\n","    return [e for e in dataset_eval.map(dataset_ref_map).unbatch()]\n","\n","# Generate the embeddings and the corresponding dataset reference for an image dataset\n","def generate_image_embeddings(\n","    image_encoder,                 # Image encoder of clip model\n","    dataset_eval,                  # Dataset to generate embeddings (WARNING: the dataset must not be shuffling or have a shuffle buffer size of 1)\n","    dataset_pred_map=lambda *x: x, # Lambda mapping function for prediction\n","):\n","    print(\"Generating image embeddings\")\n","    # Generate image embedding\n","    image_embeddings = image_encoder.predict(\n","        dataset_eval.map(dataset_pred_map),\n","        verbose=1,\n","    )\n","    return image_embeddings\n","\n","# Generate the embeddings and the corresponding dataset reference for a text dataset\n","def generate_text_embeddings(\n","    text_encoder,                  # Image encoder of clip model\n","    dataset_eval,                  # Dataset to generate embeddings (WARNING: the dataset must not be shuffling or have a shuffle buffer size of 1)\n","    dataset_pred_map=lambda *x: x, # Lambda mapping function for prediction\n","):\n","    print(\"Generating image embeddings\")\n","    # Generate text embedding\n","    text_embeddings = text_encoder.predict(\n","        dataset_eval.map(dataset_pred_map),\n","        verbose=1,\n","    )\n","    return text_embeddings\n","\n","# Return the results in the form of reference dataset indexes of a text to image retrieval for a series of queries\n","def find_t2i_matches(\n","    queries,                # Queries to search\n","    text_encoder,           # Text encoder of clip model\n","    image_embeddings,       # Generated image embeddings\n","    k=10,                   # Number of elements for top-k\n","    normalize=True,         # Embedding normalization\n","):\n","    print(\"Computing Text-to-Image matches\")\n","    query_embedding = text_encoder.predict(queries)\n","    # Normalize the query and the image embeddings\n","    if normalize:\n","        image_embeddings = tf.math.l2_normalize(image_embeddings, axis=1)\n","        query_embedding = tf.math.l2_normalize(query_embedding, axis=1)\n","    # Compute the dot product between the query and the image embeddings\n","    dot_similarity = tf.matmul(query_embedding, image_embeddings, transpose_b=True)\n","    # Retrieve top k indices\n","    results = tf.math.top_k(dot_similarity, k).indices.numpy()\n","    return results\n","\n","# Return the results in the form of reference dataset indexes of a image to text retrieval for a series of queries\n","def find_i2t_matches(\n","    queries,                # Queries to search\n","    image_encoder,          # Text encoder of clip model\n","    text_embeddings,        # Generated image embeddings\n","    k=10,                   # Number of elements for top-k\n","    normalize=True,         # Embedding normalization\n","):\n","    print(\"Computing Image-to-Text matches\")\n","    query_embedding = image_encoder.predict(queries)\n","    # Normalize the query and the text embeddings\n","    if normalize:\n","        text_embeddings = tf.math.l2_normalize(text_embeddings, axis=1)\n","        query_embedding = tf.math.l2_normalize(query_embedding, axis=1)\n","    # Compute the dot product between the query and the text embeddings\n","    dot_similarity = tf.matmul(query_embedding, text_embeddings, transpose_b=True)\n","    # Retrieve top k indices\n","    results = tf.math.top_k(dot_similarity, k).indices.numpy()\n","    return results\n","\n","# Extract the reference dataset objects given a list of indexes\n","def index_to_reference(results, dataset_reference):\n","    return [[dataset_reference[match] | {\"index\": match} for match in result] for result in results]\n","\n","# Transform a one-hot encoded list of boolean concepts to the respective list of raw concepts labels\n","# If the flag string_form is set to true, the returned list will contain strings of concatenated concept text\n","def decode_concepts(concepts, encoder, concept_list, string_form=True):\n","    c = np.array(concepts)\n","    c = encoder.inverse_transform(c)\n","    if string_form:\n","        c = [\" \".join([concept_list[concept] for concept in e]) for e in c]\n","    return c"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Retrieve relevant items given a list of queries (DO NOT RUN THIS ON A COMPLETE DATASET!!!)\n","def retrieve_relevant(queries, dataset_reference, reference_preprocess=lambda x: x, relevance=lambda m, o: m == o):\n","    return [\n","        [element for element in map(reference_preprocess, dataset_reference) if relevance(query, element)]\n","        for query in queries\n","    ]\n","\n","# Compute the number of relevant items in the first k matches in a list of results\n","# If queries is None, it is assumed that the queries ran to obtain the list of results are parallel to the elements in dataset_reference\n","def compute_relevant_at_k(results, dataset_reference, queries=None, k=None, reference_preprocess=lambda x: x, relevance=lambda m, o: m == o):\n","    if not k:\n","        k = len(results[0])\n","    if queries:\n","        relevant_reference = retrieve_relevant(queries, dataset_reference, reference_preprocess=reference_preprocess, relevance=relevance)\n","    else:\n","        relevant_reference = map(reference_preprocess, dataset_reference)\n","    return [ \n","        np.count_nonzero([relevance(match, reference) for match in list(map(reference_preprocess, matches))[0:k]])\n","        for matches, reference in zip(results, relevant_reference) \n","    ]\n","\n","# Computes the total number of relevant elements for a dataset or queries\n","# It is assumed that the element returned by reference_preprocess is hashable and can be used as a dictionary key\n","# If queries is None, it is assumed that the queries ran to obtain the list of results are parallel to the elements in dataset_reference\n","def compute_total_relevance(\n","    dataset_reference, queries=None,                                   # Dataset reference or queries to compute total relevance for\n","    reference_preprocess=lambda x: x, relevance=lambda m, o: m == o,   # Preprocessing function and relevance function\n","    load_from_file=True, save_to_file=True,                            # Load/Save flags\n","    fileinfo={}                                                        # Info for loading/saving data from/to a file in the form of a dictionary with the following keys:\n","                                                                       # path, filename, test_split, val_split, split, metric, other\n","                                                                       # if a filename is specified, only the base path is needed\n","):\n","    tot_relevant = True\n","    # Check if queries are passed, if so run general function without loading/saving to file\n","    if queries:\n","        relevant_reference = retrieve_relevant(queries, dataset_reference, reference_preprocess=reference_preprocess, relevance=relevance)\n","        return [len(e) for e in relevant_reference]\n","    # Check for existing file and load relevance data\n","    if load_from_file:\n","        tot_relevant = load_relevance_from_csv(fileinfo)\n","        if not tot_relevant:\n","            print(\"Proceeding with total relevance calculation...\")\n","    if not tot_relevant:\n","        # Build preprocessed dataset\n","        relevant_reference = list(map(reference_preprocess, dataset_reference))\n","        total_n = {}\n","        # Iterate through dataset and count equal items\n","        for element in relevant_reference:\n","            if element in total_n:\n","                total_n[element] += 1\n","            else:\n","                total_n[element] = 1\n","        # Check bytecode of relevance function to determine if the relevance function is equality,\n","        # if so, return counts, otherwise apply relevance to the whole dataset\n","        if not relevance.__code__.co_code == (lambda m, o: m == o).__code__.co_code:\n","            total_n = {element: sum([total_n[x] for x in total_n if relevance(x, element) and element != x]) + 1 for element in tqdm(total_n)} \n","        tot_relevant = [total_n[element] for element in relevant_reference]\n","    # Check for existing file and save relevance data\n","    if save_to_file:\n","        save_relevance_to_csv(tot_relevant, fileinfo)\n","    return tot_relevant\n","\n","# Build the filename for a relevance file given some dataset and preprocessing attributes\n","def build_relevance_filename(\n","    path,                              # Base path for the file\n","    filename=None,                     # Name of the csv file to load, if None it will be inferred from dataset attributes\n","    test_split=0.2,                    # Test split percentage\n","    val_split=0,                       # Validation split percentage\n","    split=\"train\",                     # Either \"train\", \"test\" or \"val\"\n","    metric=\"\",                         # Metric used to compute relevance\n","    other=[],                          # Other attributes as an ordered list of (name, value) tuples\n","):\n","    if not filename:\n","        filename = \"TotRelevant_\" + str(test_split) + \"_\" + str(val_split) + \"_\" + split + \"_\" + metric\n","        for attr in other:\n","            filename += \"_\" + attr[0] + \"-\" + str(attr[1])\n","        filename += \".csv\"\n","    filename = path + filename\n","    return filename\n","\n","# Load a csv relevance file given a filename or some dataset and preprocessing attributes\n","def load_relevance_from_csv(fileinfo={}):\n","    filename = build_relevance_filename(**fileinfo)\n","    if not os.path.exists(filename):\n","        print(f\"The relevance file \\\"{filename}\\\" does not exist!\")\n","        return False\n","    else:\n","        try:\n","            return np.squeeze(pd.read_csv(filename, header=None).values.tolist())\n","        except OSError as error:\n","            print(f\"Couldn't load file \\\"{filename}\\\": {error}\")\n","    return False\n","\n","# Save total relevant data to a csv relevance file given a filename or some dataset and preprocessing attributes\n","def save_relevance_to_csv(tot_relevant, fileinfo={}):\n","    filename = build_relevance_filename(**fileinfo)\n","    if os.path.exists(filename):\n","        print(f\"Overwriting \\\"{filename}\\\" relevance file!\")\n","    df = pd.DataFrame(tot_relevant)\n","    try:\n","        df.to_csv(filename, index=False, header=False)\n","        return True\n","    except OSError as error:\n","            print(f\"Couldn't save file \\\"{filename}\\\": {error}\")\n","    return False"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def compute_top_k_accuracy(results, dataset_reference, relevant_at_k):\n","    hits = np.count_nonzero(relevant_at_k)\n","    return hits / len(dataset_reference)\n","\n","def compute_map_k(results, dataset_reference, relevant_at_k, k=None):\n","    if not k:\n","        k = len(results[0])\n","    precision_at_k = [r/k for r in relevant_at_k]\n","    return np.sum(precision_at_k) / len(dataset_reference)\n","\n","def compute_mar_k(results, dataset_reference, relevant_at_k, total_relevant):\n","    recall_at_k = [rk/tr for rk, tr in zip(relevant_at_k, total_relevant)]\n","    return np.sum(recall_at_k) / len(dataset_reference)\n","\n","def compute_F1_k(precision=0, recall=0):\n","    if precision + recall == 0:\n","        f1_score = 0\n","    else:\n","        f1_score = 2 * (precision * recall) / (precision + recall)\n","        return f1_score"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Visualize results for text to image queries\n","def visualize_t2i_results(query, matches):\n","    print(\"Top matches for query: \\\"\" + query + \"\\\"\")\n","    if \"image path\" in matches[0]:\n","        plt.figure(figsize=(18, 18))\n","    for i in range(len(matches)):\n","        if \"image path\" in matches[i]:\n","            path = matches[i][\"image path\"].numpy().decode('UTF-8')\n","            ax = plt.subplot(3, 3, i + 1)\n","            plt.imshow(mpimg.imread(path))\n","            plt.axis(\"off\")\n","        if \"caption\" in matches[i]:\n","            caption = matches[i][\"caption\"].numpy().decode('UTF-8')\n","            print(f\"{i}) {caption}\")\n","        \n","# Standard isualization for a multi-purpose plotly graph\n","def visualize_multigraph(functions, titlexyf=(None, None, None), legend=True):\n","    fig = go.Figure()\n","    for function in functions:\n","        x = function['x']\n","        y = function['y']\n","        label = function['label'] if 'label' in function else \"\"\n","        color = function['color'] if 'color' in function else None\n","        linestyle = function['style'] if 'style' in function else \"solid\"\n","        marker = go.scatter.Marker(symbol=function['marker'], size=10) if 'marker' in function else None\n","        opacity = function['opacity'] if 'opacity' in function else 1\n","        k = len(x)\n","        fig.add_trace(go.Scatter(\n","            x=x, y=y,\n","            line=go.scatter.Line(color=color, dash=linestyle),\n","            opacity=opacity,\n","            marker=marker,\n","            mode=\"lines+markers+text\" if marker else \"lines+text\",\n","            name=label,\n","        ))\n","    fig.update_xaxes(\n","        title=titlexyf[0],\n","        ticks=\"outside\", ticklen=8, minor=dict(dtick=0.5, ticklen=6, tickcolor=\"black\", showgrid=True), ticklabelstep=1, dtick=1, \n","        range=(1,k), \n","    )\n","    fig.update_yaxes(\n","        title=titlexyf[1],\n","        ticks=\"outside\", ticklen=8, minor=dict(dtick=0.01, ticklen=6, tickcolor=\"black\", showgrid=True), ticklabelstep=1, dtick=0.1,\n","    )\n","    fig.update_layout(\n","        title=titlexyf[2],\n","        width=900, height=600,\n","        margin=dict(l=50, r=50, b=20, t=40, pad=4),\n","        paper_bgcolor=\"LightSteelBlue\",\n","    )\n","    fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Compute baselines for retrieval\n","# Assumption of sampling with repetitions, results get more inaccurate as k/l -> inf\n","def retrieval_baselines(dataset_reference, total_relevant, k, metrics=[]):\n","    l = len(dataset_reference)\n","    metrics_out = {}\n","    for metric in metrics:\n","        if metric[\"id\"] == evm.METRIC_ACCURACY:\n","            metrics_out[evm.METRIC_ACCURACY] = sum([ 1 - pow((l - n_el) / l, k) for n_el in total_relevant]) / l\n","        elif metric[\"id\"] == evm.METRIC_MAP:\n","            metrics_out[evm.METRIC_MAP] = sum([ n_el / l for n_el in total_relevant]) / l\n","        elif metric[\"id\"] == evm.METRIC_MAR:\n","            metrics_out[evm.METRIC_MAR] = sum([ k / l for n_el in total_relevant]) / l\n","        elif metric[\"id\"] == evm.METRIC_F1:\n","            metrics_out[evm.METRIC_F1] = compute_F1_k(metrics_out[evm.METRIC_MAP], metrics_out[evm.METRIC_MAR])\n","    return metrics_out\n","    \n","# Computation of a retrieval report containing metrics\n","def retrieval_report(\n","    results, reference, relevant,   # Task results, dataset reference and relevant hits at k for task\n","    tot_relevant=None,              # Rotal number of relevant elements for each dataset element\n","    k=None,                         # k for metrics computation (should be less or equal than k of retrieval)\n","    baselines=True,                 # Calculate baselines alongside metrics\n","    metrics=[],                     # Metrics to take into consideration\n","    output=True,                    # Print outputs to stdout\n","    title=\"Retrieval Report\",       # Title of the report\n","    decimal_precision=4,            # Decimal precision of values\n","):\n","    if not k:\n","        k = len(results[0])\n","    metrics_out = {}\n","    \n","    for metric in metrics:\n","        if metric[\"id\"] == evm.METRIC_ACCURACY:\n","            metrics_out[evm.METRIC_ACCURACY] = compute_top_k_accuracy(results, reference, relevant)\n","        elif metric[\"id\"] == evm.METRIC_MAP:\n","            metrics_out[evm.METRIC_MAP] = compute_map_k(results, reference, relevant, k=k)\n","        elif metric[\"id\"] == evm.METRIC_MAR:\n","            metrics_out[evm.METRIC_MAR] = compute_mar_k(results, reference, relevant, tot_relevant)\n","        elif metric[\"id\"] == evm.METRIC_F1:\n","            metrics_out[evm.METRIC_F1] = compute_F1_k(metrics_out[evm.METRIC_MAP], metrics_out[evm.METRIC_MAR])\n","            \n","    if baselines:\n","            baselines = retrieval_baselines(reference, tot_relevant, k, metrics=metrics)\n","            \n","    if output:\n","        print(f\"\\n ### {title} ###\")\n","        for metric in metrics:\n","            string = f\"{metric['name']:<30}: {round(metrics_out[metric['id']] * 100, decimal_precision):10}%\"\n","            if baselines:\n","                string += f\"{'   Baseline':<8}: {round(baselines[metric['id']] * 100, decimal_precision):10}%\"\n","            print(string)\n","    \n","    if baselines:\n","        return metrics_out, baselines\n","    return metrics_out\n","        \n","# Computation of a retrieval report in graph form containing metrics \n","def retrieval_graph_report(\n","    results, reference,                 # Task results and dataset reference\n","    tot_relevant=None,                  # Total number of relevant elements for each dataset element\n","    k_range=(1, 10),                    # k range for metrics computation (maximum value shoul not be greater than k of retrieval)\n","    baselines=True,                     # Calculate baselines alongside metrics\n","    metrics=[],                         # Metrics to take into consideration\n","    titlexyf=(None, None, None),        # Tuple containing: (title of x axis, title of y axis, figure title)\n","    reference_preprocess=lambda x: x,   # Function to preprocess data contained in the reference dataset\n","    relevance=lambda m, o: m == o,      # Function to compare elements\n","    functions=None,                     # Plot pre-existing function data\n","):\n","    if not functions:\n","        functions = {metric[\"id\"]: {\"x\": [], \"y\": [], \"label\": metric[\"id\"], \"color\": metric[\"color\"], \"marker\": \"0\", \"opacity\": 0.8} for metric in metrics}\n","        if baselines:\n","            functions |= {metric[\"id\"] + \"_base\": {\"x\": [], \"y\": [], \"label\": metric[\"id\"] + \" Baseline\", \"color\": metric[\"color\"], \"style\": \"dash\", \"opacity\": 0.5} for metric in metrics}\n","        for k in range(k_range[0], k_range[1] + 1):\n","            relevant = compute_relevant_at_k(results, reference, k=k, reference_preprocess=reference_preprocess, relevance=relevance)\n","            report = retrieval_report(results, reference, relevant, tot_relevant, k=k, baselines=baselines, metrics=metrics, output=False)\n","            metrics_out = report[0] if baselines else report\n","            baselines = report[1] if baselines else None\n","            for metric in metrics_out:\n","                functions[metric][\"x\"].append(k)\n","                functions[metric][\"y\"].append(metrics_out[metric])\n","                if baselines:\n","                    functions[metric + \"_base\"][\"x\"].append(k)\n","                    functions[metric + \"_base\"][\"y\"].append(baselines[metric])\n","    visualize_multigraph(functions.values(), titlexyf)\n","    return functions\n","\n","# Computation of a retrieval report in graph form containing metrics, comparing multiple models on the same dataset\n","def retrieval_graph_compare(\n","    multi_results, reference,           # Per-model task results and dataset reference\n","    model_ids,                          # List of ordered model ids and labels in the form {\"id\": id, \"label\": label} \n","    tot_relevant=None,                  # Total number of relevant elements for each dataset element\n","    k_range=(1, 10),                    # k range for metrics computation (maximum value shoul not be greater than k of retrieval)\n","    metrics=[],                         # Metrics to take into consideration\n","    titlexyf=(None, None, None),        # Tuple containing: (title of x axis, title of y axis, figure title)\n","    reference_preprocess=lambda x: x,   # Function to preprocess data contained in the reference dataset\n","    relevance=lambda m, o: m == o,      # Function to compare elements\n","    functions=None,                     # Plot pre-existing function data\n","):\n","    if not functions:\n","        # Add random markers\n","        markers = [\"0\", \"1\", \"2\", \"3\", \"17\", \"26\"]\n","        if len(markers) < len(model_ids):\n","            print(\"Too many models!\")\n","            return None\n","        markers = random.sample(markers, len(model_ids))\n","        model_ids = [model | {\"marker\": marker} for model, marker in zip(model_ids, markers)]\n","        # Generate function models\n","        functions = {\n","            metric[\"id\"] + model[\"id\"]: \n","            {\"x\": [], \"y\": [], \"label\": model[\"label\"] + \" \" + metric[\"id\"], \"color\": metric[\"color\"], \"marker\": model[\"marker\"], \"opacity\": 0.8} \n","            for model in model_ids for metric in metrics\n","        }\n","        # Fill functions\n","        for model, results in zip(model_ids, multi_results):\n","            for k in range(k_range[0], k_range[1] + 1):\n","                relevant = compute_relevant_at_k(results, reference, k=k, reference_preprocess=reference_preprocess, relevance=relevance)\n","                metrics_out = retrieval_report(results, reference, relevant, tot_relevant, k=k, baselines=False, metrics=metrics, output=False)\n","                for metric in metrics_out:\n","                    functions[metric + model[\"id\"]][\"x\"].append(k)\n","                    functions[metric + model[\"id\"]][\"y\"].append(metrics_out[metric])\n","    visualize_multigraph(functions.values(), titlexyf)\n","    return functions\n","    \n","# Manually compute some text to image queries\n","def manual_t2i_queries(queries, text_encoder, image_embeddings, dataset_reference, k=10, normalize=True):\n","    results = find_t2i_matches(queries, clip_text_encoder, test_image_embeddings, k=k, normalize=normalize)\n","    results = index_to_reference(results, dataset_reference)\n","    for query, matches in zip(queries, results):\n","        visualize_t2i_results(query, matches)"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Dataset reference initialization\n","test_dataset_reference = generate_dataset_reference(test_dataset, lambda x, y: y | {'image path': x['image path']})\n","train_dataset_reference = generate_dataset_reference(train_dataset, lambda x, y: y | {'image path': x['image path']})\n","\n","# Compute relevance for all the test queries in the dataset \n","test_tot_relevant_cap = compute_total_relevance(test_dataset_reference, reference_preprocess=reference_preprocess_cap, save_to_file=False, fileinfo=relevance_fileinfo_cap | {\"split\": \"test\"})\n","test_tot_relevant_con = compute_total_relevance(test_dataset_reference, reference_preprocess=reference_preprocess_con_hash, relevance=concept_relevance_hash, save_to_file=False, fileinfo=relevance_fileinfo_con | {\"split\": \"test\"})\n","\n","# Compute relevance for all the training queries in the dataset\n","train_tot_relevant_cap = compute_total_relevance(train_dataset_reference, reference_preprocess=reference_preprocess_cap, save_to_file=False, fileinfo=relevance_fileinfo_cap | {\"split\": \"train\"})\n","train_tot_relevant_con = compute_total_relevance(train_dataset_reference, reference_preprocess=reference_preprocess_con_hash, relevance=concept_relevance_hash, save_to_file=False, fileinfo=relevance_fileinfo_con | {\"split\": \"train\"})"]},{"cell_type":"markdown","metadata":{},"source":["## Text to Image Task"]},{"cell_type":"markdown","metadata":{},"source":["### Test Set Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"\\n### Scoring test data ###\")\n","\n","test_image_embeddings = generate_image_embeddings(\n","    clip_image_encoder,\n","    test_dataset,\n","    dataset_pred_map=lambda x, y: x['image'],\n",")\n","test_queries = test_dataset.map(lambda x, y: y[\"caption\"])\n","\n","# Compute matching results and extrapolate relevant matches based on different criterions\n","test_raw_results = find_t2i_matches(test_queries, clip_text_encoder, test_image_embeddings, k=k, normalize=True)\n","test_results = index_to_reference(test_raw_results, test_dataset_reference)\n","test_relevant_cap = compute_relevant_at_k(test_results, test_dataset_reference, k=k, reference_preprocess=reference_preprocess_cap)\n","test_relevant_con = compute_relevant_at_k(test_results, test_dataset_reference, k=k, reference_preprocess=reference_preprocess_con, relevance=concept_relevance)\n","\n","# Compute alternative caption results based on concept text concatenation\n","test_queries_fromconcepts = tf.data.Dataset.from_tensor_slices(decode_concepts([e[\"concepts\"] for e in test_dataset_reference], concepts_onehot, concept_list, string_form=True)).batch(batch_size)\n","test_results_fromconcepts = index_to_reference(find_t2i_matches(test_queries_fromconcepts, clip_text_encoder, test_image_embeddings, k=k, normalize=True), test_dataset_reference)"]},{"cell_type":"markdown","metadata":{},"source":["#### Caption equality relevance metric"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["_ = retrieval_report(\n","    test_results, test_dataset_reference, test_relevant_cap, test_tot_relevant_cap,\n","    k=k,\n","    metrics=metrics,\n","    title=f\"Test Data - Caption equality metrics @ k={k}\",\n","    decimal_precision=decimal_precision\n",")\n","_ = retrieval_graph_report(\n","    test_results, test_dataset_reference, test_tot_relevant_cap,\n","    k_range=(1, k),\n","    metrics=metrics, \n","    titlexyf=(\"k\", None, \"Test Data - Caption equality metrics\"),\n","    reference_preprocess=reference_preprocess_cap\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["compare_models = [{\"id\": \"ds\", \"label\": \"Dataset Caption\"}, {\"id\": \"cpfs\", \"label\": \"Concept Fusion\"}]\n","_ = retrieval_graph_compare(\n","    [test_results, test_results_fromconcepts], test_dataset_reference, compare_models, test_tot_relevant_cap,\n","    k_range=(1, k),\n","    metrics=metrics, \n","    titlexyf=(\"k\", None, \"Test Data - Dataset Captions vs Concept-Fusion Captions\"),\n","    reference_preprocess=reference_preprocess_cap\n",")"]},{"cell_type":"markdown","metadata":{},"source":["#### Concept overlap relevance metric"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["_ = retrieval_report(\n","    test_results, test_dataset_reference, test_relevant_con, test_tot_relevant_con,\n","    k=k,\n","    metrics=metrics,\n","    title=f\"Test Data - Concept overlap metrics @ k={k}\",\n","    decimal_precision=decimal_precision,\n",")\n","_ = retrieval_graph_report(\n","    test_results, test_dataset_reference, test_tot_relevant_con,\n","    metrics=metrics,\n","    k_range=(1, k),\n","    titlexyf=(\"k\", None, \"Test Data - Concept overlap metrics\"),\n","    reference_preprocess=reference_preprocess_con, relevance=concept_relevance,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Training Set Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"### Scoring training data ###\")\n","\n","train_image_embeddings = generate_image_embeddings(\n","    clip_image_encoder,\n","    train_dataset,\n","    dataset_pred_map=lambda x, y: x['image'],\n",")\n","train_queries = train_dataset.map(lambda x, y: y[\"caption\"])\n","\n","# Compute matching results and extrapolate relevant matches based on different criterions\n","train_raw_results = find_t2i_matches(train_queries, clip_text_encoder, train_image_embeddings, k=k, normalize=True)\n","train_results = index_to_reference(train_raw_results, train_dataset_reference)\n","train_relevant_cap = compute_relevant_at_k(train_results, train_dataset_reference, k=k, reference_preprocess=reference_preprocess_cap)\n","train_relevant_con = compute_relevant_at_k(train_results, train_dataset_reference, k=k, reference_preprocess=reference_preprocess_con, relevance=concept_relevance)\n","                                                 \n","# Compute alternative caption results based on concept text concatenation\n","train_queries_fromconcepts = tf.data.Dataset.from_tensor_slices(decode_concepts([e[\"concepts\"] for e in train_dataset_reference], concepts_onehot, concept_list, string_form=True)).batch(batch_size)\n","train_results_fromconcepts = index_to_reference(find_t2i_matches(train_queries_fromconcepts, clip_text_encoder, train_image_embeddings, k=k, normalize=True), train_dataset_reference)"]},{"cell_type":"markdown","metadata":{},"source":["#### Caption equality relevance metric"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["_ = retrieval_report(\n","    train_results, train_dataset_reference, train_relevant_cap, train_tot_relevant_cap,\n","    k=k,\n","    metrics=metrics,\n","    title=\"Training Data - Caption equality metric\",\n","    decimal_precision=decimal_precision\n",")\n","_ = retrieval_graph_report(\n","    train_results, train_dataset_reference, train_tot_relevant_cap,\n","    k_range=(1, k),\n","    metrics=metrics, \n","    titlexyf=(\"k\", None, \"Training Data - Caption equality metrics\"),\n","    reference_preprocess=reference_preprocess_cap\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["compare_models = [{\"id\": \"ds\", \"label\": \"Dataset Caption\"}, {\"id\": \"cpfs\", \"label\": \"Concept Fusion\"}]\n","_ = retrieval_graph_compare(\n","    [train_results, train_results_fromconcepts], train_dataset_reference, compare_models, train_tot_relevant_cap,\n","    k_range=(1, k),\n","    metrics=metrics, \n","    titlexyf=(\"k\", None, \"Test Data - Dataset Captions vs Concept-Fusion Captions\"),\n","    reference_preprocess=reference_preprocess_cap\n",")"]},{"cell_type":"markdown","metadata":{},"source":["#### Concept overlap relevance metric"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["_ = retrieval_report(\n","    train_results, train_dataset_reference, train_relevant_con, train_tot_relevant_con,\n","    k=k,\n","    metrics=metrics,\n","    title=\"Training Data - Concept overlap metric\",\n","    decimal_precision=decimal_precision\n",")\n","_ = retrieval_graph_report(\n","    train_results, train_dataset_reference, train_tot_relevant_con,\n","    k_range=(1, k),\n","    metrics=metrics, \n","    titlexyf=(\"k\", None, \"Training - Caption equality metrics\"),\n","    reference_preprocess=reference_preprocess_con, relevance=concept_relevance,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Image to Text"]},{"cell_type":"markdown","metadata":{},"source":["### Test Set Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"\\n### Scoring test data ###\")\n","\n","test_text_embeddings = generate_text_embeddings(\n","    clip_text_encoder,\n","    test_dataset,\n","    dataset_pred_map=lambda x, y: y['caption'],\n",")\n","test_queries = test_dataset.map(lambda x, y: x[\"image\"])\n","\n","# Compute matching results and extrapolate relevant matches based on different criterions\n","test_raw_results = find_i2t_matches(test_queries, clip_image_encoder, test_text_embeddings, k=k, normalize=True)\n","test_results = index_to_reference(test_raw_results, test_dataset_reference)\n","test_relevant_cap = compute_relevant_at_k(test_results, test_dataset_reference, k=k, reference_preprocess=reference_preprocess_cap)\n","test_relevant_con = compute_relevant_at_k(test_results, test_dataset_reference, k=k, reference_preprocess=reference_preprocess_con, relevance=concept_relevance)"]},{"cell_type":"markdown","metadata":{},"source":["#### Caption equality relevance metric"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["_ = retrieval_report(\n","    test_results, test_dataset_reference, test_relevant_cap, test_tot_relevant_cap,\n","    k=k,\n","    metrics=metrics,\n","    title=f\"Test Data - Caption equality metrics @ k={k}\",\n","    decimal_precision=decimal_precision\n",")\n","_ = retrieval_graph_report(\n","    test_results, test_dataset_reference, test_tot_relevant_cap,\n","    k_range=(1, k),\n","    metrics=metrics, \n","    titlexyf=(\"k\", None, \"Test Data - Caption equality metrics\"),\n","    reference_preprocess=reference_preprocess_cap\n",")"]},{"cell_type":"markdown","metadata":{},"source":["#### Concept overlap relevance metric"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["_ = retrieval_report(\n","    test_results, test_dataset_reference, test_relevant_con, test_tot_relevant_con,\n","    k=k,\n","    metrics=metrics,\n","    title=f\"Test Data - Concept overlap metrics @ k={k}\",\n","    decimal_precision=decimal_precision,\n",")\n","_ = retrieval_graph_report(\n","    test_results, test_dataset_reference, test_tot_relevant_con,\n","    metrics=metrics,\n","    k_range=(1, k),\n","    titlexyf=(\"k\", None, \"Test Data - Concept overlap metrics\"),\n","    reference_preprocess=reference_preprocess_con, relevance=concept_relevance,\n",")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":4}

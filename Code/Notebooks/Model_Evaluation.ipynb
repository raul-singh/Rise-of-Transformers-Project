{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Declarations"]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import re\n","import os\n","import math\n","import string\n","import random\n","import requests\n","import importlib\n","import itertools\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-11-24 11:29:03.304464: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-11-24 11:29:03.304504: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-11-24 11:29:03.304535: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","\n","import plotly.graph_objects as go\n","\n","from tqdm import tqdm\n","\n","from IPython.display import display\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","tfk = tf.keras\n","tfkl = tf.keras.layers\n","kb = tf.keras.backend"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2.14.0\n","Num GPUs Available:  1\n"]}],"source":["print(tf.__version__)\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"]},{"cell_type":"markdown","metadata":{},"source":["## Constants"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["# Randomness\n","seed = 42\n","\n","random.seed(seed)\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","tf.compat.v1.set_random_seed(seed)"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[],"source":["# Filepaths\n","kaggle = False\n","\n","model_versions = [\"v5.0\"]\n","\n","github_repo = \"raul-singh/Rise-of-Transformers-Project\"\n","github_branch = \"main\"\n","github_python_prefix = [\"Code\", \"Notebooks\", \"py_files\"]\n","github_clip_models_prefix = [\"Code\", \"Models\"] if kaggle else [\"..\", \"Models\"]\n","github_pyfiles_data = [\n","    {\"name\": \"preprocessing\", \"imports\": [\"import_datasets\"]}, \n","    {\"name\": \"clip\", \"imports\": [\"build_clip\"]}\n","]\n","github_pyfiles = [\"/\".join(github_python_prefix) + \"/\" + pf[\"name\"] + \".py\" for pf in github_pyfiles_data]\n","github_clip_models = [f\"{'/'.join(github_clip_models_prefix)}/{version}.yaml\" for version in model_versions]\n","\n","kaggle_dataset1 = \"/kaggle/input/transformers-hackathon/\"\n","kaggle_dataset2 = \"/kaggle/input/transformers-hackathon-features/\"\n","kaggle_weights = \"/kaggle/input/clip-weights/\"\n","kaggle_relevance = \"/kaggle/input/clip-relevance/\"\n","\n","image_dir = \"./resized_train\"\n","relevance_dir = \"./relevance\"\n","caption_pred_file = \"caption_prediction_train.csv\"\n","concept_det_file = \"concept_detection_train.csv\"\n","concept_file = \"concepts.csv\"\n","clip_weights_files = [f\"{version}.h5\" for version in model_versions] if kaggle else [None for _ in model_versions]\n","\n","if kaggle:\n","    image_dir = kaggle_dataset1 + image_dir\n","    relevance_dir = kaggle_relevance + relevance_dir\n","    caption_pred_file = kaggle_dataset2 + caption_pred_file\n","    concept_det_file = kaggle_dataset2 + concept_det_file\n","    concept_file = kaggle_dataset2 + concept_file\n","    clip_weights_files = [kaggle_weights + weight for weight in clip_weights_files]"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["# Train/Val/Test split and filter percentages\n","test_size = 0.2\n","val_size = 0\n","filter_percent_dataset = 1\n","\n","# Batch size\n","batch_size = 32\n","\n","# Import dataset types and shapes\n","in_feat_typ = {'caption': tf.string, 'concepts': tf.bool, 'image path': tf.string}\n","feature_shapes = {'image': (128, 128, 3), 'caption': (), 'concepts': (8374)}\n","\n","# Output dataset structure\n","x_features_eval = ['image path', 'image']\n","y_features_eval = ['caption', 'concepts']\n","\n","# Define parameters for dataset import\n","dataset_parameters = [{\n","    'x_features': x_features_eval, 'y_features': y_features_eval,\n","    'x_dict': True, 'y_dict': True,           \n","    'shuffle_buffer_size': 1,\n","    'batch_size': batch_size,\n","    'cached': True,\n","}]"]},{"cell_type":"markdown","metadata":{},"source":["## Meta-Imports"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["def clean_recursive_imports(source, import_list, prefix):\n","    import_prefix = re.sub(r\"/\", \".\", prefix)\n","    for target_import in import_list:\n","        source = re.sub(r\"from[ \\t]+\" + re.escape(target_import) + r\"[ \\t]+import\", f\"from {import_prefix + target_import} import\", source)\n","    return source\n","    \n","def import_py_from_repo(repository, branch, filepath, prefix, recursive_imports_list=None):\n","    # Build path for retrieval and write name\n","    path_pre = \"https://raw.githubusercontent.com/\"\n","    path = path_pre + repository + \"/\" + branch + \"/\" + filepath \n","    write_path = prefix + filepath.split(\"/\")[-1]\n","    print(\"Downloading file from \" + path)\n","    # Obtain raw text from file\n","    text = requests.get(path).text\n","    # Clean recursive imports\n","    text = clean_recursive_imports(text, recursive_imports_list, prefix) if recursive_imports_list else text\n","    # Create subdirectories if not exist\n","    os.makedirs(os.path.dirname(write_path), exist_ok=True)\n","    # Write file\n","    f = open(write_path, \"w\")\n","    f.write(text)\n","    f.close()"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["if kaggle:\n","    for pf_data, py_file in zip(github_pyfiles_data, github_pyfiles):\n","        import_py_from_repo(\n","            github_repo, github_branch, py_file, \n","            \"/\".join(github_python_prefix) + \"/\", \n","            recursive_imports_list=[pf[\"name\"] for pf in github_pyfiles_data],\n","        )\n","        import_string = f'from {\".\".join(github_python_prefix) + \".\" + pf_data[\"name\"]} import {\", \".join(pf_data[\"imports\"])}'\n","        exec(import_string)\n","    \n","    for model in github_clip_models:\n","        import_py_from_repo(github_repo, github_branch, model, \"/\".join(github_clip_models_prefix) + \"/\")\n","        \n","else:\n","    for pf_data in github_pyfiles_data:\n","        import_string = f'from py_files.{pf_data[\"name\"]} import {\", \".join(pf_data[\"imports\"])}'\n","        exec(import_string)"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-11-24 11:29:05.522142: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n"]},{"name":"stdout","output_type":"stream","text":["Extracting features from CSV file(s)\n"]},{"name":"stderr","output_type":"stream","text":["83275it [00:49, 1696.23it/s]\n"]}],"source":["concept_info, datasets, dataset_sizes = import_datasets(\n","    image_dir, caption_pred_file, concept_file, concept_det_file,\n","    in_feat_typ, feature_shapes,\n","    dataset_parameters,\n","    filter_percent_dataset,\n","    test_size, val_size,\n","    seed,\n",")"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["# Select loaded datasets and variables\n","concept_list, concepts_onehot = concept_info\n","_, _, test_dataset = datasets[0]\n","train_ds_size, val_ds_size, test_ds_size = dataset_sizes\n","\n","del datasets"]},{"cell_type":"markdown","metadata":{},"source":["# Model Import"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating model ../Models/v5.0.yaml\n","Downloading models...\n","Models downloaded.\n","Building clip...\n","Loading parameters...\n","Done.\n"]}],"source":["models = []\n","for structure, weights in zip(github_clip_models, clip_weights_files):\n","    print(f\"Creating model {structure}\")\n","    clip_image_encoder, clip_text_encoder, clip = build_clip(structure, weights_path=weights)\n","    models.append({\n","        \"image_encoder\": clip_image_encoder,\n","        \"clip_text_encoder\": clip_text_encoder,\n","        \"clip\": clip,\n","    })"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":["# Select the loaded model to evaluate\n","clip_image_encoder, clip_text_encoder, clip = models[0].values()\n","\n","del models"]},{"cell_type":"markdown","metadata":{},"source":["# Model Evaluation"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation Definitions"]},{"cell_type":"markdown","metadata":{},"source":["### Evaluation Variables"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[],"source":["# Top-k number\n","k = 10\n","# Threshold for concept overlap metric\n","concept_overlap_threshold = 2\n","# Visualization decimal precision\n","decimal_precision = 4\n","# Index to choose model from the array of models\n","model_index = 0\n","# Dictionaries used to load/save total relevance files\n","relevance_fileinfo_cap = {\"path\": relevance_dir, \"test_split\": test_size, \"val_split\": val_size, \"metric\": \"cap\"}\n","relevance_fileinfo_con = {\"path\": relevance_dir, \"test_split\": test_size, \"val_split\": val_size, \"metric\": \"con\", \"other\": [(\"conthresh\", concept_overlap_threshold)]}\n","# Function to preprocess data when we want to evaluate captions\n","reference_preprocess_cap = lambda x: x[\"caption\"].numpy().decode('UTF-8')          \n","# Function to preprocess data when we want to evaluate concepts\n","reference_preprocess_con = lambda x: x[\"concepts\"].numpy()\n","reference_preprocess_con_hash = lambda x: frozenset(sorted(np.where(x[\"concepts\"].numpy())[0]))\n","# Function to compute if a match is relevant given concept arrays \n","concept_relevance = lambda m, o: np.count_nonzero(np.logical_and(m, o)) >= min(concept_overlap_threshold, np.count_nonzero(m), np.count_nonzero(o))\n","concept_relevance_hash = lambda m, o: len(m.intersection(o)) >= min(concept_overlap_threshold, len(m), len(o))"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[],"source":["# Metric IDs\n","class EvalMetrics:\n","    METRIC_ACCURACY = \"Accuracy\"\n","    METRIC_MAP = \"MAP\"\n","    METRIC_MAR = \"MAR\"\n","    METRIC_F1 = \"F1\"\n","# Import alias for consistency\n","evm = EvalMetrics\n","\n","# Metric visualization parameters\n","metrics = [\n","    {\"id\": evm.METRIC_ACCURACY, \"name\": \"Accuracy\", \"color\": \"green\"},\n","    {\"id\": evm.METRIC_MAP, \"name\": \"Mean Average Precision\", \"color\": \"blue\"},\n","    {\"id\": evm.METRIC_MAR, \"name\": \"Mean Average Recall\", \"color\": \"red\"},\n","    {\"id\": evm.METRIC_F1, \"name\": \"F1 Score\", \"color\": \"blueviolet\"}\n","]"]},{"cell_type":"markdown","metadata":{},"source":["### Evaluation Functions"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[],"source":["# Construct reference dataset for retrieving side data of elements\n","# Unusable due to TensorFlow funny stuff\n","def generate_dataset_reference(\n","    dataset_eval,                 # Dataset to generate embeddings\n","    dataset_ref_map=lambda *x: x, # Lambda mapping function for reference\n","):\n","    return [e for e in dataset_eval.map(dataset_ref_map).unbatch()]\n","\n","# Generate the embeddings and the corresponding dataset reference for an image dataset\n","def generate_image_embeddings(\n","    image_encoder,                 # Image encoder of clip model\n","    dataset_eval,                  # Dataset to generate embeddings (WARNING: the dataset must not be shuffling or have a shuffle buffer size of 1)\n","    dataset_pred_map=lambda *x: x, # Lambda mapping function for prediction\n","    dataset_ref_map=lambda *x: x,  # Lambda mapping function for reference\n","):\n","    print(\"Generating image embeddings\")\n","    # Generate image embedding\n","    image_embeddings = image_encoder.predict(\n","        dataset_eval.map(dataset_pred_map),\n","        verbose=1,\n","    )\n","    # Construct reference dataset for retrieving side data of elements\n","    dataset_reference = [e for e in dataset_eval.map(dataset_ref_map).unbatch()]\n","    return dataset_reference, image_embeddings\n","\n","# Generate the embeddings and the corresponding dataset reference for a text dataset\n","def generate_text_embeddings(\n","    text_encoder,                  # Image encoder of clip model\n","    dataset_eval,                  # Dataset to generate embeddings (WARNING: the dataset must not be shuffling or have a shuffle buffer size of 1)\n","    dataset_pred_map=lambda *x: x, # Lambda mapping function for prediction\n","    dataset_ref_map=lambda *x: x,  # Lambda mapping function for reference\n","):\n","    print(\"Generating text embeddings\")\n","    # Generate text embedding\n","    text_embeddings = text_encoder.predict(\n","        dataset_eval.map(dataset_pred_map),\n","        verbose=1,\n","    )\n","    # Construct reference dataset for retrieving side data of elements\n","    dataset_reference = [e for e in dataset_eval.map(dataset_ref_map).unbatch()]\n","    return dataset_reference, text_embeddings\n","\n","# Return the results in the form of reference dataset indexes of a text to image retrieval for a series of queries\n","def find_t2i_matches(\n","    queries,                # Queries to search\n","    text_encoder,           # Text encoder of clip model\n","    image_embeddings,       # Generated image embeddings\n","    k=10,                   # Number of elements for top-k\n","    normalize=True,         # Embedding normalization\n","):\n","    print(\"Computing Text-to-Image matches\")\n","    query_embedding = text_encoder.predict(queries)\n","    # Normalize the query and the image embeddings\n","    if normalize:\n","        image_embeddings = tf.math.l2_normalize(image_embeddings, axis=1)\n","        query_embedding = tf.math.l2_normalize(query_embedding, axis=1)\n","    # Compute the dot product between the query and the image embeddings\n","    dot_similarity = tf.matmul(query_embedding, image_embeddings, transpose_b=True)\n","    # Retrieve top k indices\n","    results = tf.math.top_k(dot_similarity, k).indices.numpy()\n","    return results\n","\n","# Return the results in the form of reference dataset indexes of a image to text retrieval for a series of queries\n","def find_i2t_matches(\n","    queries,                # Queries to search\n","    image_encoder,          # Text encoder of clip model\n","    text_embeddings,        # Generated image embeddings\n","    k=10,                   # Number of elements for top-k\n","    normalize=True,         # Embedding normalization\n","):\n","    print(\"Computing Image-to-Text matches\")\n","    query_embedding = image_encoder.predict(queries)\n","    # Normalize the query and the text embeddings\n","    if normalize:\n","        text_embeddings = tf.math.l2_normalize(text_embeddings, axis=1)\n","        query_embedding = tf.math.l2_normalize(query_embedding, axis=1)\n","    # Compute the dot product between the query and the text embeddings\n","    dot_similarity = tf.matmul(query_embedding, text_embeddings, transpose_b=True)\n","    # Retrieve top k indices\n","    results = tf.math.top_k(dot_similarity, k).indices.numpy()\n","    return results\n","\n","# Extract the reference dataset objects given a list of indexes\n","def index_to_reference(results, dataset_reference):\n","    return [[dataset_reference[match] | {\"index\": match} for match in result] for result in results]\n","\n","# Transform a one-hot encoded list of boolean concepts to the respective list of raw concepts labels\n","# If the flag string_form is set to true, the returned list will contain strings of concatenated concept text\n","def decode_concepts(concepts, encoder, concept_list, string_form=True):\n","    c = np.array(concepts)\n","    c = encoder.inverse_transform(c)\n","    if string_form:\n","        c = [\" \".join([concept_list[concept] for concept in e]) for e in c]\n","    return c"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[],"source":["# Retrieve relevant items given a list of queries (DO NOT RUN THIS ON A COMPLETE DATASET!!!)\n","def retrieve_relevant(queries, dataset_reference, reference_preprocess=lambda x: x, relevance=lambda m, o: m == o):\n","    return [\n","        [element for element in map(reference_preprocess, dataset_reference) if relevance(query, element)]\n","        for query in queries\n","    ]\n","\n","# Compute the number of relevant items in the first k matches in a list of results\n","# If queries is None, it is assumed that the queries ran to obtain the list of results are parallel to the elements in dataset_reference\n","def compute_relevant_at_k(results, dataset_reference, queries=None, k=None, reference_preprocess=lambda x: x, relevance=lambda m, o: m == o):\n","    if not k:\n","        k = len(results[0])\n","    if queries:\n","        relevant_reference = retrieve_relevant(queries, dataset_reference, reference_preprocess=reference_preprocess, relevance=relevance)\n","    else:\n","        relevant_reference = map(reference_preprocess, dataset_reference)\n","    return [ \n","        np.count_nonzero([relevance(match, reference) for match in list(map(reference_preprocess, matches))[0:k]])\n","        for matches, reference in zip(results, relevant_reference) \n","    ]\n","\n","# Computes the total number of relevant elements for a dataset or queries\n","# It is assumed that the element returned by reference_preprocess is hashable and can be used as a dictionary key\n","# If queries is None, it is assumed that the queries ran to obtain the list of results are parallel to the elements in dataset_reference\n","def compute_total_relevance(\n","    dataset_reference, queries=None,                                   # Dataset reference or queries to compute total relevance for\n","    reference_preprocess=lambda x: x, relevance=lambda m, o: m == o,   # Preprocessing function and relevance function\n","    load_from_file=True, save_to_file=True,                            # Load/Save flags\n","    fileinfo={}                                                        # Info for loading/saving data from/to a file in the form of a dictionary with the following keys:\n","                                                                       # path, filename, test_split, val_split, split, metric, other\n","                                                                       # if a filename is specified, only the base path is needed\n","):\n","    tot_relevant = True\n","    # Check if queries are passed, if so run general function without loading/saving to file\n","    if queries:\n","        relevant_reference = retrieve_relevant(queries, dataset_reference, reference_preprocess=reference_preprocess, relevance=relevance)\n","        return [len(e) for e in relevant_reference]\n","    # Check for existing file and load relevance data\n","    if load_from_file:\n","        tot_relevant = load_relevance_from_csv(fileinfo)\n","        if not tot_relevant:\n","            print(\"Proceeding with total relevance calculation...\")\n","    if not tot_relevant:\n","        # Build preprocessed dataset\n","        relevant_reference = list(map(reference_preprocess, dataset_reference))\n","        total_n = {}\n","        # Iterate through dataset and count equal items\n","        for element in relevant_reference:\n","            if element in total_n:\n","                total_n[element] += 1\n","            else:\n","                total_n[element] = 1\n","        # Check bytecode of relevance function to determine if the relevance function is equality,\n","        # if so, return counts, otherwise apply relevance to the whole dataset\n","        if not relevance.__code__.co_code == (lambda m, o: m == o).__code__.co_code:\n","            total_n = {element: sum([total_n[x] for x in total_n if relevance(x, element) and element != x]) + 1 for element in tqdm(total_n)} \n","        tot_relevant = [total_n[element] for element in relevant_reference]\n","    # Check for existing file and save relevance data\n","    if save_to_file:\n","        save_relevance_to_csv(tot_relevant, fileinfo)\n","    return tot_relevant\n","\n","# Build the filename for a relevance file given some dataset and preprocessing attributes\n","def build_relevance_filename(\n","    path,                              # Base path for the file\n","    filename=None,                     # Name of the csv file to load, if None it will be inferred from dataset attributes\n","    test_split=0.2,                    # Test split percentage\n","    val_split=0,                       # Validation split percentage\n","    split=\"train\",                     # Either \"train\", \"test\" or \"val\"\n","    metric=\"\",                         # Metric used to compute relevance\n","    other=[],                          # Other attributes as an ordered list of (name, value) tuples\n","):\n","    if not filename:\n","        filename = \"TotRelevant_\" + str(test_split) + \"_\" + str(val_split) + \"_\" + split + \"_\" + metric\n","        for attr in other:\n","            filename += \"_\" + attr[0] + \"-\" + str(attr[1])\n","        filename += \".csv\"\n","    filename = path + filename\n","    return filename\n","\n","# Load a csv relevance file given a filename or some dataset and preprocessing attributes\n","def load_relevance_from_csv(fileinfo={}):\n","    filename = build_relevance_filename(**fileinfo)\n","    if not os.path.exists(filename):\n","        print(f\"The relevance file \\\"{filename}\\\" does not exist!\")\n","        return False\n","    else:\n","        try:\n","            return np.squeeze(pd.read_csv(filename, header=None).values.tolist())\n","        except OSError as error:\n","            print(f\"Couldn't load file \\\"{filename}\\\": {error}\")\n","    return False\n","\n","# Save total relevant data to a csv relevance file given a filename or some dataset and preprocessing attributes\n","def save_relevance_to_csv(tot_relevant, fileinfo={}):\n","    filename = build_relevance_filename(**fileinfo)\n","    if os.path.exists(filename):\n","        print(f\"Overwriting \\\"{filename}\\\" relevance file!\")\n","    df = pd.DataFrame(tot_relevant)\n","    try:\n","        df.to_csv(filename, index=False, header=False)\n","        return True\n","    except OSError as error:\n","            print(f\"Couldn't save file \\\"{filename}\\\": {error}\")\n","    return False"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[],"source":["def compute_top_k_accuracy(results, dataset_reference, relevant_at_k):\n","    hits = np.count_nonzero(relevant_at_k)\n","    return hits / len(dataset_reference)\n","\n","def compute_map_k(results, dataset_reference, relevant_at_k, k=None):\n","    if not k:\n","        k = len(results[0])\n","    precision_at_k = [r/k for r in relevant_at_k]\n","    return np.sum(precision_at_k) / len(dataset_reference)\n","\n","def compute_mar_k(results, dataset_reference, relevant_at_k, total_relevant):\n","    recall_at_k = [rk/tr for rk, tr in zip(relevant_at_k, total_relevant)]\n","    return np.sum(recall_at_k) / len(dataset_reference)\n","\n","def compute_F1_k(precision=0, recall=0):\n","    if precision + recall == 0:\n","        f1_score = 0\n","    else:\n","        f1_score = 2 * (precision * recall) / (precision + recall)\n","        return f1_score"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[],"source":["# Visualize results for text to image queries\n","def visualize_t2i_results(query, matches):\n","    print(\"Top matches for query: \\\"\" + query + \"\\\"\")\n","    if \"image path\" in matches[0]:\n","        plt.figure(figsize=(18, 18))\n","    for i in range(len(matches)):\n","        if \"image path\" in matches[i]:\n","            path = matches[i][\"image path\"].numpy().decode('UTF-8')\n","            ax = plt.subplot(3, 3, i + 1)\n","            plt.imshow(mpimg.imread(path))\n","            plt.axis(\"off\")\n","        if \"caption\" in matches[i]:\n","            caption = matches[i][\"caption\"].numpy().decode('UTF-8')\n","            print(f\"{i}) {caption}\")\n","        \n","# Standard isualization for a multi-purpose plotly graph\n","def visualize_multigraph(functions, titlexyf=(None, None, None), legend=True):\n","    fig = go.Figure()\n","    for function in functions:\n","        x = function['x']\n","        y = function['y']\n","        label = function['label'] if 'label' in function else \"\"\n","        color = function['color'] if 'color' in function else None\n","        linestyle = function['style'] if 'style' in function else \"solid\"\n","        marker = go.scatter.Marker(symbol=function['marker'], size=10) if 'marker' in function else None\n","        opacity = function['opacity'] if 'opacity' in function else 1\n","        k = len(x)\n","        fig.add_trace(go.Scatter(\n","            x=x, y=y,\n","            line=go.scatter.Line(color=color, dash=linestyle),\n","            opacity=opacity,\n","            marker=marker,\n","            mode=\"lines+markers+text\" if marker else \"lines+text\",\n","            name=label,\n","        ))\n","    fig.update_xaxes(\n","        title=titlexyf[0],\n","        ticks=\"outside\", ticklen=8, minor=dict(dtick=0.5, ticklen=6, tickcolor=\"black\", showgrid=True), ticklabelstep=1, dtick=1, \n","        range=(1,k), \n","    )\n","    fig.update_yaxes(\n","        title=titlexyf[1],\n","        ticks=\"outside\", ticklen=8, minor=dict(dtick=0.01, ticklen=6, tickcolor=\"black\", showgrid=True), ticklabelstep=1, dtick=0.1,\n","    )\n","    fig.update_layout(\n","        title=titlexyf[2],\n","        width=900, height=600,\n","        margin=dict(l=50, r=50, b=20, t=40, pad=4),\n","        paper_bgcolor=\"LightSteelBlue\",\n","    )\n","    fig.show()"]},{"cell_type":"code","execution_count":19,"metadata":{"trusted":true},"outputs":[],"source":["# Compute baselines for retrieval\n","# Assumption of sampling with repetitions, results get more inaccurate as k/l -> inf\n","def retrieval_baselines(dataset_reference, total_relevant, k, metrics=[]):\n","    l = len(dataset_reference)\n","    metrics_out = {}\n","    for metric in metrics:\n","        if metric[\"id\"] == evm.METRIC_ACCURACY:\n","            metrics_out[evm.METRIC_ACCURACY] = sum([ 1 - pow((l - n_el) / l, k) for n_el in total_relevant]) / l\n","        elif metric[\"id\"] == evm.METRIC_MAP:\n","            metrics_out[evm.METRIC_MAP] = sum([ n_el / l for n_el in total_relevant]) / l\n","        elif metric[\"id\"] == evm.METRIC_MAR:\n","            metrics_out[evm.METRIC_MAR] = sum([ k / l for n_el in total_relevant]) / l\n","        elif metric[\"id\"] == evm.METRIC_F1:\n","            metrics_out[evm.METRIC_F1] = compute_F1_k(metrics_out[evm.METRIC_MAP], metrics_out[evm.METRIC_MAR])\n","    return metrics_out\n","    \n","# Computation of a retrieval report containing metrics\n","def retrieval_report(\n","    results, reference, relevant,   # Task results, dataset reference and relevant hits at k for task\n","    tot_relevant=None,              # Rotal number of relevant elements for each dataset element\n","    k=None,                         # k for metrics computation (should be less or equal than k of retrieval)\n","    baselines=True,                 # Calculate baselines alongside metrics\n","    metrics=[],                     # Metrics to take into consideration\n","    output=True,                    # Print outputs to stdout\n","    title=\"Retrieval Report\",       # Title of the report\n","    decimal_precision=4,            # Decimal precision of values\n","):\n","    if not k:\n","        k = len(results[0])\n","    metrics_out = {}\n","    \n","    for metric in metrics:\n","        if metric[\"id\"] == evm.METRIC_ACCURACY:\n","            metrics_out[evm.METRIC_ACCURACY] = compute_top_k_accuracy(results, reference, relevant)\n","        elif metric[\"id\"] == evm.METRIC_MAP:\n","            metrics_out[evm.METRIC_MAP] = compute_map_k(results, reference, relevant, k=k)\n","        elif metric[\"id\"] == evm.METRIC_MAR:\n","            metrics_out[evm.METRIC_MAR] = compute_mar_k(results, reference, relevant, tot_relevant)\n","        elif metric[\"id\"] == evm.METRIC_F1:\n","            metrics_out[evm.METRIC_F1] = compute_F1_k(metrics_out[evm.METRIC_MAP], metrics_out[evm.METRIC_MAR])\n","            \n","    if baselines:\n","            baselines = retrieval_baselines(reference, tot_relevant, k, metrics=metrics)\n","            \n","    if output:\n","        print(f\"\\n ### {title} ###\")\n","        for metric in metrics:\n","            string = f\"{metric['name']:<30}: {round(metrics_out[metric['id']] * 100, decimal_precision):10}%\"\n","            if baselines:\n","                string += f\"{'   Baseline':<8}: {round(baselines[metric['id']] * 100, decimal_precision):10}%\"\n","            print(string)\n","    \n","    if baselines:\n","        return metrics_out, baselines\n","    return metrics_out\n","        \n","# Computation of a retrieval report in graph form containing metrics \n","def retrieval_graph_report(\n","    results, reference,                 # Task results and dataset reference\n","    tot_relevant=None,                  # Total number of relevant elements for each dataset element\n","    k_range=(1, 10),                    # k range for metrics computation (maximum value shoul not be greater than k of retrieval)\n","    baselines=True,                     # Calculate baselines alongside metrics\n","    metrics=[],                         # Metrics to take into consideration\n","    titlexyf=(None, None, None),        # Tuple containing: (title of x axis, title of y axis, figure title)\n","    reference_preprocess=lambda x: x,   # Function to preprocess data contained in the reference dataset\n","    relevance=lambda m, o: m == o,      # Function to compare elements\n","    functions=None,                     # Plot pre-existing function data\n","):\n","    if not functions:\n","        functions = {metric[\"id\"]: {\"x\": [], \"y\": [], \"label\": metric[\"id\"], \"color\": metric[\"color\"], \"opacity\": 0.8} for metric in metrics}\n","        if baselines:\n","            functions |= {metric[\"id\"] + \"_base\": {\"x\": [], \"y\": [], \"label\": metric[\"id\"] + \" Baseline\", \"color\": metric[\"color\"], \"style\": \"dash\", \"opacity\": 0.5} for metric in metrics}\n","        for k in range(k_range[0], k_range[1] + 1):\n","            relevant = compute_relevant_at_k(results, reference, k=k, reference_preprocess=reference_preprocess, relevance=relevance)\n","            report = retrieval_report(results, reference, relevant, tot_relevant, k=k, baselines=baselines, metrics=metrics, output=False)\n","            metrics_out = report[0] if baselines else report\n","            baselines = report[1] if baselines else None\n","            for metric in metrics_out:\n","                functions[metric][\"x\"].append(k)\n","                functions[metric][\"y\"].append(metrics_out[metric])\n","                if baselines:\n","                    functions[metric + \"_base\"][\"x\"].append(k)\n","                    functions[metric + \"_base\"][\"y\"].append(baselines[metric])\n","    visualize_multigraph(functions.values(), titlexyf)\n","    return functions\n","\n","# Computation of a retrieval report in graph form containing metrics, comparing multiple models on the same dataset\n","def retrieval_graph_compare(\n","    multi_results, reference,           # Per-model task results and dataset reference\n","    model_ids,                          # List of ordered model ids and labels in the form {\"id\": id, \"label\": label} \n","    tot_relevant=None,                  # Total number of relevant elements for each dataset element\n","    k_range=(1, 10),                    # k range for metrics computation (maximum value shoul not be greater than k of retrieval)\n","    metrics=[],                         # Metrics to take into consideration\n","    titlexyf=(None, None, None),        # Tuple containing: (title of x axis, title of y axis, figure title)\n","    reference_preprocess=lambda x: x,   # Function to preprocess data contained in the reference dataset\n","    relevance=lambda m, o: m == o,      # Function to compare elements\n","    functions=None,                     # Plot pre-existing function data\n","):\n","    if not functions:\n","        # Add random markers\n","        linestyles = [\"solid\", \"dash\", \"dot\", \"dashdot\", \"longdash\", \"longdashdot\"]\n","        if len(linestyles) < len(model_ids):\n","            print(\"Too many models!\")\n","            return None\n","        linestyles = linestyles[:len(model_ids)]\n","        model_ids = [model | {\"style\": style} for model, style in zip(model_ids, linestyles)]\n","        # Generate function models\n","        functions = {\n","            metric[\"id\"] + model[\"id\"]: \n","            {\"x\": [], \"y\": [], \"label\": model[\"label\"] + \" \" + metric[\"id\"], \"color\": metric[\"color\"], \"style\": model[\"style\"], \"opacity\": 0.8} \n","            for model in model_ids for metric in metrics\n","        }\n","        # Fill functions\n","        for model, results in zip(model_ids, multi_results):\n","            for k in range(k_range[0], k_range[1] + 1):\n","                relevant = compute_relevant_at_k(results, reference, k=k, reference_preprocess=reference_preprocess, relevance=relevance)\n","                metrics_out = retrieval_report(results, reference, relevant, tot_relevant, k=k, baselines=False, metrics=metrics, output=False)\n","                for metric in metrics_out:\n","                    functions[metric + model[\"id\"]][\"x\"].append(k)\n","                    functions[metric + model[\"id\"]][\"y\"].append(metrics_out[metric])\n","    visualize_multigraph(functions.values(), titlexyf)\n","    return functions\n","    \n","# Manually compute some text to image queries\n","def manual_t2i_queries(queries, text_encoder, image_embeddings, dataset_reference, k=10, normalize=True):\n","    results = find_t2i_matches(queries, clip_text_encoder, test_image_embeddings, k=k, normalize=normalize)\n","    results = index_to_reference(results, dataset_reference)\n","    for query, matches in zip(queries, results):\n","        visualize_t2i_results(query, matches)"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset Metrics"]},{"cell_type":"code","execution_count":20,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Generating image embeddings\n","521/521 [==============================] - 11s 18ms/step\n","Generating text embeddings\n","521/521 [==============================] - 18s 34ms/step\n","The relevance file \"./relevanceTotRelevant_0.2_0_test_cap.csv\" does not exist!\n","Proceeding with total relevance calculation...\n","The relevance file \"./relevanceTotRelevant_0.2_0_test_con_conthresh-2.csv\" does not exist!\n","Proceeding with total relevance calculation...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 14403/14403 [01:09<00:00, 207.79it/s]\n"]}],"source":["# Generating embeddings for image-to-text and text-to-image tasks\n","test_dataset_reference, test_image_embeddings = generate_image_embeddings(\n","    clip_image_encoder,\n","    test_dataset,\n","    dataset_pred_map=lambda x, y: x['image'],\n","    dataset_ref_map=lambda x, y: y | {'image path': x['image path']}\n",")\n","\n","_, test_text_embeddings = generate_text_embeddings(\n","    clip_text_encoder,\n","    test_dataset,\n","    dataset_pred_map=lambda x, y: y['caption'],\n","    dataset_ref_map=lambda x, y: y | {'image path': x['image path']}\n",")\n","\n","# Compute relevance for all the test queries in the dataset \n","test_tot_relevant_cap = compute_total_relevance(test_dataset_reference, reference_preprocess=reference_preprocess_cap, save_to_file=False, fileinfo=relevance_fileinfo_cap | {\"split\": \"test\"})\n","test_tot_relevant_con = compute_total_relevance(test_dataset_reference, reference_preprocess=reference_preprocess_con_hash, relevance=concept_relevance_hash, save_to_file=False, fileinfo=relevance_fileinfo_con | {\"split\": \"test\"})"]},{"cell_type":"markdown","metadata":{},"source":["## Text to Image Task"]},{"cell_type":"code","execution_count":21,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","### Scoring test data ###\n","Computing Text-to-Image matches\n","521/521 [==============================] - 18s 34ms/step\n","Computing Text-to-Image matches\n","521/521 [==============================] - 17s 33ms/step\n"]}],"source":["print(\"\\n### Scoring test data ###\")\n","\n","test_queries = test_dataset.map(lambda x, y: y[\"caption\"])\n","\n","# Compute matching results and extrapolate relevant matches based on different criterions\n","test_raw_results = find_t2i_matches(test_queries, clip_text_encoder, test_image_embeddings, k=k, normalize=True)\n","test_results = index_to_reference(test_raw_results, test_dataset_reference)\n","test_relevant_cap = compute_relevant_at_k(test_results, test_dataset_reference, k=k, reference_preprocess=reference_preprocess_cap)\n","test_relevant_con = compute_relevant_at_k(test_results, test_dataset_reference, k=k, reference_preprocess=reference_preprocess_con, relevance=concept_relevance)\n","\n","# Compute alternative caption results based on concept text concatenation\n","test_queries_fromconcepts = tf.data.Dataset.from_tensor_slices(decode_concepts([e[\"concepts\"] for e in test_dataset_reference], concepts_onehot, concept_list, string_form=True)).batch(batch_size)\n","test_results_fromconcepts = index_to_reference(find_t2i_matches(test_queries_fromconcepts, clip_text_encoder, test_image_embeddings, k=k, normalize=True), test_dataset_reference)"]},{"cell_type":"markdown","metadata":{},"source":["### Caption equality relevance metric"]},{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," ### Test Data - Caption equality metrics @ k=10 ###\n","Accuracy                      :    10.3873%   Baseline:     0.0785%\n","Mean Average Precision        :     1.0429%   Baseline:     0.0079%\n","Mean Average Recall           :    10.2732%   Baseline:       0.06%\n","F1 Score                      :     1.8936%   Baseline:     0.0139%\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"line":{"color":"green","dash":"solid"},"mode":"lines+text","name":"Accuracy","opacity":0.8,"type":"scatter","x":[1,2,3,4,5,6,7,8,9,10],"y":[0.014650255178625038,0.027799459621735214,0.039747823476433505,0.049954968477934555,0.06034223956769739,0.07006904833383368,0.07805463824677274,0.0881416991894326,0.09588712098468928,0.10387271089762834]},{"line":{"color":"blue","dash":"solid"},"mode":"lines+text","name":"MAP","opacity":0.8,"type":"scatter","x":[1,2,3,4,5,6,7,8,9,10],"y":[0.014650255178625038,0.013899729810867607,0.0132492744921445,0.012488742119483639,0.01206844791353948,0.011678174722305614,0.011150662606681818,0.011017712398679075,0.010654124553854364,0.01042930051035725]},{"line":{"color":"red","dash":"solid"},"mode":"lines+text","name":"MAR","opacity":0.8,"type":"scatter","x":[1,2,3,4,5,6,7,8,9,10],"y":[0.014470129090363255,0.027138997298108675,0.038967277093965774,0.049174422095466824,0.059561693185229664,0.06922845992194535,0.0772140498348844,0.08724106874812368,0.09492644851395976,0.10273191233863703]},{"line":{"color":"blueviolet","dash":"solid"},"mode":"lines+text","name":"F1","opacity":0.8,"type":"scatter","x":[1,2,3,4,5,6,7,8,9,10],"y":[0.014559635043499523,0.018383841622566666,0.01977488496451157,0.01991875325385998,0.020070243638275212,0.019985061899544895,0.019487141290108662,0.019564602646233664,0.01915803592528671,0.01893620541542023]},{"line":{"color":"green","dash":"dash"},"mode":"lines+text","name":"Accuracy Baseline","opacity":0.5,"type":"scatter","x":[1,2,3,4,5,6,7,8,9,10],"y":[0.00007873058423991576,0.00015741237688128248,0.0002360455029428421,0.0003146300870771098,0.0003931662535702296,0.0004716541263441168,0.000550093828956902,0.0006284854846053034,0.0007068292161228052,0.0007851251459845208]},{"line":{"color":"blue","dash":"dash"},"mode":"lines+text","name":"MAP Baseline","opacity":0.5,"type":"scatter","x":[1,2,3,4,5,6,7,8,9,10],"y":[0.00007873058423993139,0.00007873058423993139,0.00007873058423993139,0.00007873058423993139,0.00007873058423993139,0.00007873058423993139,0.00007873058423993139,0.00007873058423993139,0.00007873058423993139,0.00007873058423993139]},{"line":{"color":"red","dash":"dash"},"mode":"lines+text","name":"MAR Baseline","opacity":0.5,"type":"scatter","x":[1,2,3,4,5,6,7,8,9,10],"y":[0.00006004202942058159,0.00012008405884116318,0.0001801260882617893,0.00024016811768232636,0.00030021014710292525,0.0003602521765235786,0.00042029420594413933,0.0004803362353646527,0.0005403782647855157,0.0006004202942058505]},{"line":{"color":"blueviolet","dash":"dash"},"mode":"lines+text","name":"F1 Baseline","opacity":0.5,"type":"scatter","x":[1,2,3,4,5,6,7,8,9,10],"y":[0.00006812790983093838,0.00009510655718261922,0.00010956976328751853,0.00011858672429180193,0.00012474626410526458,0.00012922085724768566,0.00013261869565411578,0.00013528669960637466,0.00013743721015809708,0.00013920747819841592]}],"layout":{"height":600,"margin":{"b":20,"l":50,"pad":4,"r":50,"t":40},"paper_bgcolor":"LightSteelBlue","template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Test Data - Caption equality metrics"},"width":900,"xaxis":{"dtick":1,"minor":{"dtick":0.5,"showgrid":true,"tickcolor":"black","ticklen":6},"range":[1,10],"ticklabelstep":1,"ticklen":8,"ticks":"outside","title":{"text":"k"}},"yaxis":{"dtick":0.1,"minor":{"dtick":0.01,"showgrid":true,"tickcolor":"black","ticklen":6},"ticklabelstep":1,"ticklen":8,"ticks":"outside","title":{}}}}},"metadata":{},"output_type":"display_data"}],"source":["_ = retrieval_report(\n","    test_results, test_dataset_reference, test_relevant_cap, test_tot_relevant_cap,\n","    k=k,\n","    metrics=metrics,\n","    title=f\"Test Data - Caption equality metrics @ k={k}\",\n","    decimal_precision=decimal_precision\n",")\n","_ = retrieval_graph_report(\n","    test_results, test_dataset_reference, test_tot_relevant_cap,\n","    k_range=(1, k),\n","    metrics=metrics, \n","    titlexyf=(\"k\", None, \"Test Data - Caption equality metrics\"),\n","    reference_preprocess=reference_preprocess_cap\n",")"]},{"cell_type":"code","execution_count":23,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"line":{"color":"green","dash":"solid"},"mode":"lines+text","name":"Dataset Caption Accuracy","opacity":0.8,"type":"scatter","x":[1,2,3,4,5,6,7,8,9,10],"y":[0.014650255178625038,0.027799459621735214,0.039747823476433505,0.049954968477934555,0.06034223956769739,0.07006904833383368,0.07805463824677274,0.0881416991894326,0.09588712098468928,0.10387271089762834]},{"line":{"color":"blue","dash":"solid"},"mode":"lines+text","name":"Dataset Caption MAP","opacity":0.8,"type":"scatter","x":[1,2,3,4,5,6,7,8,9,10],"y":[0.014650255178625038,0.013899729810867607,0.0132492744921445,0.012488742119483639,0.01206844791353948,0.011678174722305614,0.011150662606681818,0.011017712398679075,0.010654124553854364,0.01042930051035725]},{"line":{"color":"red","dash":"solid"},"mode":"lines+text","name":"Dataset Caption MAR","opacity":0.8,"type":"scatter","x":[1,2,3,4,5,6,7,8,9,10],"y":[0.014470129090363255,0.027138997298108675,0.038967277093965774,0.049174422095466824,0.059561693185229664,0.06922845992194535,0.0772140498348844,0.08724106874812368,0.09492644851395976,0.10273191233863703]},{"line":{"color":"blueviolet","dash":"solid"},"mode":"lines+text","name":"Dataset Caption F1","opacity":0.8,"type":"scatter","x":[1,2,3,4,5,6,7,8,9,10],"y":[0.014559635043499523,0.018383841622566666,0.01977488496451157,0.01991875325385998,0.020070243638275212,0.019985061899544895,0.019487141290108662,0.019564602646233664,0.01915803592528671,0.01893620541542023]},{"line":{"color":"green","dash":"dash"},"mode":"lines+text","name":"Concept Fusion Accuracy","opacity":0.8,"type":"scatter","x":[1,2,3,4,5,6,7,8,9,10],"y":[0.006964875412788952,0.013989792854998498,0.019513659561693184,0.026178324827379165,0.03032122485740018,0.036325427799459624,0.041789252476733714,0.04737316121284899,0.052716901831281894,0.05716001200840588]},{"line":{"color":"blue","dash":"dash"},"mode":"lines+text","name":"Concept Fusion MAP","opacity":0.8,"type":"scatter","x":[1,2,3,4,5,6,7,8,9,10],"y":[0.006964875412788952,0.006994896427499249,0.006504553187231061,0.006544581206844791,0.006064244971480037,0.006054237966576603,0.005969893210961958,0.005921645151606124,0.005857433536809099,0.005716001200840588]},{"line":{"color":"red","dash":"dash"},"mode":"lines+text","name":"Concept Fusion MAR","opacity":0.8,"type":"scatter","x":[1,2,3,4,5,6,7,8,9,10],"y":[0.006904833383368358,0.013575162991452505,0.019099029698147196,0.02559857938292654,0.029741479412947556,0.0357156613402967,0.04117948601757079,0.04670335272426548,0.05157476271125637,0.05569491954831504]},{"line":{"color":"blueviolet","dash":"dash"},"mode":"lines+text","name":"Concept Fusion F1","opacity":0.8,"type":"scatter","x":[1,2,3,4,5,6,7,8,9,10],"y":[0.006934724436976013,0.009232531338645024,0.00970416172238534,0.010424113775839327,0.010074345377734358,0.010353441903185786,0.010428011483062277,0.010510620176203352,0.010520083315395069,0.010367935316236596]}],"layout":{"height":600,"margin":{"b":20,"l":50,"pad":4,"r":50,"t":40},"paper_bgcolor":"LightSteelBlue","template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Test Data - Dataset Captions vs Concept-Fusion Captions"},"width":900,"xaxis":{"dtick":1,"minor":{"dtick":0.5,"showgrid":true,"tickcolor":"black","ticklen":6},"range":[1,10],"ticklabelstep":1,"ticklen":8,"ticks":"outside","title":{"text":"k"}},"yaxis":{"dtick":0.1,"minor":{"dtick":0.01,"showgrid":true,"tickcolor":"black","ticklen":6},"ticklabelstep":1,"ticklen":8,"ticks":"outside","title":{}}}}},"metadata":{},"output_type":"display_data"}],"source":["compare_models = [{\"id\": \"ds\", \"label\": \"Dataset Caption\"}, {\"id\": \"cpfs\", \"label\": \"Concept Fusion\"}]\n","_ = retrieval_graph_compare(\n","    [test_results, test_results_fromconcepts], test_dataset_reference, compare_models, test_tot_relevant_cap,\n","    k_range=(1, k),\n","    metrics=metrics, \n","    titlexyf=(\"k\", None, \"Test Data - Dataset Captions vs Concept-Fusion Captions\"),\n","    reference_preprocess=reference_preprocess_cap\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Concept overlap relevance metric"]},{"cell_type":"code","execution_count":23,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," ### Test Data - Concept overlap metrics @ k=10 ###\n","Accuracy                      :    77.4842%   Baseline:    26.6456%\n","Mean Average Precision        :    32.3254%   Baseline:     3.7382%\n","Mean Average Recall           :     0.7177%   Baseline:       0.06%\n","F1 Score                      :     1.4041%   Baseline:     0.1182%\n"]},{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"line":{"color":"green","dash":"solid"},"mode":"lines+text","name":"Accuracy","opacity":0.8,"type":"scatter","x":[1,2,3,4,5,6,7,8,9,10],"y":[0.3289102371660162,0.45541879315520867,0.5362954067847493,0.5981386970879615,0.6438907235064545,0.6779945962173521,0.7081356949864905,0.7339537676373461,0.756469528670069,0.774842389672771]},{"line":{"color":"blue","dash":"solid"},"mode":"lines+text","name":"MAP","opacity":0.8,"type":"scatter","x":[1,2,3,4,5,6,7,8,9,10],"y":[0.3289102371660162,0.32755929150405283,0.3273491444010807,0.32761933353347344,0.32632842990093064,0.3252576803762634,0.32434704293005095,0.3239192434704293,0.3237866506554588,0.3232542779945962]},{"line":{"color":"red","dash":"solid"},"mode":"lines+text","name":"MAR","opacity":0.8,"type":"scatter","x":[1,2,3,4,5,6,7,8,9,10],"y":[0.0007228028910100744,0.0014831346899038863,0.0023063304576835613,0.003028540434323346,0.0037408828876165103,0.004417944361330042,0.005092512212956828,0.005784907140264365,0.0065053093347963225,0.007176571586040264]},{"line":{"color":"blueviolet","dash":"solid"},"mode":"lines+text","name":"F1","opacity":0.8,"type":"scatter","x":[1,2,3,4,5,6,7,8,9,10],"y":[0.0014424359297555691,0.002952899137350955,0.004580389889488861,0.0060016015634132055,0.007396970223288891,0.008717480014734871,0.010027583218661802,0.011366813192677723,0.012754366294920027,0.014041409689598943]},{"line":{"color":"green","dash":"dash"},"mode":"lines+text","name":"Accuracy Baseline","opacity":0.5,"type":"scatter","x":[1,2,3,4,5,6,7,8,9,10],"y":[0.03738243429061417,0.07110950906493718,0.10189048483557155,0.130252599525714,0.15659215650826686,0.1812108382539269,0.20434156816198293,0.22616697173977424,0.24683259191365792,0.2664563810839064]},{"line":{"color":"blue","dash":"dash"},"mode":"lines+text","name":"MAP Baseline","opacity":0.5,"type":"scatter","x":[1,2,3,4,5,6,7,8,9,10],"y":[0.037382434290614164,0.037382434290614164,0.037382434290614164,0.037382434290614164,0.037382434290614164,0.037382434290614164,0.037382434290614164,0.037382434290614164,0.037382434290614164,0.037382434290614164]},{"line":{"color":"red","dash":"dash"},"mode":"lines+text","name":"MAR Baseline","opacity":0.5,"type":"scatter","x":[1,2,3,4,5,6,7,8,9,10],"y":[0.00006004202942058159,0.00012008405884116318,0.0001801260882617893,0.00024016811768232636,0.00030021014710292525,0.0003602521765235786,0.00042029420594413933,0.0004803362353646527,0.0005403782647855157,0.0006004202942058505]},{"line":{"color":"blueviolet","dash":"dash"},"mode":"lines+text","name":"F1 Baseline","opacity":0.5,"type":"scatter","x":[1,2,3,4,5,6,7,8,9,10],"y":[0.00011989149437154166,0.0002393990929969227,0.00035852463679544335,0.0004772699549343361,0.0005956368649233053,0.0007136271727063791,0.0008312426727537981,0.000948485148153638,0.0010653563707022225,0.001181858100990215]}],"layout":{"height":600,"margin":{"b":20,"l":50,"pad":4,"r":50,"t":40},"paper_bgcolor":"LightSteelBlue","template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"title":{"text":"Test Data - Concept overlap metrics"},"width":900,"xaxis":{"dtick":1,"minor":{"dtick":0.5,"showgrid":true,"tickcolor":"black","ticklen":6},"range":[1,10],"ticklabelstep":1,"ticklen":8,"ticks":"outside","title":{"text":"k"}},"yaxis":{"dtick":0.1,"minor":{"dtick":0.01,"showgrid":true,"tickcolor":"black","ticklen":6},"ticklabelstep":1,"ticklen":8,"ticks":"outside","title":{}}}}},"metadata":{},"output_type":"display_data"}],"source":["_ = retrieval_report(\n","    test_results, test_dataset_reference, test_relevant_con, test_tot_relevant_con,\n","    k=k,\n","    metrics=metrics,\n","    title=f\"Test Data - Concept overlap metrics @ k={k}\",\n","    decimal_precision=decimal_precision,\n",")\n","_ = retrieval_graph_report(\n","    test_results, test_dataset_reference, test_tot_relevant_con,\n","    metrics=metrics,\n","    k_range=(1, k),\n","    titlexyf=(\"k\", None, \"Test Data - Concept overlap metrics\"),\n","    reference_preprocess=reference_preprocess_con, relevance=concept_relevance,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Image to Text"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"\\n### Scoring test data ###\")\n","\n","test_queries = test_dataset.map(lambda x, y: x[\"image\"])\n","\n","# Compute matching results and extrapolate relevant matches based on different criterions\n","test_raw_results = find_i2t_matches(test_queries, clip_image_encoder, test_text_embeddings, k=k, normalize=True)\n","test_results = index_to_reference(test_raw_results, test_dataset_reference)\n","test_relevant_cap = compute_relevant_at_k(test_results, test_dataset_reference, k=k, reference_preprocess=reference_preprocess_cap)\n","test_relevant_con = compute_relevant_at_k(test_results, test_dataset_reference, k=k, reference_preprocess=reference_preprocess_con, relevance=concept_relevance)"]},{"cell_type":"markdown","metadata":{},"source":["### Caption equality relevance metric"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["_ = retrieval_report(\n","    test_results, test_dataset_reference, test_relevant_cap, test_tot_relevant_cap,\n","    k=k,\n","    metrics=metrics,\n","    title=f\"Test Data - Caption equality metrics @ k={k}\",\n","    decimal_precision=decimal_precision\n",")\n","_ = retrieval_graph_report(\n","    test_results, test_dataset_reference, test_tot_relevant_cap,\n","    k_range=(1, k),\n","    metrics=metrics, \n","    titlexyf=(\"k\", None, \"Test Data - Caption equality metrics\"),\n","    reference_preprocess=reference_preprocess_cap\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Concept overlap relevance metric"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["_ = retrieval_report(\n","    test_results, test_dataset_reference, test_relevant_con, test_tot_relevant_con,\n","    k=k,\n","    metrics=metrics,\n","    title=f\"Test Data - Concept overlap metrics @ k={k}\",\n","    decimal_precision=decimal_precision,\n",")\n","_ = retrieval_graph_report(\n","    test_results, test_dataset_reference, test_tot_relevant_con,\n","    metrics=metrics,\n","    k_range=(1, k),\n","    titlexyf=(\"k\", None, \"Test Data - Concept overlap metrics\"),\n","    reference_preprocess=reference_preprocess_con, relevance=concept_relevance,\n",")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":4}

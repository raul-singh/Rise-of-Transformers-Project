{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Declarations","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport math\nimport string\nimport random\nimport requests\nimport importlib\nimport itertools\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimport plotly.graph_objects as go\n\nfrom tqdm import tqdm\n\nfrom IPython.display import display\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\ntfk = tf.keras\ntfkl = tf.keras.layers\nkb = tf.keras.backend","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tf.__version__)\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Constants","metadata":{}},{"cell_type":"code","source":"# Randomness\nseed = 42\n\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filepaths\nkaggle = False\n\nmodel_versions = [\"v2.1\", \"v2.1\"]\n\ngithub_repo = \"raul-singh/Rise-of-Transformers-Project\"\ngithub_branch = \"main\"\ngithub_python_prefix = [\"Code\", \"Notebooks\", \"py_files\"]\ngithub_clip_models_prefix = [\"Code\", \"Models\"]\ngithub_pyfiles_data = [\n    {\"name\": \"preprocessing\", \"imports\": [\"import_datasets\"]}, \n    {\"name\": \"evaluation\", \"imports\": [\n        \"EvalMetrics as evm\", \"generate_dataset_reference\", \"compute_total_relevance\", \"generate_image_embeddings\", \"generate_text_embeddings\", \n        \"find_t2i_matches\", \"find_i2t_matches\", \"index_to_reference\", \"compute_relevant_at_k\"\n    ]}, \n    {\"name\": \"visualization\", \"imports\": [\"retrieval_report\", \"retrieval_graph_compare\"]}, \n    {\"name\": \"clip\", \"imports\": [\"build_clip\"]}\n]\ngithub_pyfiles = [\"/\".join(github_python_prefix) + \"/\" + pf[\"name\"] + \".py\" for pf in github_pyfiles_data]\ngithub_clip_models = [f\"{'/'.join(github_clip_models_prefix)}/{version}.yaml\" for version in model_versions]\n\nkaggle_dataset1 = \"/kaggle/input/transformers-hackathon/\"\nkaggle_dataset2 = \"/kaggle/input/transformers-hackathon-features/\"\nkaggle_weights = \"/kaggle/input/clip-weights/\"\nkaggle_relevance = \"/kaggle/input/clip-relevance/\"\n\nimage_dir = \"./resized_train\"\nrelevance_dir = \"./relevance\"\ncaption_pred_file = \"caption_prediction_train.csv\"\nconcept_det_file = \"concept_detection_train.csv\"\nconcept_file = \"concepts.csv\"\nclip_weights_files = [f\"{version}.h5\" for version in model_versions]\n\nif kaggle:\n    image_dir = kaggle_dataset1 + image_dir\n    relevance_dir = kaggle_relevance + relevance_dir\n    caption_pred_file = kaggle_dataset2 + caption_pred_file\n    concept_det_file = kaggle_dataset2 + concept_det_file\n    concept_file = kaggle_dataset2 + concept_file\n    clip_weights_files = [kaggle_weights + weight for weight in clip_weights_files]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train/Val/Test split and filter percentages\ntest_size = 0.2\nval_size = 0\nfilter_percent_dataset = 1\n\n# Batch size\nbatch_size = 32\n\n# Import dataset types and shapes\nin_feat_typ = {'caption': tf.string, 'concepts': tf.bool, 'image path': tf.string}\nfeature_shapes = {'image': (128, 128, 3), 'caption': (), 'concepts': (8374)}\n\n# Output dataset structure\nx_features_eval = ['image path', 'image']\ny_features_eval = ['caption', 'concepts']\n\n# Define parameters for dataset import\ndataset_parameters = [{\n    'x_features': x_features_eval, 'y_features': y_features_eval,\n    'x_dict': True, 'y_dict': True,           \n    'shuffle_buffer_size': 1,\n    'batch_size': batch_size,\n    'cached': True,\n}]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Meta-Imports","metadata":{}},{"cell_type":"code","source":"def clean_recursive_imports(source, import_list, prefix):\n    import_prefix = re.sub(r\"/\", \".\", prefix)\n    for target_import in import_list:\n        source = re.sub(r\"from[ \\t]+\" + re.escape(target_import) + r\"[ \\t]+import\", f\"from {import_prefix + target_import} import\", source)\n    return source\n    \ndef import_py_from_repo(repository, branch, filepath, prefix, recursive_imports_list=None):\n    # Build path for retrieval and write name\n    path_pre = \"https://raw.githubusercontent.com/\"\n    path = path_pre + repository + \"/\" + branch + \"/\" + filepath \n    write_path = prefix + filepath.split(\"/\")[-1]\n    print(\"Downloading file from \" + path)\n    # Obtain raw text from file\n    text = requests.get(path).text\n    # Clean recursive imports\n    text = clean_recursive_imports(text, recursive_imports_list, prefix) if recursive_imports_list else text\n    # Create subdirectories if not exist\n    os.makedirs(os.path.dirname(write_path), exist_ok=True)\n    # Write file\n    f = open(write_path, \"w\")\n    f.write(text)\n    f.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if kaggle:\n    for pf_data, py_file in zip(github_pyfiles_data, github_pyfiles):\n        import_py_from_repo(\n            github_repo, github_branch, py_file, \n            \"/\".join(github_python_prefix) + \"/\", \n            recursive_imports_list=[pf[\"name\"] for pf in github_pyfiles_data],\n        )\n        import_string = f'from {\".\".join(github_python_prefix) + \".\" + pf_data[\"name\"]} import ' + \" \".join(pf_data[\"imports\"])\n        exec(import_string)\n    \n    for model in github_clip_models:\n        import_py_from_repo(github_repo, github_branch, model, \"/\".join(github_clip_models_prefix) + \"/\")\n        \nelse:\n    from pyfiles.clip import build_clip\n    from pyfiles.preprocessing import import_datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"concept_info, datasets, dataset_sizes = import_datasets(\n    image_dir, caption_pred_file, concept_file, concept_det_file,\n    in_feat_typ, feature_shapes,\n    dataset_parameters,\n    filter_percent_dataset,\n    test_size, val_size,\n    seed,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select loaded datasets and variables\nconcept_list, concepts_onehot = concept_info\n_, _, test_dataset = datasets[0]\ntrain_ds_size, val_ds_size, test_ds_size = dataset_sizes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Import","metadata":{}},{"cell_type":"code","source":"models = []\nfor structure, weights, version in zip(github_clip_models, clip_weights_files, model_versions):\n    print(f\"Creating model {version} fom {structure}\")\n    clip_image_encoder, clip_text_encoder, clip = build_clip(structure, weights_path=weights)\n    models.append({\n        \"name\": version,\n        \"image_encoder\": clip_image_encoder,\n        \"text_encoder\": clip_text_encoder,\n        \"clip\": clip,\n    })","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"markdown","source":"## Evaluation Variables","metadata":{}},{"cell_type":"code","source":"# Top-k number\nk = 10\n# Threshold for concept overlap metric\nconcept_overlap_threshold = 2\n# Visualization decimal precision\ndecimal_precision = 4\n# Index to choose model from the array of models\nmodel_index = 0\n# Dictionaries used to load/save total relevance files\nrelevance_fileinfo_cap = {\"path\": relevance_dir, \"test_split\": test_size, \"val_split\": val_size, \"metric\": \"cap\"}\nrelevance_fileinfo_con = {\"path\": relevance_dir, \"test_split\": test_size, \"val_split\": val_size, \"metric\": \"con\", \"other\": [(\"conthresh\", concept_overlap_threshold)]}\n# Function to preprocess data when we want to evaluate captions\nreference_preprocess_cap = lambda x: x[\"caption\"].numpy().decode('UTF-8')          \n# Function to preprocess data when we want to evaluate concepts\nreference_preprocess_con = lambda x: x[\"concepts\"].numpy()\nreference_preprocess_con_hash = lambda x: frozenset(sorted(np.where(x[\"concepts\"].numpy())[0]))\n# Function to compute if a match is relevant given concept arrays \nconcept_relevance = lambda m, o: np.count_nonzero(np.logical_and(m, o)) >= min(concept_overlap_threshold, np.count_nonzero(m), np.count_nonzero(o))\nconcept_relevance_hash = lambda m, o: len(m.intersection(o)) >= min(concept_overlap_threshold, len(m), len(o))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Metric IDs\nMETRIC_ACCURACY = \"Accuracy\"\nMETRIC_MAP = \"MAP\"\nMETRIC_MAR = \"MAR\"\nMETRIC_F1 = \"F1\"\n\n# Metric visualization parameters\nmetrics = [\n    {\"id\": METRIC_ACCURACY, \"name\": \"Accuracy\", \"color\": \"green\"},\n    {\"id\": METRIC_MAP, \"name\": \"Mean Average Precision\", \"color\": \"blue\"},\n    {\"id\": METRIC_MAR, \"name\": \"Mean Average Recall\", \"color\": \"red\"},\n    {\"id\": METRIC_F1, \"name\": \"F1 Score\", \"color\": \"blueviolet\"}\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Metrics","metadata":{}},{"cell_type":"code","source":"# Dataset reference initialization\ndataset_reference = generate_dataset_reference(test_dataset, lambda x, y: y | {'image path': x['image path']})\n\n# Compute relevance for all the test queries in the dataset\ntot_relevant_cap = compute_total_relevance(dataset_reference, reference_preprocess=reference_preprocess_cap, save_to_file=False, fileinfo=relevance_fileinfo_cap | {\"split\": \"test\"})\ntot_relevant_con = compute_total_relevance(dataset_reference, reference_preprocess=reference_preprocess_con_hash, relevance=concept_relevance_hash, save_to_file=False, fileinfo=relevance_fileinfo_con | {\"split\": \"test\"})\n\n# Define model comparison labels\nmodel_labels = [{\"id\": model[\"name\"] + f\"({str(id(model['clip']))})\", \"label\": model[\"name\"]} for model in models] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text to Image Task","metadata":{}},{"cell_type":"code","source":"results = []\nfor model in models:\n    print(f\"\\n### Scoring test data for {model['name']} ###\")\n    test_image_embeddings = generate_image_embeddings(\n        model[\"image_encoder\"],\n        test_dataset,\n        dataset_pred_map=lambda x, y: x['image'],\n    )\n    test_queries = test_dataset.map(lambda x, y: y[\"caption\"])\n    # Compute matching results and extrapolate relevant matches based on different criterions\n    test_raw_results = find_t2i_matches(test_queries, model[\"text_encoder\"], test_image_embeddings, k=k, normalize=True)\n    test_results = index_to_reference(test_raw_results, dataset_reference)\n    test_relevant_cap = compute_relevant_at_k(test_results, dataset_reference, k=k, reference_preprocess=reference_preprocess_cap)\n    test_relevant_con = compute_relevant_at_k(test_results, dataset_reference, k=k, reference_preprocess=reference_preprocess_con, relevance=concept_relevance)\n    \n    results.append({\n        \"results\": test_results,\n        \"relevant_cap\": test_relevant_cap,\n        \"relevant_con\": test_relevant_con,\n    })","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Caption equality relevance metric","metadata":{}},{"cell_type":"code","source":"for model in results:\n    _ = retrieval_report(\n        model[\"results\"], dataset_reference, model[\"relevant_cap\"], tot_relevant_cap,\n        k=k,\n        metrics=metrics,\n        title=f\"Test Data - Caption equality metrics @ k={k}\",\n        decimal_precision=decimal_precision\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = retrieval_graph_compare(\n    [model[\"results\"] for model in results], dataset_reference, model_labels, tot_relevant_cap,\n    k_range=(1, k),\n    metrics=metrics, \n    titlexyf=(\"k\", None, \"Test Data - Caption Equality model comparison\"),\n    reference_preprocess=reference_preprocess_cap\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Concept overlap relevance metric","metadata":{}},{"cell_type":"code","source":"for model in results:\n    _ = retrieval_report(\n        model[\"results\"], dataset_reference, model[\"relevant_con\"], tot_relevant_con,\n        k=k,\n        metrics=metrics,\n        title=f\"Test Data - Concept overlap metrics @ k={k}\",\n        decimal_precision=decimal_precision\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = retrieval_graph_compare(\n    [model[\"results\"] for model in results], dataset_reference, model_labels, tot_relevant_con,\n    k_range=(1, k),\n    metrics=metrics, \n    titlexyf=(\"k\", None, \"Test Data - Concept Overlap model comparison\"),\n    relevance=concept_relevance,\n    reference_preprocess=reference_preprocess_con\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image to Text","metadata":{}},{"cell_type":"code","source":"results = []\nfor model in models:\n    print(f\"\\n### Scoring test data for {model['name']} ###\")\n    test_text_embeddings = generate_text_embeddings(\n        model[\"text_encoder\"],\n        test_dataset,\n        dataset_pred_map=lambda x, y: y['caption'],\n    )\n    test_queries = test_dataset.map(lambda x, y: x[\"image\"])\n    # Compute matching results and extrapolate relevant matches based on different criterions\n    test_raw_results = find_i2t_matches(test_queries, model[\"image_encoder\"], test_text_embeddings, k=k, normalize=True)\n    test_results = index_to_reference(test_raw_results, dataset_reference)\n    test_relevant_cap = compute_relevant_at_k(test_results, dataset_reference, k=k, reference_preprocess=reference_preprocess_cap)\n    test_relevant_con = compute_relevant_at_k(test_results, dataset_reference, k=k, reference_preprocess=reference_preprocess_con, relevance=concept_relevance)\n    \n    results.append({\n        \"results\": test_results,\n        \"relevant_cap\": test_relevant_cap,\n        \"relevant_con\": test_relevant_con,\n    })","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Caption equality relevance metric","metadata":{}},{"cell_type":"code","source":"for model in results:\n    _ = retrieval_report(\n        model[\"results\"], dataset_reference, model[\"relevant_cap\"], tot_relevant_cap,\n        k=k,\n        metrics=metrics,\n        title=f\"Test Data - Caption equality metrics @ k={k}\",\n        decimal_precision=decimal_precision\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = retrieval_graph_compare(\n    [model[\"results\"] for model in results], dataset_reference, model_labels, tot_relevant_cap,\n    k_range=(1, k),\n    metrics=metrics, \n    titlexyf=(\"k\", None, \"Test Data - Caption Equality model comparison\"),\n    reference_preprocess=reference_preprocess_cap\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Concept overlap relevance metric","metadata":{}},{"cell_type":"code","source":"for model in results:\n    _ = retrieval_report(\n        model[\"results\"], dataset_reference, model[\"relevant_con\"], tot_relevant_con,\n        k=k,\n        metrics=metrics,\n        title=f\"Test Data - Concept overlap metrics @ k={k}\",\n        decimal_precision=decimal_precision\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = retrieval_graph_compare(\n    [model[\"results\"] for model in results], dataset_reference, model_labels, tot_relevant_con,\n    k_range=(1, k),\n    metrics=metrics, \n    titlexyf=(\"k\", None, \"Test Data - Concept Overlap model comparison\"),\n    relevance=concept_relevance,\n    reference_preprocess=reference_preprocess_con\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Declarations"]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import os\n","import re\n","import math\n","import string\n","import random\n","import requests\n","import importlib\n","import itertools\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","\n","import plotly.graph_objects as go\n","\n","from tqdm import tqdm\n","\n","from IPython.display import display\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","tfk = tf.keras\n","tfkl = tf.keras.layers\n","kb = tf.keras.backend"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(tf.__version__)\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"]},{"cell_type":"markdown","metadata":{},"source":["## Constants"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Randomness\n","seed = 42\n","\n","random.seed(seed)\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","tf.compat.v1.set_random_seed(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Filepaths\n","kaggle = False\n","\n","model_versions = [\"v4.0\"]\n","\n","github_repo = \"raul-singh/Rise-of-Transformers-Project\"\n","github_branch = \"main\"\n","github_python_prefix = [\"Code\", \"Notebooks\", \"py_files\"]\n","github_clip_models_prefix = [\"Code\", \"Models\"] if kaggle else [\"..\", \"Models\"]\n","github_pyfiles_data = [\n","    {\"name\": \"preprocessing\", \"imports\": [\"import_datasets\"]}, \n","    {\"name\": \"evaluation\", \"imports\": [\n","        \"EvalMetrics as evm\", \"generate_dataset_reference\", \"compute_total_relevance\", \"generate_image_embeddings\", \"generate_text_embeddings\", \n","        \"find_t2i_matches\", \"find_i2t_matches\", \"index_to_reference\", \"compute_relevant_at_k\"\n","    ]}, \n","    {\"name\": \"visualization\", \"imports\": [\"retrieval_report\", \"retrieval_graph_compare\"]}, \n","    {\"name\": \"clip\", \"imports\": [\"build_clip\"]}\n","]\n","github_pyfiles = [\"/\".join(github_python_prefix) + \"/\" + pf[\"name\"] + \".py\" for pf in github_pyfiles_data]\n","github_clip_models = [f\"{'/'.join(github_clip_models_prefix)}/{version}.yaml\" for version in model_versions]\n","\n","kaggle_dataset1 = \"/kaggle/input/transformers-hackathon/\"\n","kaggle_dataset2 = \"/kaggle/input/transformers-hackathon-features/\"\n","kaggle_weights = \"/kaggle/input/clip-weights/\"\n","kaggle_relevance = \"/kaggle/input/clip-relevance/\"\n","\n","image_dir = \"./resized_train\"\n","relevance_dir = \"./relevance\"\n","caption_pred_file = \"caption_prediction_train.csv\"\n","concept_det_file = \"concept_detection_train.csv\"\n","concept_file = \"concepts.csv\"\n","clip_weights_files = [f\"{version}.h5\" for version in model_versions] if kaggle else [None for _ in model_versions]\n","\n","if kaggle:\n","    image_dir = kaggle_dataset1 + image_dir\n","    relevance_dir = kaggle_relevance + relevance_dir\n","    caption_pred_file = kaggle_dataset2 + caption_pred_file\n","    concept_det_file = kaggle_dataset2 + concept_det_file\n","    concept_file = kaggle_dataset2 + concept_file\n","    clip_weights_files = [kaggle_weights + weight for weight in clip_weights_files]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Train/Val/Test split and filter percentages\n","test_size = 0.2\n","val_size = 0\n","filter_percent_dataset = 1\n","\n","# Batch size\n","batch_size = 32\n","\n","# Import dataset types and shapes\n","in_feat_typ = {'caption': tf.string, 'concepts': tf.bool, 'image path': tf.string}\n","feature_shapes = {'image': (128, 128, 3), 'caption': (), 'concepts': (8374)}\n","\n","# Output dataset structure\n","x_features_eval = ['image path', 'image']\n","y_features_eval = ['caption', 'concepts']\n","\n","# Define parameters for dataset import\n","dataset_parameters = [{\n","    'x_features': x_features_eval, 'y_features': y_features_eval,\n","    'x_dict': True, 'y_dict': True,           \n","    'shuffle_buffer_size': 1,\n","    'batch_size': batch_size,\n","    'cached': True,\n","}]"]},{"cell_type":"markdown","metadata":{},"source":["## Meta-Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def clean_recursive_imports(source, import_list, prefix):\n","    import_prefix = re.sub(r\"/\", \".\", prefix)\n","    for target_import in import_list:\n","        source = re.sub(r\"from[ \\t]+\" + re.escape(target_import) + r\"[ \\t]+import\", f\"from {import_prefix + target_import} import\", source)\n","    return source\n","    \n","def import_py_from_repo(repository, branch, filepath, prefix, recursive_imports_list=None):\n","    # Build path for retrieval and write name\n","    path_pre = \"https://raw.githubusercontent.com/\"\n","    path = path_pre + repository + \"/\" + branch + \"/\" + filepath \n","    write_path = prefix + filepath.split(\"/\")[-1]\n","    print(\"Downloading file from \" + path)\n","    # Obtain raw text from file\n","    text = requests.get(path).text\n","    # Clean recursive imports\n","    text = clean_recursive_imports(text, recursive_imports_list, prefix) if recursive_imports_list else text\n","    # Create subdirectories if not exist\n","    os.makedirs(os.path.dirname(write_path), exist_ok=True)\n","    # Write file\n","    f = open(write_path, \"w\")\n","    f.write(text)\n","    f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if kaggle:\n","    for pf_data, py_file in zip(github_pyfiles_data, github_pyfiles):\n","        import_py_from_repo(\n","            github_repo, github_branch, py_file, \n","            \"/\".join(github_python_prefix) + \"/\", \n","            recursive_imports_list=[pf[\"name\"] for pf in github_pyfiles_data],\n","        )\n","        import_string = f'from {\".\".join(github_python_prefix) + \".\" + pf_data[\"name\"]} import {\", \".join(pf_data[\"imports\"])}'\n","        exec(import_string)\n","    \n","    for model in github_clip_models:\n","        import_py_from_repo(github_repo, github_branch, model, \"/\".join(github_clip_models_prefix) + \"/\")\n","        \n","else:\n","    for pf_data in github_pyfiles_data:\n","        import_string = f'from py_files.{pf_data[\"name\"]} import {\", \".join(pf_data[\"imports\"])}'\n","        exec(import_string)"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["concept_info, datasets, dataset_sizes = import_datasets(\n","    image_dir, caption_pred_file, concept_file, concept_det_file,\n","    in_feat_typ, feature_shapes,\n","    dataset_parameters,\n","    filter_percent_dataset,\n","    test_size, val_size,\n","    seed,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Select loaded datasets and variables\n","concept_list, concepts_onehot = concept_info\n","_, _, test_dataset = datasets[0]\n","train_ds_size, val_ds_size, test_ds_size = dataset_sizes"]},{"cell_type":"markdown","metadata":{},"source":["# Model Import"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["models = []\n","for structure, weights, version in zip(github_clip_models, clip_weights_files, model_versions):\n","    print(f\"Creating model {version} fom {structure}\")\n","    clip_image_encoder, clip_text_encoder, clip = build_clip(structure, weights_path=weights)\n","    models.append({\n","        \"name\": version,\n","        \"image_encoder\": clip_image_encoder,\n","        \"text_encoder\": clip_text_encoder,\n","        \"clip\": clip,\n","    })"]},{"cell_type":"markdown","metadata":{},"source":["# Model Evaluation"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation Variables"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Top-k number\n","k = 10\n","# Threshold for concept overlap metric\n","concept_overlap_threshold = 2\n","# Visualization decimal precision\n","decimal_precision = 4\n","# Index to choose model from the array of models\n","model_index = 0\n","# Dictionaries used to load/save total relevance files\n","relevance_fileinfo_cap = {\"path\": relevance_dir, \"test_split\": test_size, \"val_split\": val_size, \"metric\": \"cap\"}\n","relevance_fileinfo_con = {\"path\": relevance_dir, \"test_split\": test_size, \"val_split\": val_size, \"metric\": \"con\", \"other\": [(\"conthresh\", concept_overlap_threshold)]}\n","# Function to preprocess data when we want to evaluate captions\n","reference_preprocess_cap = lambda x: x[\"caption\"].numpy().decode('UTF-8')          \n","# Function to preprocess data when we want to evaluate concepts\n","reference_preprocess_con = lambda x: x[\"concepts\"].numpy()\n","reference_preprocess_con_hash = lambda x: frozenset(sorted(np.where(x[\"concepts\"].numpy())[0]))\n","# Function to compute if a match is relevant given concept arrays \n","concept_relevance = lambda m, o: np.count_nonzero(np.logical_and(m, o)) >= min(concept_overlap_threshold, np.count_nonzero(m), np.count_nonzero(o))\n","concept_relevance_hash = lambda m, o: len(m.intersection(o)) >= min(concept_overlap_threshold, len(m), len(o))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Metric IDs\n","METRIC_ACCURACY = \"Accuracy\"\n","METRIC_MAP = \"MAP\"\n","METRIC_MAR = \"MAR\"\n","METRIC_F1 = \"F1\"\n","\n","# Metric visualization parameters\n","metrics = [\n","    {\"id\": METRIC_ACCURACY, \"name\": \"Accuracy\", \"color\": \"green\"},\n","    {\"id\": METRIC_MAP, \"name\": \"Mean Average Precision\", \"color\": \"blue\"},\n","    {\"id\": METRIC_MAR, \"name\": \"Mean Average Recall\", \"color\": \"red\"},\n","    {\"id\": METRIC_F1, \"name\": \"F1 Score\", \"color\": \"blueviolet\"}\n","]"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Dataset reference initialization\n","dataset_reference = generate_dataset_reference(test_dataset, lambda x, y: y | {'image path': x['image path']})\n","\n","# Compute relevance for all the test queries in the dataset\n","tot_relevant_cap = compute_total_relevance(dataset_reference, reference_preprocess=reference_preprocess_cap, save_to_file=False, fileinfo=relevance_fileinfo_cap | {\"split\": \"test\"})\n","tot_relevant_con = compute_total_relevance(dataset_reference, reference_preprocess=reference_preprocess_con_hash, relevance=concept_relevance_hash, save_to_file=False, fileinfo=relevance_fileinfo_con | {\"split\": \"test\"})\n","\n","# Define model comparison labels\n","model_labels = [{\"id\": model[\"name\"] + f\"({str(id(model['clip']))})\", \"label\": model[\"name\"]} for model in models] "]},{"cell_type":"markdown","metadata":{},"source":["## Text to Image Task"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["results = []\n","for model in models:\n","    print(f\"\\n### Scoring test data for {model['name']} ###\")\n","    test_image_embeddings = generate_image_embeddings(\n","        model[\"image_encoder\"],\n","        test_dataset,\n","        dataset_pred_map=lambda x, y: x['image'],\n","    )\n","    test_queries = test_dataset.map(lambda x, y: y[\"caption\"])\n","    # Compute matching results and extrapolate relevant matches based on different criterions\n","    test_raw_results = find_t2i_matches(test_queries, model[\"text_encoder\"], test_image_embeddings, k=k, normalize=True)\n","    test_results = index_to_reference(test_raw_results, dataset_reference)\n","    test_relevant_cap = compute_relevant_at_k(test_results, dataset_reference, k=k, reference_preprocess=reference_preprocess_cap)\n","    test_relevant_con = compute_relevant_at_k(test_results, dataset_reference, k=k, reference_preprocess=reference_preprocess_con, relevance=concept_relevance)\n","    \n","    results.append({\n","        \"results\": test_results,\n","        \"relevant_cap\": test_relevant_cap,\n","        \"relevant_con\": test_relevant_con,\n","    })"]},{"cell_type":"markdown","metadata":{},"source":["#### Caption equality relevance metric"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for model in results:\n","    _ = retrieval_report(\n","        model[\"results\"], dataset_reference, model[\"relevant_cap\"], tot_relevant_cap,\n","        k=k,\n","        metrics=metrics,\n","        title=f\"Test Data - Caption equality metrics @ k={k}\",\n","        decimal_precision=decimal_precision\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["_ = retrieval_graph_compare(\n","    [model[\"results\"] for model in results], dataset_reference, model_labels, tot_relevant_cap,\n","    k_range=(1, k),\n","    metrics=metrics, \n","    titlexyf=(\"k\", None, \"Test Data - Caption Equality model comparison\"),\n","    reference_preprocess=reference_preprocess_cap\n",")"]},{"cell_type":"markdown","metadata":{},"source":["#### Concept overlap relevance metric"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for model in results:\n","    _ = retrieval_report(\n","        model[\"results\"], dataset_reference, model[\"relevant_con\"], tot_relevant_con,\n","        k=k,\n","        metrics=metrics,\n","        title=f\"Test Data - Concept overlap metrics @ k={k}\",\n","        decimal_precision=decimal_precision\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["_ = retrieval_graph_compare(\n","    [model[\"results\"] for model in results], dataset_reference, model_labels, tot_relevant_con,\n","    k_range=(1, k),\n","    metrics=metrics, \n","    titlexyf=(\"k\", None, \"Test Data - Concept Overlap model comparison\"),\n","    relevance=concept_relevance,\n","    reference_preprocess=reference_preprocess_con\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Image to Text"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["results = []\n","for model in models:\n","    print(f\"\\n### Scoring test data for {model['name']} ###\")\n","    test_text_embeddings = generate_text_embeddings(\n","        model[\"text_encoder\"],\n","        test_dataset,\n","        dataset_pred_map=lambda x, y: y['caption'],\n","    )\n","    test_queries = test_dataset.map(lambda x, y: x[\"image\"])\n","    # Compute matching results and extrapolate relevant matches based on different criterions\n","    test_raw_results = find_i2t_matches(test_queries, model[\"image_encoder\"], test_text_embeddings, k=k, normalize=True)\n","    test_results = index_to_reference(test_raw_results, dataset_reference)\n","    test_relevant_cap = compute_relevant_at_k(test_results, dataset_reference, k=k, reference_preprocess=reference_preprocess_cap)\n","    test_relevant_con = compute_relevant_at_k(test_results, dataset_reference, k=k, reference_preprocess=reference_preprocess_con, relevance=concept_relevance)\n","    \n","    results.append({\n","        \"results\": test_results,\n","        \"relevant_cap\": test_relevant_cap,\n","        \"relevant_con\": test_relevant_con,\n","    })"]},{"cell_type":"markdown","metadata":{},"source":["#### Caption equality relevance metric"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for model in results:\n","    _ = retrieval_report(\n","        model[\"results\"], dataset_reference, model[\"relevant_cap\"], tot_relevant_cap,\n","        k=k,\n","        metrics=metrics,\n","        title=f\"Test Data - Caption equality metrics @ k={k}\",\n","        decimal_precision=decimal_precision\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["_ = retrieval_graph_compare(\n","    [model[\"results\"] for model in results], dataset_reference, model_labels, tot_relevant_cap,\n","    k_range=(1, k),\n","    metrics=metrics, \n","    titlexyf=(\"k\", None, \"Test Data - Caption Equality model comparison\"),\n","    reference_preprocess=reference_preprocess_cap\n",")"]},{"cell_type":"markdown","metadata":{},"source":["#### Concept overlap relevance metric"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for model in results:\n","    _ = retrieval_report(\n","        model[\"results\"], dataset_reference, model[\"relevant_con\"], tot_relevant_con,\n","        k=k,\n","        metrics=metrics,\n","        title=f\"Test Data - Concept overlap metrics @ k={k}\",\n","        decimal_precision=decimal_precision\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["_ = retrieval_graph_compare(\n","    [model[\"results\"] for model in results], dataset_reference, model_labels, tot_relevant_con,\n","    k_range=(1, k),\n","    metrics=metrics, \n","    titlexyf=(\"k\", None, \"Test Data - Concept Overlap model comparison\"),\n","    relevance=concept_relevance,\n","    reference_preprocess=reference_preprocess_con\n",")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}

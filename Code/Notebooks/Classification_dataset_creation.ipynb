{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Declarations"]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-11-20T22:30:29.564034Z","iopub.status.busy":"2023-11-20T22:30:29.563391Z","iopub.status.idle":"2023-11-20T22:30:29.660379Z","shell.execute_reply":"2023-11-20T22:30:29.659370Z","shell.execute_reply.started":"2023-11-20T22:30:29.563990Z"},"trusted":true},"outputs":[],"source":["import re\n","import os\n","import math\n","import string\n","import random\n","import requests\n","import importlib\n","import itertools\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-20T22:30:29.663148Z","iopub.status.busy":"2023-11-20T22:30:29.662780Z","iopub.status.idle":"2023-11-20T22:30:43.345935Z","shell.execute_reply":"2023-11-20T22:30:43.345081Z","shell.execute_reply.started":"2023-11-20T22:30:29.663120Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-11-21 12:45:03.714226: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-11-21 12:45:03.714266: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-11-21 12:45:03.714291: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","\n","import plotly.graph_objects as go\n","\n","from tqdm import tqdm\n","\n","from IPython.display import display\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","tfk = tf.keras\n","tfkl = tf.keras.layers\n","kb = tf.keras.backend"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-20T22:30:43.347810Z","iopub.status.busy":"2023-11-20T22:30:43.347245Z","iopub.status.idle":"2023-11-20T22:30:43.359615Z","shell.execute_reply":"2023-11-20T22:30:43.358665Z","shell.execute_reply.started":"2023-11-20T22:30:43.347781Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2.14.0\n","Num GPUs Available:  1\n"]}],"source":["print(tf.__version__)\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"]},{"cell_type":"markdown","metadata":{},"source":["## Constants"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-20T22:30:43.362302Z","iopub.status.busy":"2023-11-20T22:30:43.362006Z","iopub.status.idle":"2023-11-20T22:30:43.395472Z","shell.execute_reply":"2023-11-20T22:30:43.393815Z","shell.execute_reply.started":"2023-11-20T22:30:43.362276Z"},"trusted":true},"outputs":[],"source":["# Randomness\n","seed = 42\n","\n","random.seed(seed)\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","tf.compat.v1.set_random_seed(seed)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-20T22:30:43.397801Z","iopub.status.busy":"2023-11-20T22:30:43.396879Z","iopub.status.idle":"2023-11-20T22:30:43.406732Z","shell.execute_reply":"2023-11-20T22:30:43.405869Z","shell.execute_reply.started":"2023-11-20T22:30:43.397758Z"},"trusted":true},"outputs":[],"source":["# Filepaths\n","kaggle = False\n","\n","model_versions = [\"v4.0\"]\n","\n","github_repo = \"raul-singh/Rise-of-Transformers-Project\"\n","github_branch = \"main\"\n","github_python_prefix = [\"Code\", \"Notebooks\", \"py_files\"]\n","github_clip_models_prefix = [\"Code\", \"Models\"] if kaggle else [\"..\", \"Models\"]\n","github_pyfiles_data = [\n","    {\"name\": \"preprocessing\", \"imports\": [\"import_datasets\"]},\n","    {\"name\": \"evaluation\", \"imports\": [\n","        \"EvalMetrics as evm\", \"compute_total_relevance\", \"generate_image_embeddings\", \"generate_text_embeddings\", \n","        \"index_to_reference\", \"compute_relevant_at_k\", \"decode_concepts\"\n","    ]}, \n","    {\"name\": \"clip\", \"imports\": [\"build_clip\"]}\n","]\n","github_pyfiles = [\"/\".join(github_python_prefix) + \"/\" + pf[\"name\"] + \".py\" for pf in github_pyfiles_data]\n","github_clip_models = [f\"{'/'.join(github_clip_models_prefix)}/{version}.yaml\" for version in model_versions]\n","\n","kaggle_dataset1 = \"/kaggle/input/transformers-hackathon/\"\n","kaggle_dataset2 = \"/kaggle/input/transformers-hackathon-features/\"\n","kaggle_weights = \"/kaggle/input/clip-weights/\"\n","kaggle_relevance = \"/kaggle/input/clip-relevance/\"\n","\n","image_dir = \"./resized_train\"\n","relevance_dir = \"./relevance\"\n","caption_pred_file = \"caption_prediction_train.csv\"\n","concept_det_file = \"concept_detection_train.csv\"\n","concept_file = \"concepts.csv\"\n","clip_weights_files = [f\"{version}.h5\" for version in model_versions] if kaggle else [None for _ in model_versions]\n","\n","if kaggle:\n","    image_dir = kaggle_dataset1 + image_dir\n","    relevance_dir = kaggle_relevance + relevance_dir\n","    caption_pred_file = kaggle_dataset2 + caption_pred_file\n","    concept_det_file = kaggle_dataset2 + concept_det_file\n","    concept_file = kaggle_dataset2 + concept_file\n","    clip_weights_files = [kaggle_weights + weight for weight in clip_weights_files]"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-11-20T22:30:43.408783Z","iopub.status.busy":"2023-11-20T22:30:43.408152Z","iopub.status.idle":"2023-11-20T22:30:43.419083Z","shell.execute_reply":"2023-11-20T22:30:43.418250Z","shell.execute_reply.started":"2023-11-20T22:30:43.408754Z"},"trusted":true},"outputs":[],"source":["# Train/Val/Test split and filter percentages\n","test_size = 0.2\n","val_size = 0\n","filter_percent_dataset = 1\n","\n","# Batch size\n","batch_size = 32\n","\n","# Import dataset types and shapes\n","in_feat_typ = {'caption': tf.string, 'concepts': tf.bool, 'image path': tf.string}\n","feature_shapes = {'image': (128, 128, 3), 'caption': (), 'concepts': (8374)}\n","\n","# Output dataset structure\n","x_features_eval = ['image path', 'image']\n","y_features_eval = ['caption', 'concepts']\n","\n","# Define parameters for dataset import\n","dataset_parameters = [{\n","    'x_features': x_features_eval, 'y_features': y_features_eval,\n","    'x_dict': True, 'y_dict': True,           \n","    'shuffle_buffer_size': 1,\n","    'batch_size': batch_size,\n","    'cached': True,\n","}]"]},{"cell_type":"markdown","metadata":{},"source":["## Meta-Imports"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-11-20T22:30:43.421398Z","iopub.status.busy":"2023-11-20T22:30:43.420677Z","iopub.status.idle":"2023-11-20T22:30:43.432285Z","shell.execute_reply":"2023-11-20T22:30:43.431541Z","shell.execute_reply.started":"2023-11-20T22:30:43.421361Z"},"trusted":true},"outputs":[],"source":["def clean_recursive_imports(source, import_list, prefix):\n","    import_prefix = re.sub(r\"/\", \".\", prefix)\n","    for target_import in import_list:\n","        source = re.sub(r\"from[ \\t]+\" + re.escape(target_import) + r\"[ \\t]+import\", f\"from {import_prefix + target_import} import\", source)\n","    return source\n","    \n","def import_py_from_repo(repository, branch, filepath, prefix, recursive_imports_list=None):\n","    # Build path for retrieval and write name\n","    path_pre = \"https://raw.githubusercontent.com/\"\n","    path = path_pre + repository + \"/\" + branch + \"/\" + filepath \n","    write_path = prefix + filepath.split(\"/\")[-1]\n","    print(\"Downloading file from \" + path)\n","    # Obtain raw text from file\n","    text = requests.get(path).text\n","    # Clean recursive imports\n","    text = clean_recursive_imports(text, recursive_imports_list, prefix) if recursive_imports_list else text\n","    # Create subdirectories if not exist\n","    os.makedirs(os.path.dirname(write_path), exist_ok=True)\n","    # Write file\n","    f = open(write_path, \"w\")\n","    f.write(text)\n","    f.close()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-11-20T22:30:43.434197Z","iopub.status.busy":"2023-11-20T22:30:43.433731Z","iopub.status.idle":"2023-11-20T22:30:45.698715Z","shell.execute_reply":"2023-11-20T22:30:45.697742Z","shell.execute_reply.started":"2023-11-20T22:30:43.434170Z"},"trusted":true},"outputs":[],"source":["if kaggle:\n","    for pf_data, py_file in zip(github_pyfiles_data, github_pyfiles):\n","        import_py_from_repo(\n","            github_repo, github_branch, py_file, \n","            \"/\".join(github_python_prefix) + \"/\", \n","            recursive_imports_list=[pf[\"name\"] for pf in github_pyfiles_data],\n","        )\n","        import_string = f'from {\".\".join(github_python_prefix) + \".\" + pf_data[\"name\"]} import {\", \".join(pf_data[\"imports\"])}'\n","        exec(import_string)\n","    \n","    for model in github_clip_models:\n","        import_py_from_repo(github_repo, github_branch, model, \"/\".join(github_clip_models_prefix) + \"/\")\n","        \n","else:\n","    for pf_data in github_pyfiles_data:\n","        import_string = f'from py_files.{pf_data[\"name\"]} import {\", \".join(pf_data[\"imports\"])}'\n","        exec(import_string)"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset Creation"]},{"cell_type":"markdown","metadata":{},"source":["### Importing the Complete Test Dataset"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-11-20T22:30:45.700358Z","iopub.status.busy":"2023-11-20T22:30:45.700034Z","iopub.status.idle":"2023-11-20T22:32:17.943649Z","shell.execute_reply":"2023-11-20T22:32:17.942567Z","shell.execute_reply.started":"2023-11-20T22:30:45.700314Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-11-21 12:45:09.380960: E ./tensorflow/compiler/xla/stream_executor/stream_executor_internal.h:124] SetPriority unimplemented for this stream.\n"]},{"name":"stdout","output_type":"stream","text":["Extracting features from CSV file(s)\n"]},{"name":"stderr","output_type":"stream","text":["83275it [00:47, 1742.64it/s]\n"]}],"source":["concept_info, datasets, dataset_sizes = import_datasets(\n","    image_dir, caption_pred_file, concept_file, concept_det_file,\n","    in_feat_typ, feature_shapes,\n","    dataset_parameters,\n","    filter_percent_dataset,\n","    test_size, val_size,\n","    seed,\n",")"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-11-20T22:32:17.947688Z","iopub.status.busy":"2023-11-20T22:32:17.947346Z","iopub.status.idle":"2023-11-20T22:32:17.953141Z","shell.execute_reply":"2023-11-20T22:32:17.951991Z","shell.execute_reply.started":"2023-11-20T22:32:17.947659Z"},"trusted":true},"outputs":[],"source":["# Select loaded datasets and variables\n","concept_list, concepts_onehot = concept_info\n","_, _, test_dataset = datasets[0]\n","train_ds_size, val_ds_size, test_ds_size = dataset_sizes\n","\n","del datasets"]},{"cell_type":"markdown","metadata":{},"source":["### Choosing the Labels"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["labels = ['angiography', 'echocardiography', 'ultrasound', 'tomography', 'xray']"]},{"cell_type":"markdown","metadata":{},"source":["### Creating the New Dataset"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["dataset = test_dataset.unbatch()\n","dataset = dataset.map(lambda x,y: (x['image path'], y['caption']))\n","actual_dataset = [x for x in dataset]\n","\n","for i in range(len(actual_dataset)):\n","    actual_dataset[i] = (actual_dataset[i][0].numpy().decode('UTF-8'), actual_dataset[i][1].numpy().decode('UTF-8').split(' '), '')\n","\n","for i in range(len(actual_dataset)):\n","    for l in labels:\n","        if l in actual_dataset[i][1]:\n","            actual_dataset[i] = (actual_dataset[i][0], actual_dataset[i][1], l)\n","\n","filtered_dataset = []\n","\n","for d in actual_dataset:\n","    if d[2] != '':\n","        filtered_dataset.append(d)\n","\n","for i in range(len(filtered_dataset)):\n","    filtered_dataset[i] = (filtered_dataset[i][0], filtered_dataset[i][2])"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Original dataset size: 16655\n","New dataset size: 4111\n"]}],"source":["print(f'Original dataset size: {len(actual_dataset)}')\n","print(f'New dataset size: {len(filtered_dataset)}')"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'angiography': 299, 'echocardiography': 211, 'ultrasound': 520, 'tomography': 1873, 'xray': 1208}\n"]}],"source":["class_distribution = { l:0 for l in labels }\n","\n","for d in filtered_dataset:\n","    class_distribution[d[1]] += 1\n","\n","print(class_distribution)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.DataFrame(filtered_dataset)\n","df = df.rename(columns={0: \"image path\", 1: \"label\"})\n","\n","# Save dataset\n","df.to_csv('./classification.csv', index=False)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":3771795,"sourceId":6658619,"sourceType":"datasetVersion"},{"datasetId":3358934,"sourceId":5842422,"sourceType":"datasetVersion"},{"datasetId":3358628,"sourceId":5841976,"sourceType":"datasetVersion"}],"dockerImageVersionId":30587,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":4}

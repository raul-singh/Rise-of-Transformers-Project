{"cells":[{"cell_type":"markdown","metadata":{"id":"xffJLUZCLC51"},"source":["## Declarations"]},{"cell_type":"markdown","metadata":{"id":"7TgHFX6XLQGl"},"source":["### Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4674,"status":"ok","timestamp":1685806019663,"user":{"displayName":"Ri ga","userId":"15539171450533002544"},"user_tz":-120},"id":"2d093180","outputId":"bc84c9fd-b81e-49b9-d8a7-0c7e03c595e0","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2.13.0\n","Num GPUs Available:  1\n"]}],"source":["import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","\n","import math\n","import string\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import numpy as np\n","import random\n","from IPython.display import display\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from tqdm import tqdm\n","\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","\n","tfk = tf.keras\n","tfkl = tf.keras.layers\n","kb = tf.keras.backend\n","print(tf.__version__)\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"]},{"cell_type":"markdown","metadata":{"id":"-icpKfuyLSrH"},"source":["### Constants"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1685806019664,"user":{"displayName":"Ri ga","userId":"15539171450533002544"},"user_tz":-120},"id":"f3cf13ac","trusted":true},"outputs":[],"source":["# Random seed for reproducibility\n","seed = 42\n","\n","random.seed(seed)\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","tf.compat.v1.set_random_seed(seed)\n","\n","# Turn ON for kaggle filepaths\n","kaggle = False\n","\n","kaggle1 = \"/kaggle/input/transformers-hackathon/\"\n","kaggle2 = \"/kaggle/input/transformers-hackathon-features/\"\n","\n","image_dir = \"./resized_train\"\n","caption_pred_file = \"caption_prediction_train.csv\"\n","concept_det_file = \"concept_detection_train.csv\"\n","concept_file = \"concepts.csv\"\n","\n","if kaggle:\n","    image_dir = kaggle1 + image_dir\n","    caption_pred_file = kaggle2 + caption_pred_file\n","    concept_det_file = kaggle2 + concept_det_file\n","    concept_file = kaggle2 + concept_file\n","\n","image_size = (128, 128, 3)\n","\n","batch_size = 10\n","pretrain_batch_size = 32\n","\n","pretrain_epochs = 5\n","epochs_phase1 = 50\n","epochs_phase2 = 50"]},{"cell_type":"markdown","metadata":{"id":"n1w8gAuILhCl"},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[],"source":["feature_types = {'image': tf.float16, 'caption': tf.string, 'concepts': tf.bool, 'raw caption': tf.string, 'image path': tf.string}\n","feature_shapes = {'image': (128, 128, 3), 'caption': (), 'concepts': (8374)}\n","base_features = [\"image\", \"caption\"]"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["concepts = pd.read_csv(concept_file, sep='\\t')\n","concept_list = concepts.set_index('concept')['concept_name'].to_dict()\n","# Concept one-hot encoder\n","concepts_onehot = MultiLabelBinarizer(classes = list(concept_list.keys()))\n","concepts_onehot.fit([list(concept_list.keys())])\n","\n","captions = pd.read_csv(caption_pred_file, sep='\\t')\n","captions = captions.set_index('ID')['caption'].to_dict()\n","\n","concepts = pd.read_csv(concept_det_file, sep='\\t')\n","concepts = concepts.set_index('ID')['cuis'].to_dict()\n","concepts = {id: item_concepts.split(\";\") for id, item_concepts in concepts.items()}"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[],"source":["def split(x, test_size=0.2, val_size=0.0, seed=0):\n","    if val_size + test_size >= 1:\n","        return None\n","    x_train, x_test = train_test_split(\n","        x, test_size=test_size + val_size, random_state=seed\n","    )\n","    x_val = None\n","    if val_size > 0:\n","        x_test, x_val = train_test_split(\n","            x_test,\n","            test_size=val_size / (test_size + val_size),\n","            random_state=seed,\n","        )\n","    return x_train, x_val, x_test\n","\n","def load_image_from_path(path):\n","    image = tf.io.read_file(path)\n","    image = tf.io.decode_jpeg(image, channels=3, dct_method=\"INTEGER_ACCURATE\")\n","\n","    # may need resizing\n","    #image = tf.image.resize(image, image_shape[:2])\n","    image = tf.cast(image, dtype=tf.float16)\n","    image = image / 255.0\n","    return image"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocab size:\n","35489\n","Longest sequence:\n","391\n","Max number of concepts:\n","50\n"]}],"source":["result = \"\"\n","for i in captions.values():\n","    result += \" \" + i\n","result = tf.strings.as_string(result)\n","result = bytes.decode(result.numpy())\n","vocab_size = len(set(result.split()))\n","print(\"Vocab size:\")\n","print(vocab_size)\n","\n","longest = max(captions.values(), key=len)\n","longest = tf.strings.as_string(longest)\n","longest = bytes.decode(longest.numpy())\n","longest = longest.split()\n","sequence_length = len(longest)\n","print(\"Longest sequence:\")\n","print(sequence_length)\n","\n","concept_size = max([len(c) for _, c in concepts.items()])\n","print(\"Max number of concepts:\")\n","print(concept_size)"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["def load_features(image_folder, captions_file, concepts_file, concept_encoder, filter_percent=1):\n","    features = []\n","    \n","    # Import CSVs\n","    csv_caption_dataset = tf.data.experimental.CsvDataset(\n","        captions_file,\n","        field_delim='\\t',\n","        record_defaults=[tf.string, tf.string],\n","        header=True,\n","        select_cols=[0, 1]\n","    )\n","    csv_concept_dataset = tf.data.experimental.CsvDataset(\n","        concepts_file,\n","        field_delim='\\t',\n","        record_defaults=[tf.string, tf.string],\n","        header=True,\n","        select_cols=[0, 1]\n","    )\n","    \n","    # We make the assumption that CSV files contain the same key values (image names)\n","    # following the same ordering\n","\n","    # Extract features from dataset\n","    print(\"Extracting features from CSV file(s)\")\n","    for caption_el, concept_el in tqdm(zip(csv_caption_dataset, csv_concept_dataset)):\n","        filename_cap, caption = caption_el\n","        filename_con , concepts = concept_el\n","        \n","        # Sanity check\n","        assert filename_cap == filename_con\n","        \n","        image_path = image_dir + \"/\" + filename_cap + \".jpg\"\n","        \n","        features.append({\n","            'caption': caption,\n","            'image path': image_path,\n","            'concepts': concept_encoder.transform([concepts.numpy().decode(\"utf-8\").split(\";\")]),\n","        })\n","        \n","    # Filter elements\n","    if filter_percent != 1:\n","        n_features = int(len(features) * filter_percent)\n","        features = random.sample(features, n_features)\n","        \n","    return features\n","\n","def preprocess_features(features, concept_encoder, filter_percent=1):\n","    print(\"Preprocessing features\")\n","    \n","    # Filter elements\n","    if filter_percent != 1:\n","        n_features = int(len(features) * filter_percent)\n","        features = random.sample(features, n_features)\n","        \n","    return {\n","        'image paths': tf.convert_to_tensor([x[\"image path\"] for x in tqdm(features)], dtype=tf.string),\n","        'captions': tf.convert_to_tensor([x[\"caption\"] for x in tqdm(features)], dtype=tf.string),\n","        'concepts': tf.convert_to_tensor(np.vstack([concept_encoder.transform(x[\"concepts\"]).flatten() for x in tqdm(features)]), dtype=tf.bool),\n","        # 'images': tf.convert_to_tensor([load_image(x[\"image path\"]) for x in tqdm(features)], dtype=tf.float16),\n","    }"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Extracting features from CSV file(s)\n"]},{"name":"stderr","output_type":"stream","text":["83275it [00:42, 1950.24it/s]\n"]}],"source":["# Load dataset features from csv files, split them and preprocess them\n","features = load_features(image_dir, caption_pred_file, concept_det_file, concepts_onehot, filter_percent=1)\n","feat_train, feat_val, feat_test = split(features, test_size=0.2, val_size=0.0, seed=seed)"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["def create_dataset(\n","        features, \n","        input_features_types,\n","        feature_shapes,\n","        x_features, y_features=None, \n","        x_dict=True, y_dict=True,\n","        load_images=True, \n","        shuffle_buffer_size=1024, \n","        batch_size=10, \n","        cached=False\n","):\n","    # Generate dataset following initial input feature types\n","    dataset = tf.data.Dataset.from_generator(\n","        lambda: features, { x: input_features_types[x] for x in input_features_types }\n","    )\n","    \n","    # Preprocessing internal functions\n","    def setshape(e):\n","        for (k, v) in feature_shapes.items():\n","            if k in e:\n","                e[k].set_shape(v)\n","        return e\n","    def add_images(e):\n","        # Maybe parametrize\n","        img_from = \"image path\"\n","        img_to = \"image\"\n","        new_features = list(input_features_types.keys()) + [img_to]\n","        return {f:e[f] if f != img_to else load_image_from_path(e[img_from]) for f in new_features}\n","    def split_xy(e):\n","        e_x = {xf:tf.squeeze(e[xf]) for xf in x_features} if x_dict else tf.squeeze([e[xf] for xf in x_features])\n","        if y_features:\n","            e_y = {yf:tf.squeeze(e[yf]) for yf in y_features} if y_dict else tf.squeeze([e[yf] for yf in y_features])\n","            return (e_x, e_y)\n","        return e_x\n","    \n","    # Preprocess\n","    if load_images:\n","        dataset = dataset.map(add_images)\n","    dataset = dataset.map(setshape)\n","    dataset = dataset.map(split_xy)\n","\n","    # Compile dataset\n","    if cached:\n","        dataset = dataset.cache()\n","    dataset = dataset.shuffle(shuffle_buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","    return dataset\n","\n","def visualize_first_of_dataset_batch(dataset_batch, nums=5):\n","    for c in range(0, nums):\n","        i = tf.cast(dataset_batch[\"image\"][c], dtype=tf.float32)\n","        t = dataset_batch[\"raw caption\"][c]\n","        plt.figure(figsize=(50, 100))\n","        plt.subplot(nums, 1, c + 1)\n","        plt.imshow(i)\n","        plt.title(f\"{t}\", fontsize=100)\n","        plt.xticks([])\n","        plt.yticks([])\n","def visualize_first_of_dataset_batch(dataset_batch, nums=5):\n","    for c in range(0, nums):\n","        i = tf.cast(dataset_batch[\"image\"][c], dtype=tf.float32)\n","        t = dataset_batch[\"raw caption\"][c]\n","        plt.figure(figsize=(50, 100))\n","        plt.subplot(nums, 1, c + 1)\n","        plt.imshow(i)\n","        plt.title(f\"{t}\", fontsize=100)\n","        plt.xticks([])\n","        plt.yticks([])"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[],"source":["in_feat_typ = {'caption': tf.string, 'concepts': tf.bool, 'image path': tf.string}\n","x_features = ['caption', 'image']\n","x_features_iep = ['image']\n","y_features_iep = ['concepts']\n","\n","train_ds_size = len(feat_train) if feat_train else 0\n","val_ds_size = len(feat_val) if feat_val else 0\n","test_ds_size = len(feat_test) if feat_test else 0\n","\n","train_dataset = create_dataset(feat_train, input_features_types=in_feat_typ, feature_shapes=feature_shapes, x_features=x_features, batch_size=batch_size) if feat_train else None\n","val_dataset = create_dataset(feat_val, input_features_types=in_feat_typ, feature_shapes=feature_shapes, x_features=x_features, batch_size=batch_size) if feat_val else None\n","test_dataset = create_dataset(feat_test, input_features_types=in_feat_typ, feature_shapes=feature_shapes, x_features=x_features, batch_size=batch_size) if feat_test else None\n","\n","train_dataset_iep = create_dataset(feat_train, input_features_types=in_feat_typ, feature_shapes=feature_shapes, x_features=x_features_iep, y_features=y_features_iep, x_dict=False, y_dict=False, batch_size=pretrain_batch_size, cached=True) if feat_train else None\n","val_dataset_iep = create_dataset(feat_val, input_features_types=in_feat_typ, feature_shapes=feature_shapes, x_features=x_features_iep, y_features=y_features_iep, x_dict=False, y_dict=False, batch_size=pretrain_batch_size, cached=True) if feat_val else None\n","test_dataset_iep = create_dataset(feat_test, input_features_types=in_feat_typ, feature_shapes=feature_shapes, x_features=x_features_iep, y_features=y_features_iep, x_dict=False, y_dict=False, batch_size=pretrain_batch_size, cached=True) if feat_test else None\n","\n","train_dataset_eval = create_dataset(feat_train, input_features_types=in_feat_typ, feature_shapes=feature_shapes, x_features=['image path', 'image'], y_features=['caption', 'concepts'], x_dict=True, y_dict=True, batch_size=1, shuffle_buffer_size=1)\n","test_dataset_eval = create_dataset(feat_test, input_features_types=in_feat_typ, feature_shapes=feature_shapes, x_features=['image path', 'image'], y_features=['caption', 'concepts'], x_dict=True, y_dict=True, batch_size=1, shuffle_buffer_size=1)"]},{"cell_type":"markdown","metadata":{},"source":["## Download Models"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[],"source":["text_preprocess = hub.KerasLayer(\n","        \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n","        name=\"text_preprocessing\",\n","    )\n","\n","text_transformer = hub.KerasLayer(\n","        \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\",\n","        trainable=True,\n","        name=\"bert\",\n","    )\n","\n","img_preprocess = tfk.applications.convnext.preprocess_input\n","\n","img_supernet = tfk.applications.ConvNeXtTiny(weights='imagenet', include_top=False)\n","supernet_name = img_supernet.name"]},{"cell_type":"markdown","metadata":{},"source":["## Pre-training"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[],"source":["def image_encoder_pretrainer(preprocessing, supernet, n_concepts, input_shape=(128,128,3), learning_rate=1e-5):\n","    \n","    input_layer = tfkl.Input(shape=input_shape, name='image')\n","\n","    x = preprocessing(input_layer)\n","    x = supernet(x)\n","    \n","    x = tfkl.GlobalMaxPooling2D(name='GAP')(x)\n","    x = tfkl.Dense(256, activation='relu')(x)\n","    x = tfkl.Dense(128, activation='relu')(x)\n","    x = tfkl.Dense(n_concepts, activation=\"sigmoid\", name='output')(x)\n","\n","    image_encoder_pretrainer = tfk.Model(inputs=input_layer, outputs=x, name=\"image_encoder_pretrainer\")\n","    image_encoder_pretrainer.compile(\n","        loss=\"binary_crossentropy\", optimizer=tf.optimizers.Adam(learning_rate=learning_rate),\n","    )\n","    \n","    return image_encoder_pretrainer"]},{"cell_type":"code","execution_count":21,"metadata":{"trusted":true},"outputs":[],"source":["iep = image_encoder_pretrainer(img_preprocess, img_supernet, len(concept_list.keys()))"]},{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n"]},{"name":"stdout","output_type":"stream","text":["2082/2082 [==============================] - 223s 96ms/step - loss: 0.0619 - val_loss: 0.0036\n","Epoch 2/5\n","2082/2082 [==============================] - 199s 96ms/step - loss: 0.0035 - val_loss: 0.0035\n","Epoch 3/5\n","2082/2082 [==============================] - 198s 95ms/step - loss: 0.0035 - val_loss: 0.0035\n","Epoch 4/5\n","2082/2082 [==============================] - 199s 95ms/step - loss: 0.0035 - val_loss: 0.0035\n","Epoch 5/5\n","2082/2082 [==============================] - 199s 95ms/step - loss: 0.0034 - val_loss: 0.0034\n"]}],"source":["# Create an early stopping callback.\n","early_stopping = tf.keras.callbacks.EarlyStopping(\n","    monitor = \"val_loss\", patience = 5, restore_best_weights = True\n",")\n","\n","history = iep.fit(\n","    train_dataset_iep,\n","    epochs = pretrain_epochs,\n","    validation_data = test_dataset_iep,\n","    callbacks = [early_stopping],\n",")"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["img_supernet = iep.layers[1]"]},{"cell_type":"markdown","metadata":{"id":"oiUz4hxwNRVS"},"source":["## Network"]},{"cell_type":"markdown","metadata":{"id":"WhVp1-YONWpY"},"source":["### Network blocks"]},{"cell_type":"code","execution_count":24,"metadata":{"trusted":true},"outputs":[],"source":["def projection(embedding_input, embed_dim, name):\n","    \n","    embeddings = tfkl.Dense(embed_dim, name=f'{name}_1')(embedding_input)\n","    x = tf.nn.selu(embeddings)\n","    x = tfkl.Dense(embed_dim, name=f'{name}_2')(x)\n","    x = tfkl.Dropout(0.1)(x)\n","    x = tfkl.Add()([x, embeddings])\n","    embeddings = tfkl.LayerNormalization()(x)\n","\n","    return embeddings"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":441,"status":"ok","timestamp":1685807496567,"user":{"displayName":"Ri ga","userId":"15539171450533002544"},"user_tz":-120},"id":"2bc14deb","trusted":true},"outputs":[],"source":["def image_encoder(input_shape, embed_dim, seed=42, supernet=None, preprocessing=None):\n","    \n","    tf.random.set_seed(seed)\n","\n","    input_layer = tfkl.Input(shape=input_shape, name='img_input_layer')\n","    x = preprocessing(input_layer)\n","    x = supernet(x)\n","    x = tfkl.GlobalAveragePooling2D(name='GAP')(x)\n","\n","    x = projection(x, embed_dim, 'img_embedding_dense_layer')\n","    \n","    # Connect input and output through the Model class\n","    cnn_encoder = tfk.Model(inputs=input_layer, outputs=x, name='image_encoder')\n","\n","    # Return the encoder\n","    return cnn_encoder"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1685807500205,"user":{"displayName":"Ri ga","userId":"15539171450533002544"},"user_tz":-120},"id":"50bc32b1","trusted":true},"outputs":[],"source":["def text_encoder(embed_dim, preprocess, transformer, trainable=True):\n","\n","    transformer.trainable = trainable\n","    \n","    input_layer = tfkl.Input(shape=(), dtype=tf.string, name=\"text_input\")\n","    x = preprocess(input_layer)\n","    x = transformer(x)[\"pooled_output\"]\n","    x = projection(x, embed_dim, 'txt_embedding_dense_layer')\n","\n","    text_encoder = tfk.Model(inputs=input_layer, outputs=x, name=\"text_encoder\")\n","    \n","    return text_encoder"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1685807500648,"user":{"displayName":"Ri ga","userId":"15539171450533002544"},"user_tz":-120},"id":"tQfOhkjPjz70","trusted":true},"outputs":[],"source":["class CLIP(tfk.Model):\n","    def __init__(self, image_encoder, text_encoder, **kwargs):\n","        super().__init__(**kwargs)\n","        self.image_encoder = image_encoder\n","        self.text_encoder = text_encoder\n","        self.loss_tracker = tfk.metrics.Mean(name=\"loss\")\n","        self.temp = self.add_weight(name='t',\n","                                 shape=(1, ),\n","                                 initializer=tfk.initializers.Constant(1.),\n","                                 trainable=True)\n","\n","        self.call_model()\n","\n","        \n","    @property\n","    def metrics(self):\n","        return [self.loss_tracker]\n","\n","    def call(self, features, training=False):\n","        image_emb = self.image_encoder(features[\"image\"], training=training)\n","        text_emb = self.text_encoder(features[\"caption\"], training=training)\n","        return image_emb, text_emb\n","\n","    def CLIP_loss(self, image_emb, text_emb):\n","        norm_image_emb = tf.math.l2_normalize(image_emb, axis=1)\n","        norm_text_emb = tf.math.l2_normalize(text_emb, axis=1)\n","\n","        logits = tf.linalg.matmul(norm_image_emb, norm_text_emb, transpose_b=True) * tf.math.exp(self.temp)\n","\n","        n = tf.shape(logits)[0]\n","        labels = tf.range(n)\n","\n","        labels = tf.one_hot(labels, n)\n","\n","        loss_img = tfk.losses.categorical_crossentropy(labels, logits, from_logits=True)\n","        loss_txt = tfk.losses.categorical_crossentropy(labels, kb.transpose(logits), from_logits=True)\n","\n","        return (loss_img + loss_txt) / tf.constant(2.0)\n","\n","    def train_step(self, features):\n","        with tf.GradientTape() as tape:\n","            image_embeddings, caption_embeddings = self(features, training=True)\n","            loss = self.CLIP_loss(caption_embeddings, image_embeddings)\n","\n","        gradients = tape.gradient(loss, self.trainable_variables)\n","        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n","\n","        self.loss_tracker.update_state(loss)\n","        return {\"loss\": self.loss_tracker.result()}\n","\n","    def test_step(self, features):\n","        image_embeddings, caption_embeddings = self(features, training=False)\n","        loss = self.CLIP_loss(caption_embeddings, image_embeddings)\n","        self.loss_tracker.update_state(loss)\n","        return {\"loss\": self.loss_tracker.result()}\n","\n","    def call_model(self):\n","\n","        image = tf.reshape(tf.convert_to_tensor(np.zeros((128,128,3))), (1,128,128,3))\n","        caption = tf.convert_to_tensor([\"Hello there\"], dtype=tf.string)\n","\n","        sample = {\"image\": image, \"caption\": caption}\n","\n","        self(sample)\n","\n","    def summary(self):\n","        super().summary()\n","\n","        print(\"\\n\")\n","        self.image_encoder.summary()\n","\n","        print(\"\\n\")\n","        self.text_encoder.summary()"]},{"cell_type":"markdown","metadata":{"id":"FWPNhGjzNbnP"},"source":["### Building network"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1685807502773,"user":{"displayName":"Ri ga","userId":"15539171450533002544"},"user_tz":-120},"id":"mra2VO7JoqGj","trusted":true},"outputs":[],"source":["def build_clip(img_supernet,\n","               img_preprocess,\n","               text_transformer,\n","               text_preprocess,\n","               img_input_shape=(128,128,3),\n","               txt_input_shape=(393, ), \n","               embed_dim=64, \n","               learning_rate=2e-5):\n","\n","    \n","    text_encoder_model = text_encoder(embed_dim, text_preprocess, text_transformer)\n","    image_encoder_model = image_encoder(img_input_shape, embed_dim, supernet=img_supernet, preprocessing=img_preprocess)\n","\n","    clip = CLIP(image_encoder_model, text_encoder_model)\n","    clip.compile(optimizer = tf.optimizers.AdamW(learning_rate=learning_rate))\n","\n","    return image_encoder_model, text_encoder_model, clip"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":564,"status":"ok","timestamp":1685807504408,"user":{"displayName":"Ri ga","userId":"15539171450533002544"},"user_tz":-120},"id":"f0778191","trusted":true},"outputs":[],"source":["clip_image_encoder, clip_text_encoder, clip = build_clip(img_supernet, img_preprocess, text_transformer, text_preprocess)"]},{"cell_type":"code","execution_count":34,"id":"8f8fc278","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"clip_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," image_encoder (Functional)  (None, 64)                27873632  \n","                                                                 \n"," text_encoder (Functional)   (None, 64)                28800769  \n","                                                                 \n","=================================================================\n","Total params: 56674404 (216.20 MB)\n","Trainable params: 56674401 (216.20 MB)\n","Non-trainable params: 3 (9.00 Byte)\n","_________________________________________________________________\n","\n","\n","Model: \"image_encoder\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," img_input_layer (InputLaye  [(None, 128, 128, 3)]        0         []                            \n"," r)                                                                                               \n","                                                                                                  \n"," convnext_tiny (Functional)  (None, None, None, 768)      2782012   ['img_input_layer[0][0]']     \n","                                                          8                                       \n","                                                                                                  \n"," GAP (GlobalAveragePooling2  (None, 768)                  0         ['convnext_tiny[3][0]']       \n"," D)                                                                                               \n","                                                                                                  \n"," img_embedding_dense_layer_  (None, 64)                   49216     ['GAP[0][0]']                 \n"," 1 (Dense)                                                                                        \n","                                                                                                  \n"," tf.nn.selu_3 (TFOpLambda)   (None, 64)                   0         ['img_embedding_dense_layer_1[\n","                                                                    0][0]']                       \n","                                                                                                  \n"," img_embedding_dense_layer_  (None, 64)                   4160      ['tf.nn.selu_3[0][0]']        \n"," 2 (Dense)                                                                                        \n","                                                                                                  \n"," dropout_3 (Dropout)         (None, 64)                   0         ['img_embedding_dense_layer_2[\n","                                                                    0][0]']                       \n","                                                                                                  \n"," add_3 (Add)                 (None, 64)                   0         ['dropout_3[0][0]',           \n","                                                                     'img_embedding_dense_layer_1[\n","                                                                    0][0]']                       \n","                                                                                                  \n"," layer_normalization_4 (Lay  (None, 64)                   128       ['add_3[0][0]']               \n"," erNormalization)                                                                                 \n","                                                                                                  \n","==================================================================================================\n","Total params: 27873632 (106.33 MB)\n","Trainable params: 27873632 (106.33 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n","\n","\n","Model: \"text_encoder\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," text_input (InputLayer)     [(None,)]                    0         []                            \n","                                                                                                  \n"," text_preprocessing (KerasL  {'input_type_ids': (None,    0         ['text_input[0][0]']          \n"," ayer)                       128),                                                                \n","                              'input_mask': (None, 128)                                           \n","                             , 'input_word_ids': (None,                                           \n","                              128)}                                                               \n","                                                                                                  \n"," bert (KerasLayer)           {'pooled_output': (None, 5   2876364   ['text_preprocessing[1][0]',  \n","                             12),                         9          'text_preprocessing[1][1]',  \n","                              'sequence_output': (None,              'text_preprocessing[1][2]']  \n","                              128, 512),                                                          \n","                              'encoder_outputs': [(None                                           \n","                             , 128, 512),                                                         \n","                              (None, 128, 512),                                                   \n","                              (None, 128, 512),                                                   \n","                              (None, 128, 512)],                                                  \n","                              'default': (None, 512)}                                             \n","                                                                                                  \n"," txt_embedding_dense_layer_  (None, 64)                   32832     ['bert[1][5]']                \n"," 1 (Dense)                                                                                        \n","                                                                                                  \n"," tf.nn.selu_2 (TFOpLambda)   (None, 64)                   0         ['txt_embedding_dense_layer_1[\n","                                                                    0][0]']                       \n","                                                                                                  \n"," txt_embedding_dense_layer_  (None, 64)                   4160      ['tf.nn.selu_2[0][0]']        \n"," 2 (Dense)                                                                                        \n","                                                                                                  \n"," dropout_2 (Dropout)         (None, 64)                   0         ['txt_embedding_dense_layer_2[\n","                                                                    0][0]']                       \n","                                                                                                  \n"," add_2 (Add)                 (None, 64)                   0         ['dropout_2[0][0]',           \n","                                                                     'txt_embedding_dense_layer_1[\n","                                                                    0][0]']                       \n","                                                                                                  \n"," layer_normalization_3 (Lay  (None, 64)                   128       ['add_2[0][0]']               \n"," erNormalization)                                                                                 \n","                                                                                                  \n","==================================================================================================\n","Total params: 28800769 (109.87 MB)\n","Trainable params: 28800768 (109.87 MB)\n","Non-trainable params: 1 (1.00 Byte)\n","__________________________________________________________________________________________________\n"]}],"source":["clip.summary()"]},{"cell_type":"markdown","metadata":{"id":"mPnccec1Nfwh"},"source":["## Training"]},{"cell_type":"markdown","metadata":{"id":"eqrFQi0ZNhvo"},"source":["### Phase 1\n","Traning all the parameters"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"executionInfo":{"elapsed":1920448,"status":"error","timestamp":1685809433717,"user":{"displayName":"Ri ga","userId":"15539171450533002544"},"user_tz":-120},"id":"jS2cVFlVrHLs","outputId":"fc619277-db77-4c8a-f742-c0a373c094b7","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n"]},{"name":"stdout","output_type":"stream","text":["6662/6662 [==============================] - 523s 74ms/step - loss: 1.5675 - val_loss: 1.2509 - lr: 2.0000e-05\n","Epoch 2/50\n","6662/6662 [==============================] - 470s 70ms/step - loss: 1.0992 - val_loss: 1.0255 - lr: 2.0000e-05\n","Epoch 3/50\n","6662/6662 [==============================] - 469s 70ms/step - loss: 0.8971 - val_loss: 0.9199 - lr: 2.0000e-05\n","Epoch 4/50\n","6662/6662 [==============================] - 461s 69ms/step - loss: 0.7505 - val_loss: 0.8137 - lr: 2.0000e-05\n","Epoch 5/50\n","6662/6662 [==============================] - 465s 70ms/step - loss: 0.6338 - val_loss: 0.7729 - lr: 2.0000e-05\n","Epoch 6/50\n","6662/6662 [==============================] - 465s 70ms/step - loss: 0.5317 - val_loss: 0.7363 - lr: 2.0000e-05\n","Epoch 7/50\n","6662/6662 [==============================] - 462s 69ms/step - loss: 0.4563 - val_loss: 0.7276 - lr: 2.0000e-05\n","Epoch 8/50\n","6662/6662 [==============================] - 463s 69ms/step - loss: 0.3948 - val_loss: 0.7402 - lr: 2.0000e-05\n","Epoch 9/50\n","6662/6662 [==============================] - 460s 69ms/step - loss: 0.3477 - val_loss: 0.7988 - lr: 2.0000e-05\n","Epoch 10/50\n","6662/6662 [==============================] - 461s 69ms/step - loss: 0.3083 - val_loss: 0.7516 - lr: 2.0000e-05\n","Epoch 11/50\n","6662/6662 [==============================] - 460s 69ms/step - loss: 0.2518 - val_loss: 0.7472 - lr: 4.0000e-06\n","Epoch 12/50\n","6662/6662 [==============================] - 458s 69ms/step - loss: 0.2281 - val_loss: 0.7611 - lr: 4.0000e-06\n"]}],"source":["# Create a learning rate scheduler callback.\n","reduce_lr = tfk.callbacks.ReduceLROnPlateau(\n","    monitor = \"val_loss\", factor = 0.2, patience = 3\n",")\n","\n","# Create an early stopping callback.\n","early_stopping = tf.keras.callbacks.EarlyStopping(\n","    monitor = \"val_loss\", patience = 5, restore_best_weights = True\n",")\n","\n","history_phase1 = clip.fit(\n","    train_dataset,\n","    epochs = epochs_phase1,\n","    validation_data = test_dataset,\n","    callbacks = [reduce_lr, early_stopping],\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Phase 2\n","Training the projection only"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["img_supernet.trainable = False\n","text_transformer.trainable = False"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["clip.compile(optimizer = tf.optimizers.AdamW(2e-5))"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","6662/6662 [==============================] - 190s 27ms/step - loss: 0.3991 - val_loss: 0.7054 - lr: 2.0000e-05\n","Epoch 2/50\n","6662/6662 [==============================] - 182s 27ms/step - loss: 0.3723 - val_loss: 0.7123 - lr: 2.0000e-05\n","Epoch 3/50\n","6662/6662 [==============================] - 181s 27ms/step - loss: 0.3615 - val_loss: 0.7243 - lr: 2.0000e-05\n","Epoch 4/50\n","6662/6662 [==============================] - 181s 27ms/step - loss: 0.3513 - val_loss: 0.7331 - lr: 2.0000e-05\n","Epoch 5/50\n","6662/6662 [==============================] - 181s 27ms/step - loss: 0.3473 - val_loss: 0.7204 - lr: 4.0000e-06\n","Epoch 6/50\n","6662/6662 [==============================] - 181s 27ms/step - loss: 0.3438 - val_loss: 0.7255 - lr: 4.0000e-06\n"]}],"source":["# Create a learning rate scheduler callback.\n","reduce_lr = tfk.callbacks.ReduceLROnPlateau(\n","    monitor = \"val_loss\", factor = 0.2, patience = 3\n",")\n","\n","# Create an early stopping callback.\n","early_stopping = tf.keras.callbacks.EarlyStopping(\n","    monitor = \"val_loss\", patience = 5, restore_best_weights = True\n",")\n","\n","history_phase2 = clip.fit(\n","    train_dataset,\n","    epochs = 50,\n","    validation_data = test_dataset,\n","    callbacks = [early_stopping, reduce_lr],\n",")"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plt.plot(history_phase1.history[\"loss\"]+history_phase2.history[\"loss\"])\n","plt.plot(history_phase1.history[\"val_loss\"]+history_phase2.history[\"val_loss\"])\n","plt.ylabel(\"Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.legend([\"train\", \"valid\"], loc=\"upper right\")\n","plt.show()"]},{"cell_type":"markdown","id":"821bd550","metadata":{},"source":["### Save model weights"]},{"cell_type":"code","execution_count":41,"id":"76b03716","metadata":{},"outputs":[],"source":["clip.save_weights('clip_weights.h5')"]},{"cell_type":"markdown","metadata":{"id":"jaCktt68IRcq"},"source":["### Task performance"]},{"cell_type":"code","execution_count":42,"metadata":{"trusted":true},"outputs":[],"source":["def generate_image_embeddings(\n","    image_encoder,                 # Image encoder of clip model\n","    dataset_eval,                  # Dataset to generate embeddings (WARNING: the dataset must not be shuffling or have a shuffle buffer size of 1)\n","    dataset_pred_map=lambda *x: x, # Lambda mapping function for prediction\n","    dataset_ref_map=lambda *x: x,  # Lambda mapping function for reference\n","):\n","    print(\"Generating image embeddings\")\n","    image_embeddings = image_encoder.predict(\n","        dataset_eval.map(dataset_pred_map),\n","        verbose=1,\n","    )\n","    dataset_reference = [x for x in dataset_eval.map(dataset_ref_map).unbatch()]\n","    return dataset_reference, image_embeddings\n","\n","def find_t2i_matches(\n","    queries,                # Queries to search\n","    text_encoder,           # Text encoder of clip model\n","    image_embeddings,       # Generated image embeddings\n","    dataset_reference=None, # Reference for retreived dataset elements following indices\n","    k=10,                    # Number of elements for top-k\n","    normalize=True,         # Embedding normalization\n","):\n","    print(\"Computing Text-to-Image matches\")\n","    # Generate query dataset and get their embeddings\n","    queries_ds = tf.data.Dataset.from_tensor_slices(queries).batch(batch_size)\n","    query_embedding = text_encoder.predict(queries_ds)\n","    # Normalize the query and the image embeddings\n","    if normalize:\n","        image_embeddings = tf.math.l2_normalize(image_embeddings, axis=1)\n","        query_embedding = tf.math.l2_normalize(query_embedding, axis=1)\n","    # Compute the dot product between the query and the image embeddings\n","    dot_similarity = tf.matmul(query_embedding, image_embeddings, transpose_b=True)\n","    # Retrieve top k indices\n","    results = tf.math.top_k(dot_similarity, k).indices.numpy()\n","    return results\n","\n","def index_to_reference(results, dataset_reference):\n","    return [[dataset_reference[match] for match in result] for result in results]\n","\n","def visualize_t2i_results(matches):\n","    # Assuming matches are in the form of tuples: (image_path, caption)\n","    print(\"Top matches for query: \\\"\" + query + \"\\\"\")\n","    plt.figure(figsize=(18, 18))\n","    for i in range(len(matches)):\n","        path = matches[i][0].numpy().decode('UTF-8')\n","        caption = matches[i][0].numpy()#.decode('UTF-8')\n","        print('Caption: '+path)\n","        #ax = plt.subplot(3, 3, i + 1)\n","        #plt.imshow(mpimg.imread(path))\n","        #plt.axis(\"off\")\n","        print(f\"{i}) {caption}\")"]},{"cell_type":"code","execution_count":43,"metadata":{"trusted":true},"outputs":[],"source":["# TODO: assumption that the whole dataset is used as a query for most metrics: **\n","def compute_relevant_at_k(results, dataset_reference, k=None, reference_preprocess=lambda x: x, relevance=lambda m, o: m == o):\n","    if not k:\n","        k = len(results[0])\n","    return [ \n","        np.count_nonzero([relevance(match, original) for match in list(map(reference_preprocess, matches))[0:k]])\n","        for matches, original in zip(results, map(reference_preprocess, dataset_reference)) # **\n","    ]\n","\n","def compute_total_relevance(dataset_reference, reference_preprocess=lambda x: x, relevance=lambda m, o: m == o):\n","    total_n = {}\n","    for element in map(reference_preprocess, dataset_reference):\n","        if element in total_n:\n","            total_n[element] += 1\n","        else:\n","            total_n[element] = 1\n","    return total_n\n","\n","def compute_top_k_accuracy(results, dataset_reference, relevant_at_k=None, k=None, reference_preprocess=lambda x: x, relevance=lambda m, o: m == o):\n","    if not relevant_at_k:\n","        relevant_at_k = compute_relevant_at_k(results, dataset_reference, k, reference_preprocess=reference_preprocess, relevance=relevance)\n","    hits = np.count_nonzero(relevant_at_k)\n","    return hits / len(dataset_reference)\n","\n","def compute_map_k(results, dataset_reference, relevant_at_k=None, k=None, reference_preprocess=lambda x: x, relevance=lambda m, o: m == o):\n","    if not k:\n","        k = len(results[0])\n","    if not relevant_at_k:\n","        relevant_at_k = compute_relevant_at_k(results, dataset_reference, k, reference_preprocess=reference_preprocess, relevance=relevance)\n","    precision_at_k = [r/k for r in relevant_at_k]\n","    return np.sum(precision_at_k) / len(dataset_reference)\n","\n","def compute_mar_k(results, dataset_reference, relevant_at_k=None, total_relevant=None, k=None, reference_preprocess=lambda x: x, relevance=lambda m, o: m == o):\n","    if not k:\n","        k = len(results[0])\n","    if not relevant_at_k:\n","        relevant_at_k = compute_relevant_at_k(results, dataset_reference, k, reference_preprocess=reference_preprocess, relevance=relevance)\n","    if not total_relevant_at_k:\n","        total_relevant = compute_total_relevance(results, dataset_reference, reference_preprocess=reference_preprocess, relevance=relevance)\n","    recall_at_k = [rk/tr for rk, tr in zip(relevant_at_k, total_relevant)] # **\n","    return np.sum(recall_at_k) / len(dataset_reference)"]},{"cell_type":"code","execution_count":44,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","### Scoring test data ###\n","Generating image embeddings\n","16655/16655 [==============================] - 257s 15ms/step\n","Computing Text-to-Image matches\n","1666/1666 [==============================] - 15s 9ms/step\n"]}],"source":["k = 5\n","concept_overlap_threshold = 2\n","reference_preprocess_cap = lambda x: x[0].numpy().decode('UTF-8')                                    # Function to preprocess data when we want to evaluate captions\n","reference_preprocess_con = lambda x: x[1]                                                            # Function to preprocess data when we want to evaluate concepts\n","concept_relevance = lambda m, o: np.count_nonzero(np.logical_and(m, o)) >= concept_overlap_threshold # Function to compute if a match is relevant given concept arrays \n","\n","'''\n","print(\"### Scoring training data ###\")\n","train_dataset_reference, train_image_embeddings = generate_image_embeddings(\n","    clip_image_encoder,\n","    train_dataset_eval,\n","    dataset_pred_map=lambda x, y: x['image'],\n","    dataset_ref_map=lambda x, y: (y['caption'], y['concepts'])\n",")\n","train_queries = [e[0] for e in train_dataset_reference]\n","# Compute relevance for all the queries in the dataset using only caption equality as a metric\n","train_tot_relevant = compute_total_relevance(train_dataset_reference, reference_preprocess=reference_preprocess_cap)\n","# Compute matching results and extrapolate relevant matches based on different criterions\n","train_raw_results = find_t2i_matches(train_queries, clip_text_encoder, train_image_embeddings, k=k, normalize=True)\n","train_results = index_to_reference(train_raw_results, train_dataset_reference)\n","train_relevant_cap = compute_relevant_at_k(train_results, train_dataset_reference, reference_preprocess=reference_preprocess_cap)\n","train_relevant_con = compute_relevant_at_k(train_results, train_dataset_reference, reference_preprocess=reference_preprocess_con, relevance=concept_relevance)\n","'''\n","\n","print(\"\\n### Scoring test data ###\")\n","test_dataset_reference, test_image_embeddings = generate_image_embeddings(\n","    clip_image_encoder,\n","    test_dataset_eval,\n","    dataset_pred_map=lambda x, y: x['image'],\n","    dataset_ref_map=lambda x, y: (y['caption'], y['concepts'])\n",")\n","test_queries = [e[0] for e in test_dataset_reference]\n","# Compute relevance for all the queries in the dataset using only caption equality as a metric\n","test_tot_relevant = compute_total_relevance(test_dataset_reference, reference_preprocess=reference_preprocess_cap)\n","# Compute matching results and extrapolate relevant matches based on different criterions\n","test_raw_results = find_t2i_matches(test_queries, clip_text_encoder, test_image_embeddings, k=k, normalize=True)\n","test_results = index_to_reference(test_raw_results, test_dataset_reference)\n","test_relevant_cap = compute_relevant_at_k(test_results, test_dataset_reference, reference_preprocess=reference_preprocess_cap)\n","test_relevant_con = compute_relevant_at_k(test_results, test_dataset_reference, reference_preprocess=reference_preprocess_con, relevance=concept_relevance)"]},{"cell_type":"code","execution_count":45,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","### Test data ###\n","Accuracy for caption equality: 1.513%\n","Accuracy for concept overlap: 44.947%\n","Mean Average Precision for caption equality: 0.305%\n","Mean Average Precision for concept overlap: 22.417%\n"]}],"source":["'''\n","print(\"### Training data ###\")\n","train_accuracy_cap = compute_top_k_accuracy(train_results, train_dataset_reference, relevant_at_k=train_relevant_cap)\n","train_accuracy_con = compute_top_k_accuracy(train_results, train_dataset_reference, relevant_at_k=train_relevant_con)\n","print(f\"Accuracy for caption equality: {round(train_accuracy_cap * 100, 3)}%\")\n","print(f\"Accuracy for concept overlap: {round(train_accuracy_con * 100, 3)}%\")\n","train_map_cap = compute_map_k(train_results, train_dataset_reference, relevant_at_k=train_relevant_cap)\n","train_map_con = compute_map_k(train_results, train_dataset_reference, relevant_at_k=train_relevant_con)\n","print(f\"Mean Average Precision for caption equality: {round(train_map_cap * 100, 3)}%\")\n","print(f\"Mean Average Precision for concept overlap: {round(train_map_con * 100, 3)}%\")\n","'''\n","\n","print(\"\\n### Test data ###\")\n","test_accuracy_cap = compute_top_k_accuracy(test_results, test_dataset_reference, relevant_at_k=test_relevant_cap)\n","test_accuracy_con = compute_top_k_accuracy(test_results, test_dataset_reference, relevant_at_k=test_relevant_con)\n","print(f\"Accuracy for caption equality: {round(test_accuracy_cap * 100, 3)}%\")\n","print(f\"Accuracy for concept overlap: {round(test_accuracy_con * 100, 3)}%\")\n","test_map_cap = compute_map_k(test_results, test_dataset_reference, relevant_at_k=test_relevant_cap)\n","test_map_con = compute_map_k(test_results, test_dataset_reference, relevant_at_k=test_relevant_con)\n","print(f\"Mean Average Precision for caption equality: {round(test_map_cap * 100, 3)}%\")\n","print(f\"Mean Average Precision for concept overlap: {round(test_map_con * 100, 3)}%\")"]},{"cell_type":"code","execution_count":46,"id":"2b43ec7a","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Computing Text-to-Image matches\n","1/1 [==============================] - 0s 11ms/step\n","Top matches for query: \"brain\"\n","Caption: a  male patient with  with bilateral thalamic involvement in brain imaging\n","0) b'a  male patient with  with bilateral thalamic involvement in brain imaging'\n","Caption: mri take one month postoperatively\n","1) b'mri take one month postoperatively'\n","Caption: axial  weighted image show hypointensity of the genu white arrow and splenium white arrowhead of corpus callosum\n","2) b'axial  weighted image show hypointensity of the genu white arrow and splenium white arrowhead of corpus callosum'\n","Caption: magnetic resonance imaging axial  image show a heterogeneously hyperintense lesion with multiple flow void\n","3) b'magnetic resonance imaging axial  image show a heterogeneously hyperintense lesion with multiple flow void'\n","Caption: contrast enhancement magnetic resonance imaging scan of case\n","4) b'contrast enhancement magnetic resonance imaging scan of case'\n"]},{"data":{"text/plain":["<Figure size 1800x1800 with 0 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["query = \"brain\"\n","# WARNING: currently using train_dataset_eval\n","'''\n","dataset_reference, image_embeddings = generate_image_embeddings(\n","    clip_image_encoder,\n","    train_dataset_eval,\n","    dataset_pred_map=lambda x, y: x['image'],\n","    dataset_ref_map=lambda x, y: (x['image path'], y['caption'])\n",") '''\n","results = find_t2i_matches([query], clip_text_encoder, test_image_embeddings, k=5, normalize=False)\n","results = index_to_reference(results, test_dataset_reference)\n","for matches in results:\n","    visualize_t2i_results(matches)"]},{"cell_type":"code","execution_count":null,"id":"b10ee35f","metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["jaCktt68IRcq","Jor40RYWJefh"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"},"vscode":{"interpreter":{"hash":"5380e256bec2a872a4245067cef5da364603399a350ba03e757739343e36dd55"}}},"nbformat":4,"nbformat_minor":5}

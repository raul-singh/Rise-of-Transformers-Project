{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["jaCktt68IRcq","Jor40RYWJefh"],"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"5380e256bec2a872a4245067cef5da364603399a350ba03e757739343e36dd55"}}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Declarations","metadata":{"id":"xffJLUZCLC51"}},{"cell_type":"markdown","source":"### Imports","metadata":{"id":"7TgHFX6XLQGl"}},{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\nimport math\nimport string\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nfrom IPython.display import display\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom tqdm import tqdm\n\nimport tensorflow_hub as hub\nimport tensorflow_text as text\n\ntfk = tf.keras\ntfkl = tf.keras.layers\nkb = tf.keras.backend\nprint(tf.__version__)\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4674,"status":"ok","timestamp":1685806019663,"user":{"displayName":"Ri ga","userId":"15539171450533002544"},"user_tz":-120},"id":"2d093180","outputId":"bc84c9fd-b81e-49b9-d8a7-0c7e03c595e0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Constants","metadata":{"id":"-icpKfuyLSrH"}},{"cell_type":"code","source":"# Random seed for reproducibility\nseed = 42\n\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)\n\nkaggle1 = \"/kaggle/input/transformers-hackathon/\"\nkaggle2 = \"/kaggle/input/transformers-hackathon-features/\"\n\nimage_dir = \"./resized_train\"\ncaption_pred_file = \"caption_prediction_train.csv\"\nconcept_det_file = \"concept_detection_train.csv\"\nconcept_file = \"concepts.csv\"\n\n##### Kaggle filepath #####\nimage_dir = kaggle1 + image_dir\ncaption_pred_file = kaggle2 + caption_pred_file\nconcept_det_file = kaggle2 + concept_det_file\nconcept_file = kaggle2 + concept_file\n###########################\n\nimage_size = (128, 128, 3)\n\nbatch_size = 10\nepochs = 100","metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1685806019664,"user":{"displayName":"Ri ga","userId":"15539171450533002544"},"user_tz":-120},"id":"f3cf13ac","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{"id":"n1w8gAuILhCl"}},{"cell_type":"code","source":"feature_types = {'image': tf.float16, 'caption': tf.string, 'concepts': tf.bool, 'raw caption': tf.string, 'image path': tf.string}\nfeature_shapes = {'image': (128, 128, 3), 'caption': (), 'concepts': (8374)}\nbase_features = [\"image\", \"caption\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concepts = pd.read_csv(concept_file, sep='\\t')\nconcept_list = concepts.set_index('concept')['concept_name'].to_dict()\n# Concept one-hot encoder\nconcepts_onehot = MultiLabelBinarizer(classes = list(concept_list.keys()))\nconcepts_onehot.fit([list(concept_list.keys())])\n\ncaptions = pd.read_csv(caption_pred_file, sep='\\t')\ncaptions = captions.set_index('ID')['caption'].to_dict()\ncaptions = {id: \"[SOS] \" + caption + \" [EOS]\" for id, caption in captions.items()}\n\nconcepts = pd.read_csv(concept_det_file, sep='\\t')\nconcepts = concepts.set_index('ID')['cuis'].to_dict()\nconcepts = {id: item_concepts.split(\";\") for id, item_concepts in concepts.items()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split(x, test_size=0.2, val_size=0.0, seed=0):\n    if val_size + test_size >= 1:\n        return None\n    x_train, x_test = train_test_split(\n        x, test_size=test_size + val_size, random_state=seed\n    )\n    x_val = None\n    if val_size > 0:\n        x_test, x_val = train_test_split(\n            x_test,\n            test_size=val_size / (test_size + val_size),\n            random_state=seed,\n        )\n    return x_train, x_val, x_test\n\ndef load_image_from_path(path):\n    image = tf.io.read_file(path)\n    image = tf.io.decode_jpeg(image, channels=3, dct_method=\"INTEGER_ACCURATE\")\n\n    # may need resizing\n    #image = tf.image.resize(image, image_shape[:2])\n    image = tf.cast(image, dtype=tf.float16)\n    image = image / 255.0\n    return image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def custom_standardization(input_string):\n    # convert input string to lowercase\n    lowercase = tf.strings.lower(input_string)\n    # replace special characters with empty string\n    # TODO\n    #return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n    return lowercase","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = \"\"\nfor i in captions.values():\n    result += \" \" + i\nresult = custom_standardization(result)\nresult = bytes.decode(result.numpy())\nvocab_size = len(set(result.split()))\nprint(\"Vocab size:\")\nprint(vocab_size)\n\nlongest = max(captions.values(), key=len)\nlongest = custom_standardization(longest)\nlongest = bytes.decode(longest.numpy())\nlongest = longest.split()\nsequence_length = len(longest)\nprint(\"Longest sequence:\")\nprint(sequence_length)\n\nconcept_size = max([len(c) for _, c in concepts.items()])\nprint(\"Max number of concepts:\")\nprint(concept_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_features(image_folder, captions_file, concepts_file, concept_encoder, filter_percent=1):\n    features = []\n    \n    # Import CSVs\n    csv_caption_dataset = tf.data.experimental.CsvDataset(\n        captions_file,\n        field_delim='\\t',\n        record_defaults=[tf.string, tf.string],\n        header=True,\n        select_cols=[0, 1]\n    )\n    csv_concept_dataset = tf.data.experimental.CsvDataset(\n        concepts_file,\n        field_delim='\\t',\n        record_defaults=[tf.string, tf.string],\n        header=True,\n        select_cols=[0, 1]\n    )\n    \n    # We make the assumption that CSV files contain the same key values (image names)\n    # following the same ordering\n\n    # Extract features from dataset\n    print(\"Extracting features from CSV file(s)\")\n    for caption_el, concept_el in tqdm(zip(csv_caption_dataset, csv_concept_dataset)):\n        filename_cap, caption = caption_el\n        filename_con , concepts = concept_el\n        \n        # Sanity check\n        assert filename_cap == filename_con\n        \n        image_path = image_dir + \"/\" + filename_cap + \".jpg\"\n        \n        features.append({\n            'caption': caption,\n            'image path': image_path,\n            'concepts': concept_encoder.transform([concepts.numpy().decode(\"utf-8\").split(\";\")]),\n        })\n        \n    # Filter elements\n    if filter_percent != 1:\n        n_features = int(len(features) * filter_percent)\n        features = random.sample(features, n_features)\n        \n    return features\n\ndef preprocess_features(features, concept_encoder, filter_percent=1):\n    print(\"Preprocessing features\")\n    \n    # Filter elements\n    if filter_percent != 1:\n        n_features = int(len(features) * filter_percent)\n        features = random.sample(features, n_features)\n        \n    return {\n        'image paths': tf.convert_to_tensor([x[\"image path\"] for x in tqdm(features)], dtype=tf.string),\n        'captions': tf.convert_to_tensor([x[\"caption\"] for x in tqdm(features)], dtype=tf.string),\n        'concepts': tf.convert_to_tensor(np.vstack([concept_encoder.transform(x[\"concepts\"]).flatten() for x in tqdm(features)]), dtype=tf.bool),\n        # 'images': tf.convert_to_tensor([load_image(x[\"image path\"]) for x in tqdm(features)], dtype=tf.float16),\n    }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load dataset features from csv files, split them and preprocess them\nfeatures = load_features(image_dir, caption_pred_file, concept_det_file, concepts_onehot, filter_percent=0.01)\nfeat_train, feat_val, feat_test = split(features, test_size=0.2, val_size=0.0, seed=seed)\n\n#feat_train = preprocess_features(features, concepts_onehot, filter_percent=0.01) if feat_train else None\n#feat_val = preprocess_features(features, concepts_onehot, filter_percent=0.01) if feat_val else None\n#feat_test = preprocess_features(features, concepts_onehot, filter_percent=0.01) if feat_test else None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dataset(\n        features, \n        input_features_types, \n        x_features, y_features=None, \n        x_dict=True, y_dict=True,\n        load_images=True, \n        shuffle_buffer_size=1024, \n        batch_size=32, \n        cached=False\n):\n    # Generate dataset following initial input feature types\n    dataset = tf.data.Dataset.from_generator(\n        lambda: features, { x: input_features_types[x] for x in input_features_types }\n    )\n    \n    # Preprocessing internal functions\n    def add_images(e):\n        # Maybe parametrize\n        img_from = \"image path\"\n        img_to = \"image\"\n        new_features = list(input_features_types.keys()) + [img_to]\n        return {f:e[f] if f != img_to else load_image_from_path(e[img_from]) for f in new_features}\n    def split_xy(e):\n        e_x = {xf:e[xf] for xf in x_features} if x_dict else tf.squeeze([e[xf] for xf in x_features])\n        if y_features:\n            e_y = {yf:e[yf] for yf in y_features} if y_dict else tf.squeeze([e[yf] for yf in y_features])\n            return (e_x, e_y)\n        return e_x\n    \n    # Preprocess\n    if load_images:\n        dataset = dataset.map(add_images)\n    dataset = dataset.map(split_xy)\n\n    # Compile dataset\n    if cached:\n        dataset = dataset.cache()\n    dataset = dataset.shuffle(shuffle_buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n    return dataset\n\ndef visualize_first_of_dataset_batch(dataset_batch, nums=5):\n    for c in range(0, nums):\n        i = tf.cast(dataset_batch[\"image\"][c], dtype=tf.float32)\n        t = dataset_batch[\"raw caption\"][c]\n        plt.figure(figsize=(50, 100))\n        plt.subplot(nums, 1, c + 1)\n        plt.imshow(i)\n        plt.title(f\"{t}\", fontsize=100)\n        plt.xticks([])\n        plt.yticks([])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"in_feat_typ = {'caption': tf.string, 'concepts': tf.bool, 'image path': tf.string}\nx_features = ['caption', 'image']\nx_features_iep = ['image']\ny_features_iep = ['concepts']\n\ntrain_ds_size = len(feat_train) if feat_train else 0\nval_ds_size = len(feat_val) if feat_val else 0\ntest_ds_size = len(feat_test) if feat_test else 0\n\ntrain_dataset = create_dataset(feat_train, input_features_types=in_feat_typ, x_features=x_features) if feat_train else None\nval_dataset = create_dataset(feat_val, input_features_types=in_feat_typ, x_features=x_features) if feat_val else None\ntest_dataset = create_dataset(feat_test, input_features_types=in_feat_typ, x_features=x_features) if feat_test else None\n\ntrain_dataset_iep = create_dataset(feat_train, input_features_types=in_feat_typ, x_features=x_features_iep, y_features=y_features_iep, x_dict=False, y_dict=False) if feat_train else None\nval_dataset_iep = create_dataset(feat_val, input_features_types=in_feat_typ, x_features=x_features_iep, y_features=y_features_iep, x_dict=False, y_dict=False) if feat_val else None\ntest_dataset_iep = create_dataset(feat_test, input_features_types=in_feat_typ, x_features=x_features_iep, y_features=y_features_iep, x_dict=False, y_dict=False) if feat_test else None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Download Models","metadata":{}},{"cell_type":"code","source":"text_preprocess = hub.KerasLayer(\n        \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2\",\n        name=\"text_preprocessing\",\n    )\n\ntext_transformer = hub.KerasLayer(\n        \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\",\n        trainable=True,\n        name=\"bert\",\n    )\n\nimg_preprocess = tfk.applications.convnext.preprocess_input\n\nimg_supernet = tfk.applications.ConvNeXtTiny(weights='imagenet', include_top=False)\nsupernet_name = img_supernet.name","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pre-pre-training","metadata":{}},{"cell_type":"code","source":"def image_encoder_pretrainer(preprocessing, supernet, n_concepts, input_shape=(128,128,3), learning_rate=1e-5):\n    \n    input_layer = tfkl.Input(shape=input_shape, name='image')\n\n    x = preprocessing(input_layer)\n    x = supernet(x)\n    \n    x = tfkl.GlobalMaxPooling2D(name='GAP')(x)\n    x = tfkl.Dense(256, activation='relu')(x)\n    x = tfkl.Dense(128, activation='relu')(x)\n    x = tfkl.Dense(n_concepts, activation=\"sigmoid\", name='output')(x)\n\n    image_encoder_pretrainer = tfk.Model(inputs=input_layer, outputs=x, name=\"image_encoder_pretrainer\")\n    image_encoder_pretrainer.compile(\n        loss=\"binary_crossentropy\", optimizer=tf.optimizers.Adam(learning_rate=learning_rate),\n    )\n    \n    return image_encoder_pretrainer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"iep = image_encoder_pretrainer(img_preprocess, img_supernet, len(concept_list.keys()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.config.run_functions_eagerly(True)\n\nhistory = iep.fit(\n    train_dataset_iep,\n    epochs = 5,\n    validation_data = test_dataset_iep,\n    callbacks = [early_stopping],\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a learning rate scheduler callback.\nreduce_lr = tfk.callbacks.ReduceLROnPlateau(\n    monitor = \"val_loss\", factor = 0.2, patience = 3\n)\n\n# Create an early stopping callback.\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor = \"val_loss\", patience = 5, restore_best_weights = True\n)\n\niep.layers[0].trainable = True\niep.compile(\n    loss=\"binary_crossentropy\", optimizer=tf.optimizers.Adam(learning_rate=1e-6),\n)\n\nhistory = iep.fit(\n    train_dataset_iep,\n    epochs = 20,\n    validation_data = test_dataset_iep,\n    callbacks = [reduce_lr, early_stopping],\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_supernet = iep.layers[0]\nimg_supernet.trainable = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Network","metadata":{"id":"oiUz4hxwNRVS"}},{"cell_type":"markdown","source":"### Network blocks","metadata":{"id":"WhVp1-YONWpY"}},{"cell_type":"code","source":"def image_encoder(input_shape, latent_dim, embed_dim, seed=42, supernet=None, preprocessing=None):\n    \n    tf.random.set_seed(seed)\n\n    input_layer = tfkl.Input(shape=input_shape, name='img_input_layer')\n\n    x = preprocessing(input_layer)\n    x = supernet(x)\n\n    # Projection\n    x = tfkl.GlobalAveragePooling2D(name='GAP')(x)\n    x = tfkl.Dense(latent_dim, activation='relu')(x)\n    x = tfkl.Dense(embed_dim, name='img_embedding_output_layer')(x)\n\n    # Connect input and output through the Model class\n    cnn_encoder = tfk.Model(inputs=input_layer, outputs=x, name='image_encoder')\n\n    # Return the encoder\n    return cnn_encoder","metadata":{"executionInfo":{"elapsed":441,"status":"ok","timestamp":1685807496567,"user":{"displayName":"Ri ga","userId":"15539171450533002544"},"user_tz":-120},"id":"2bc14deb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_encoder(latent_dim, embed_dim, preprocess, transformer, trainable=True):\n\n    transformer.trainable = trainable\n    \n    input_layer = tfkl.Input(shape=(), dtype=tf.string, name=\"text_input\")\n    x = preprocess(input_layer)\n    x = transformer(x)[\"pooled_output\"]\n    \n\n    # Projection\n    x = tfkl.Dense(latent_dim, activation='relu')(x)\n    x = tfkl.Dense(embed_dim, name='txt_embedding_output_layer')(x)\n\n    text_encoder = tfk.Model(inputs=input_layer, outputs=x, name=\"text_encoder\")\n    \n    return text_encoder","metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1685807500205,"user":{"displayName":"Ri ga","userId":"15539171450533002544"},"user_tz":-120},"id":"50bc32b1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLIP(tfk.Model):\n    def __init__(self, image_encoder, text_encoder, temp=0.07, **kwargs):\n        super().__init__(**kwargs)\n        self.image_encoder = image_encoder\n        self.text_encoder = text_encoder\n        self.temp = temp\n        self.loss_tracker = tfk.metrics.Mean(name=\"loss\")\n\n    @property\n    def metrics(self):\n        return [self.loss_tracker]\n\n    def call(self, features, training=False):\n        image_emb = self.image_encoder(features[\"image\"], training=training)\n        text_emb = self.text_encoder(features[\"caption\"], training=training)\n        return image_emb, text_emb\n\n    def CLIP_loss(self, image_emb, text_emb):\n        norm_image_emb = tf.math.l2_normalize(image_emb, axis=1)\n        norm_text_emb = tf.math.l2_normalize(text_emb, axis=1)\n\n        logits = tf.linalg.matmul(norm_image_emb, norm_text_emb, transpose_b=True) * tf.math.exp(self.temp)\n\n        n = tf.shape(logits)[0]\n        labels = tf.range(n)\n\n        loss_img = tfk.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n        loss_txt = tfk.losses.sparse_categorical_crossentropy(labels, kb.transpose(logits), from_logits=True)\n\n        return (loss_img + loss_txt) / tf.constant(2.0)\n\n    def train_step(self, features):\n        with tf.GradientTape() as tape:\n            image_embeddings, caption_embeddings = self(features, training=True)\n            loss = self.CLIP_loss(caption_embeddings, image_embeddings)\n\n        gradients = tape.gradient(loss, self.trainable_variables)\n        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n\n        self.loss_tracker.update_state(loss)\n        return {\"loss\": self.loss_tracker.result()}\n\n    def test_step(self, features):\n        image_embeddings, caption_embeddings = self(features, training=False)\n        loss = self.CLIP_loss(caption_embeddings, image_embeddings)\n        self.loss_tracker.update_state(loss)\n        return {\"loss\": self.loss_tracker.result()}\n","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1685807500648,"user":{"displayName":"Ri ga","userId":"15539171450533002544"},"user_tz":-120},"id":"tQfOhkjPjz70","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building network","metadata":{"id":"FWPNhGjzNbnP"}},{"cell_type":"code","source":"def build_clip(img_input_shape=(128,128,3),\n               txt_input_shape=(393, ), \n               latent_dim=1024, \n               embed_dim=128, \n               temp=0.07,\n               learning_rate=1e-5,\n               img_supernet=None,\n               img_preprocess=None,\n               text_transformer=None,\n               text_preprocess=None):\n\n    \n    text_encoder_model = text_encoder(latent_dim, embed_dim, text_preprocess, text_transformer)\n    image_encoder_model = image_encoder(img_input_shape, latent_dim, embed_dim, supernet=img_supernet, preprocessing=img_preprocess)\n\n    clip = CLIP(image_encoder_model, text_encoder_model, temp)\n    clip.compile(optimizer = tf.optimizers.Adam(learning_rate=learning_rate))\n\n    return image_encoder_model, text_encoder_model, clip","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1685807502773,"user":{"displayName":"Ri ga","userId":"15539171450533002544"},"user_tz":-120},"id":"mra2VO7JoqGj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clip_image_encoder, clip_text_encoder, clip = build_clip(\n    img_supernet=img_supernet,\n    img_preprocess=img_preprocess,\n    text_transformer=text_transformer,\n    text_preprocess=text_preprocess,\n)","metadata":{"executionInfo":{"elapsed":564,"status":"ok","timestamp":1685807504408,"user":{"displayName":"Ri ga","userId":"15539171450533002544"},"user_tz":-120},"id":"f0778191","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{"id":"mPnccec1Nfwh"}},{"cell_type":"markdown","source":"### CLIP pre-training","metadata":{"id":"eqrFQi0ZNhvo"}},{"cell_type":"code","source":"# Create a learning rate scheduler callback.\nreduce_lr = tfk.callbacks.ReduceLROnPlateau(\n    monitor = \"val_loss\", factor = 0.2, patience = 3\n)\n\n# Create an early stopping callback.\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor = \"val_loss\", patience = 5, restore_best_weights = True\n)\n\nhistory = clip.fit(\n    train_dataset,\n    epochs = 3,\n    validation_data = test_dataset,\n    callbacks = [reduce_lr, early_stopping],\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"executionInfo":{"elapsed":1920448,"status":"error","timestamp":1685809433717,"user":{"displayName":"Ri ga","userId":"15539171450533002544"},"user_tz":-120},"id":"jS2cVFlVrHLs","outputId":"fc619277-db77-4c8a-f742-c0a373c094b7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"train\", \"valid\"], loc=\"upper right\")\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"elapsed":10294,"status":"ok","timestamp":1676839846200,"user":{"displayName":"Ri ga","userId":"15539171450533002544"},"user_tz":-60},"id":"ovNQyvWYy8g4","outputId":"62de2a46-d1e4-4280-825c-a2baf08f90de","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clip.save(\"keras_clip_model\")","metadata":{"id":"8WqAGCTRJYxn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoder/Decoder training","metadata":{"id":"jaCktt68IRcq"}},{"cell_type":"code","source":"def read_image(image_path):\n    image_array = tf.image.decode_jpeg(tf.io.read_file(image_path), channels=3)\n    return image_array","metadata":{"id":"Dpv8yhO1Ih2j","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_matches(image_embeddings, queries, k=5, normalize=True):\n    queries_vec = [text_vectorization(query) for query in queries]\n    queries_vec = tf.data.Dataset.from_tensor_slices(queries_vec).batch(batch_size)\n    # Get the embedding for the query.\n    query_embedding = text_encoder.predict(queries_vec)\n    # Normalize the query and the image embeddings.\n    if normalize:\n        image_embeddings = tf.math.l2_normalize(image_embeddings, axis=1)\n        query_embedding = tf.math.l2_normalize(query_embedding, axis=1)\n    # Compute the dot product between the query and the image embeddings.\n    dot_similarity = tf.matmul(query_embedding, image_embeddings, transpose_b=True)\n    # Retrieve top k indices.\n    results = tf.math.top_k(dot_similarity, k).indices.numpy()\n    # Return matching image paths.\n    return [[train_image_paths[idx] for idx in indices] for indices in results]","metadata":{"id":"E5uDwwUkJE5u","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = [p for p in ds_train]\nval_data = [p for p in ds_val]\ntest_data = [p for p in ds_test]\n\ntrain_image_paths = [e[\"image path\"] for e in train_data]\ntest_image_paths = [e[\"image path\"] for e in test_data]\n\n# TODO: this part only generates embeddings on the training dataset for now\n# TODO: this code re-reads the images\nimage_embeddings = image_encoder.predict(\n    tf.data.Dataset.from_tensor_slices(train_image_paths).map(read_image).batch(batch_size),\n    verbose=1,\n)\nprint(f\"Image embeddings shape: {image_embeddings.shape}.\")\n","metadata":{"id":"eWCwgWo_zMf1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query = \"active pheochromocytoma\"\nmatches = find_matches(image_embeddings, [query], normalize=True)[0]\n\nprint(\"Top matches for query: \\\"\" + query + \"\\\"\")\n\nplt.figure(figsize=(18, 18))\nfor i in range(9):\n    path = matches[i].numpy().decode('UTF-8')\n    caption = next(x[\"raw caption\"] for x in train_data if x[\"image path\"].numpy().decode('UTF-8') == path)\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(mpimg.imread(path))\n    plt.axis(\"off\")\n    print(caption)\n","metadata":{"id":"zWFOxWhV0kbF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{"id":"Jor40RYWJefh"}},{"cell_type":"code","source":"# TODO: might not work, needs revising\n\ndef compute_top_k_accuracy(image_paths, k=5):\n    hits = 0\n    num_batches = int(np.ceil(len(image_paths) / batch_size))\n    for idx in range(num_batches):\n        start_idx = idx * batch_size\n        end_idx = start_idx + batch_size\n        current_image_paths = image_paths[start_idx:end_idx]\n        queries = [ captions[os.path.splitext(image_path.numpy().decode('UTF-8').split(os.sep)[-1])[0]] for image_path in current_image_paths ]\n        result = find_matches(image_embeddings, queries, k)\n        hits += sum(\n            [\n                image_path in matches for (image_path, matches) in list(zip(current_image_paths, result))\n            ]\n        )\n\n    return hits / len(image_paths)\n\nn = 1920\n\nprint(test_image_paths)\n\nprint(\"Scoring training data...\")\ntrain_accuracy = compute_top_k_accuracy(random.sample(train_image_paths, n))\nprint(f\"Train accuracy: {round(train_accuracy * 100, 3)}%\")\n\nprint(\"Scoring evaluation data...\")\neval_accuracy = compute_top_k_accuracy(random.sample(test_image_paths, n))\nprint(f\"Eval accuracy: {round(eval_accuracy * 100, 3)}%\")\n","metadata":{"id":"X3HGLJ8w0ZdF","trusted":true},"execution_count":null,"outputs":[]}]}